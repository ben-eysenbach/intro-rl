{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Homework 2: Imitation Learning\n",
        "\n",
        "Welcome to the coding portion of Homework 2! This week's problem set focuses on implementing imitation learning/behavioral cloning. The goal is that by the end of the assignment, you will know how to implement behavioral cloning and evaluate the cloned policy. As an added bonus, you will get to train an agent to play Flappy Bird!  \n",
        "\n",
        "<center>\n",
        "<img width=\"300px\" src=\"https://drive.google.com/uc?id=1O4dAX_rN62fjsBRDjaYpz01ZcQ4aVP6A\">\n",
        "</center>\n",
        "\n",
        "**Notes:**\n",
        "\n",
        "* **You should be running this on Google Colab!**\n",
        "* **Be sure to upload the dataset files provided with this .ipynb file using the folder tab on the left, so that you can have access to them.**\n"
      ],
      "metadata": {
        "id": "3v3EhFObtCJC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# IMPORTANT: Always run this cell before anything else to ensure that you are able to access the Flappy Bird environment.\n",
        "from IPython.display import clear_output\n",
        "\n",
        "%pip install git+https://github.com/kchua/flappy-bird-gym.git\n",
        "clear_output()\n",
        "import flappy_bird_gym\n",
        "from matplotlib import rc\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.animation as animation\n",
        "import numpy as np\n",
        "\n",
        "rc('animation', html='jshtml')\n",
        "\n",
        "def animate_images(img_lst):\n",
        "  fig, ax = plt.subplots()\n",
        "  ax.set_axis_off()\n",
        "  ims = []\n",
        "  for i, image in enumerate(img_lst):\n",
        "    im = ax.imshow(image.swapaxes(0, 1), animated=True)\n",
        "    if i == 0:\n",
        "      ax.imshow(image.swapaxes(0, 1))\n",
        "    ims.append([im])\n",
        "  ani = animation.ArtistAnimation(fig, ims, interval=34)\n",
        "  return ani"
      ],
      "metadata": {
        "id": "X9nDr3QgSlqC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.distributions import Distribution"
      ],
      "metadata": {
        "id": "2piEzHQminlT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Problem 1: Flap Like How Flappy Sr. Taught You! (AKA Implementing Behavioral Cloning)"
      ],
      "metadata": {
        "id": "aN3H3f1LcliY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Growing up, Flappy Jr. has always dreamed of being as graceful as his mentor, Flappy Sr., in navigating the randomly generated pipes of the world. However, one day, his mentor simply disappeared on a daily pipe dodging adventure - eyewitness reports say that his mentor is still pipe dodging and has not reset since the day he left.\n",
        "\n",
        "One day, as he was going about his day, he found a secret cabinet in his mentor's house containing notes of all of his pipe dodging exploits. Flappy Jr. thought to himself, \"This is my chance! If I copy whatever he did in these notes, I will be as good as him.\"\n",
        "\n",
        "In the following problems, we will help Flappy Jr. by implementing behavioral cloning to make use of Flappy Sr.'s notes."
      ],
      "metadata": {
        "id": "O70GHQbvm76A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### (a) Policy Evaluation\n",
        "\n",
        "Before we get started with any training, let us make sure that we can evaluate whatever policy we get at the end. Recall that in reinforcement learning, the primary way in which we compare policies is by comparing their _returns_. The (infinite discounted) return of a policy $\\pi$, denoted $R(\\pi)$, is defined as\n",
        "\n",
        "$$R(\\pi) := \\mathbb{E}\\left[\\sum_{t = 0}^\\infty \\gamma^tr(s_t, a_t)\\right],$$\n",
        "\n",
        "where $s_0 \\sim p_0(s)$, and for all $t$, $a_t \\sim \\pi(s_t)$ and $s_{t + 1} \\sim p(s_{t + 1} \\ | \\ s_t, a_t)$, and $\\gamma$ is the discount factor."
      ],
      "metadata": {
        "id": "NrhEoUjWXuf-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**In the following code box, write a function `evaluate_policy`, which given an environment `env`, a callable `policy` which returns an action distribution given a state, and discount factor `discount`, returns a single-sample estimate of $R(\\pi)$ as defined above.** `env` is assumed to follow the \\*old Gym API, and the action distribution returned by `policy` is a subclass of `torch.distributions.Distribution`. You can assume that the rollout eventually terminates.\n",
        "\n",
        "\\*In newer versions of Gym/Gymnasium, `init` and `step` are assumed to return `(initial_ob, info_dict)` and `(next_ob, reward, terminated, truncated, info)`'. However, the environment we will be working with for this assignment instead returns `initial_ob` and `(next_ob, reward, done, info)`, following the old API."
      ],
      "metadata": {
        "id": "DcGIvwdhNt7V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_policy(env, policy, discount) -> float:\n",
        "  \"\"\"Returns a single-sample estimate of a policy's return.\n",
        "\n",
        "  Args:\n",
        "    env: OpenAI gym environment following old API.\n",
        "    policy: Callable representing a policy.\n",
        "\n",
        "  Returns:\n",
        "    Single-sample estimate of return.\n",
        "  \"\"\"\n",
        "  policy_return, cur_ob, done, n_steps = 0, env.reset(), False, 0\n",
        "  while not done:\n",
        "    action_dist = policy(cur_ob)\n",
        "    action = action_dist.sample()\n",
        "    cur_ob, reward, done, _ = env.step(action)\n",
        "    policy_return += (discount ** n_steps) * reward\n",
        "    n_steps += 1\n",
        "  return policy_return"
      ],
      "metadata": {
        "id": "FEhyB9RNXspl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**As a test, use the function you defined in the block above to evaluate a policy which acts uniformly at random within the Flappy Bird environment over 50 independent rollouts. Use a discount of $0.999$.**"
      ],
      "metadata": {
        "id": "oXW-MPqITqwS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env = flappy_bird_gym.make(\"FlappyBird-v0\")\n",
        "\n",
        "def uniform_policy(_ob):\n",
        "  return torch.distributions.Categorical(logits=torch.ones(env.action_space.n))\n",
        "\n",
        "returns = [evaluate_policy(env, uniform_policy, 0.999) for _ in range(50)]\n",
        "print(np.mean(returns))"
      ],
      "metadata": {
        "id": "qNy_TD0cTlUQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### (b) Defining a Policy\n",
        "\n",
        "**In the code cell below, create a subclass of ``torch.nn.Module`` to represent a policy over a finite set of actions.** Remember, a policy outputs an instance of `torch.distributions.Distribution` (can be one of its subclasses), and the policy must be able to represent any distribution over the finite set of actions! **We have defined the skeleton for you below; do not modify the inputs to the functions.**\n",
        "\n",
        "Note: Feel free to define the architecture however you like; _all we need from you is that you properly initialize the module, and that the forward method is consistent with our definition of a policy._"
      ],
      "metadata": {
        "id": "INOeN-RmdOqp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DiscretePolicy(nn.Module):\n",
        "  def __init__(self, input_dim, n_actions):\n",
        "    \"\"\"Initializes a policy over the action set {0, 1, ..., n_actions-1}.\n",
        "\n",
        "    Args:\n",
        "      input_dim: Observation dimensionality.\n",
        "      n_actions: Number of actions in environment.\n",
        "    \"\"\"\n",
        "    super().__init__()\n",
        "    self.linear1 = nn.Linear(input_dim, 64)\n",
        "    self.linear2 = nn.Linear(64, 64)\n",
        "    self.linear3 = nn.Linear(64, n_actions)\n",
        "\n",
        "  def forward(self, ob) -> Distribution:\n",
        "    \"\"\"Returns a distribution over this policy's action set.\n",
        "    \"\"\"\n",
        "    if isinstance(ob, np.ndarray):\n",
        "      ob = torch.Tensor(ob)\n",
        "    x = nn.functional.relu(self.linear1(ob))\n",
        "    x = nn.functional.relu(self.linear2(x))\n",
        "    logits = self.linear3(x)\n",
        "    return torch.distributions.Categorical(logits=logits)"
      ],
      "metadata": {
        "id": "wtTcQJVQiiEc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### (c) Setting Up Behavioral Cloning\n",
        "\n",
        "For this problem, we will be defining a standard training loop to perform behavioral cloning. **In the following code cell, write a function which takes in:**\n",
        "\n",
        "* **a policy `policy` of class `DiscretePolicy` to be trained,**\n",
        "* **a dataset `dataset` for imitation learning,**\n",
        "* **number of training steps `n_steps`,**\n",
        "* **and batch size `batch_size`,**\n",
        "\n",
        "**and returns the resulting policy after performing training according to the given parameters. Please clearly specify the format in which you expect the dataset to take within the docstring.**\n",
        "\n"
      ],
      "metadata": {
        "id": "i5rzrXjpmLsJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_policy_by_bc(policy, dataset, n_steps, batch_size) -> DiscretePolicy:\n",
        "  \"\"\"Trains the provided policy by behavioral cloning, by taking n_steps training steps with the\n",
        "  provided optimizer. During training, training batches of size batch_size are sampled from the dataset\n",
        "  to compute the loss.\n",
        "\n",
        "  Args:\n",
        "    policy: policy of class DiscretePolicy.\n",
        "    dataset: The dataset, represented as a dictionary containing keys \"obs\"\n",
        "      and \"actions\", mapping to arrays with observations and corresponding actions to\n",
        "      clone, respectively. Arrays are of shape [dataset_size, ob_dim] and [dataset_size].\n",
        "    n_steps: Number of training steps to take.\n",
        "    batch_size: Size of the sampled batch for each training step.\n",
        "\n",
        "  Returns:\n",
        "    A policy trained according to the parameters above.\n",
        "  \"\"\"\n",
        "  dataset_size = len(dataset[\"obs\"])\n",
        "  optimizer = torch.optim.Adam(policy.parameters(), lr=0.001)\n",
        "  loss = nn.CrossEntropyLoss()\n",
        "  for _ in range(n_steps):\n",
        "    # Sample batch\n",
        "    batch_idxs = np.random.randint(dataset_size, size=(batch_size))\n",
        "    batch_ob = torch.Tensor(dataset[\"obs\"][batch_idxs])\n",
        "    batch_ac = torch.Tensor(dataset[\"actions\"][batch_idxs]).type(torch.LongTensor)\n",
        "\n",
        "    # Zero out optimizer gradients\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Get predictions\n",
        "    action_dists = policy(batch_ob)\n",
        "\n",
        "    # Compute loss and backpropagate gradients\n",
        "    mean_loss = -torch.mean(action_dists.log_prob(batch_ac))\n",
        "    mean_loss.backward()\n",
        "\n",
        "    # Alternative way to compute the loss\n",
        "    # mean_loss = loss(actions_dists.logits, batch_ac)\n",
        "\n",
        "    # Adjust weights\n",
        "    optimizer.step()\n",
        "\n",
        "  return policy"
      ],
      "metadata": {
        "id": "0rsDbZo4pa9b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### (d) Behavioral Cloning and Evaluation\n",
        "\n",
        "The time has come for Flappy Jr. to learn from Flappy Sr.'s notes! **Make use of the provided dataset `flappy_sr_notes.mat` and obtain a policy for Flappy Jr. via behavioral cloning. Upon training, evaluate the policy using a discount of 0.999, and verify that it performs much better than selecting actions uniformly at random.** Remember to upload the provided dataset using the folder tab on the left to be able to access it!\n",
        "\n",
        "Hint 1: You can use `scipy.io.loadmat` to load `.mat` files. Note that uploading may take some time, and `loadmat()` will error out while this process is incomplete; you can check the progress of the upload at the bottom of the folder tab to the left.  \n",
        "Hint 2: Upon uploading the file, you can `right-click > Copy path` to get the path to the file within the Colab server.  \n",
        "Hint 3: `flappy_sr_notes.mat` contains a record of Flappy Sr.'s whole life: every one of his rollouts is contained within the dataset in sequential order. The keys available in the dataset are `observations`, `actions`, `rewards`, `next_observations`, and `is_rollout_start` (indicates whether each index is the start of a new rollout). You may not need all keys for behavioral cloning!"
      ],
      "metadata": {
        "id": "5PmL9w8Kp64T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.io import loadmat\n",
        "\n",
        "# Load dataset\n",
        "dataset = loadmat(\"/content/flappy_sr_notes.mat\")\n",
        "dataset = {\n",
        "    \"obs\": dataset[\"observations\"],\n",
        "    \"actions\": dataset[\"actions\"][:, 0]\n",
        "}\n",
        "\n",
        "# Train policy by BC\n",
        "policy = DiscretePolicy(env.observation_space.shape[0], env.action_space.n)\n",
        "policy = train_policy_by_bc(policy, dataset, 20000, 64)\n",
        "\n",
        "# Evaluate\n",
        "returns = [evaluate_policy(env, policy, 0.999) for _ in range(50)]\n",
        "print(np.mean(returns))"
      ],
      "metadata": {
        "id": "ikJbbq_Dp50G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### (e) Optional (but Fun!): Visualizing Your Learned Policy Within the Game\n",
        "\n",
        "Want to see Flappy Jr. majestically soar through the skies? We sure do! We have written for you a convenience function for visualizing a sequence of images as an animation right here within the Colab environment. **All you need to do is to copy your sampling code here from Problem 1(a) and slightly modify it to instead return a sequence of rendered images of every timestep from a rollout.** The generated animation will be saved in the folder tab to the left where you uploaded the dataset.\n",
        "\n",
        "Hint: At any point, you can call `render(mode=\"rgb_array\")` on a `gym` environment to obtain an RGB image of the current state.\n",
        "Hint: You may need to try sampling a few rollouts!"
      ],
      "metadata": {
        "id": "cl--j0bEpm9L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def rollout_and_render(env, policy) -> list[np.ndarray]:\n",
        "  \"\"\"Returns a rendering of a single rollout under the provided policy.\n",
        "\n",
        "  Args:\n",
        "    env: OpenAI gym environment following old API.\n",
        "    policy: Callable representing a policy.\n",
        "\n",
        "  Returns:\n",
        "    A list of RGB images (as numpy arrays) from a single rollout.\n",
        "  \"\"\"\n",
        "  cur_ob, done, n_steps = env.reset(), False, 0\n",
        "  images = [env.render(mode=\"rgb_array\")]\n",
        "  while not done:\n",
        "    action_dist = policy(cur_ob)\n",
        "    action = action_dist.sample()\n",
        "    cur_ob, reward, done, _ = env.step(action)\n",
        "    images.append(env.render(mode=\"rgb_array\"))\n",
        "  return images\n",
        "\n",
        "### VISUALIZE POLICY HERE!\n",
        "\n",
        "img_lst = rollout_and_render(env, policy)\n",
        "\n",
        "### DO NOT MODIFY ANYTHING BELOW THIS POINT\n",
        "ani = animate_images(img_lst)\n",
        "FFwriter = animation.FFMpegWriter(fps=30)\n",
        "ani.save('animation.mp4', writer=FFwriter)"
      ],
      "metadata": {
        "id": "WGPZYhugPBMa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Problem 2: Floppy the Sloppy Ruins (?) the Day (AKA An Introduction to Filtered Behavioral Cloning)"
      ],
      "metadata": {
        "id": "Y-5BUGGbrWTQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Flappy Jr. happily went to sleep, satisfied that he has had such a succesful day of flying through the skies imitating Flappy Sr. Alas, he is woken up by some strange noise in the middle of the night, and immediately realizes that someone has run off with their learned model weights. Calmly, Flappy Jr. tells himself that Flappy Sr.'s notes are just in the next room, and that they can always do behavioral cloning again. Flappy Jr. walks into the next room and is horrified to see that the notebook being scribbled over by his nemesis, Floppy the Sloppy. Upon being discovered, Floppy the Sloppy flies off awkwardly flailing through the skies, laughing while they continue their awkward motion one could hardly consider as flying."
      ],
      "metadata": {
        "id": "nWvjAN6CrhuB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### (a): What is Left???\n",
        "\n",
        "Unfortunately, Floppy the Sloppy has uncharacteristically good penmanship, and his cannot be distinguished from that of Flappy Sr. As a result, we now only have a vandalized dataset `vandalized_notes.mat` consisting of the combined trajectories of Floppy the Sloppy and Flappy Sr. which are indistinguishable from each other at first glance. **For this problem, try applying behavioral cloning to train a new policy as you have done before, only this time using `vandalized-notes.mat.** How does the performance compare to your policy from Problem 1(d)?"
      ],
      "metadata": {
        "id": "GaVPJw6kIaVn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.io import loadmat\n",
        "\n",
        "dataset = loadmat(\"/content/vandalized_notes.mat\")\n",
        "dataset = {\n",
        "    \"obs\": dataset[\"observations\"],\n",
        "    \"actions\": dataset[\"actions\"][:, 0]\n",
        "}\n",
        "\n",
        "# Train policy by BC\n",
        "policy = DiscretePolicy(env.observation_space.shape[0], env.action_space.n)\n",
        "policy = train_policy_by_bc(policy, dataset, 20000, 64)\n",
        "\n",
        "returns = [evaluate_policy(env, policy, 0.999) for _ in range(50)]\n",
        "print(np.mean(returns))"
      ],
      "metadata": {
        "id": "oZuN-ylyKaUK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Short Answer: What does this experiment tell you about running behavioral cloning on noisy/low-quality datasets? Intuitively, why does this happen?**\n",
        "\n",
        "Hint: Think about what the BC loss is optimizing."
      ],
      "metadata": {
        "id": "Px-5Rzw_HAs0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The quality of the cloned policy is highly dependent on the dataset, since the policy is being optimized using mean-squared error, and can thus only perform as well as the policy that generated the dataset."
      ],
      "metadata": {
        "id": "8TqihdoNHHoU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### (b): Array of Hope???\n",
        "\n",
        "While deep in thought, Flappy Jr. had a sudden epiphany; he has access to reward data! He can then weigh the importance of imitating a particular example by looking at the quality of the outcome from performing a particular action.\n",
        "\n",
        "This strategy, where one reweighs BC examples by some predefined notion of quality, is generally referred to as **filtered behavioral cloning**. More formally, assume that we have access to a BC dataset $\\{(s_1, a_1), \\dots, (s_N, a_N)\\}$. Furthermore, assume that we have pre-defined weights $w_1, \\dots, w_N$ for each of the $N$ examples. Then, we can consider a _reweighed BC loss_\n",
        "\n",
        "$$L_w(\\pi) = \\frac{1}{N}\\sum_{i = 1}^{N}w_iL(\\pi(s_i), a_i),$$\n",
        "\n",
        "where $L$ is the standard per-example BC loss (e.g. cross-entropy loss for discrete actions). If the weights represent a notion of quality, one can interpret the loss above as performing filtering to ensure that only high quality examples are being imitated.\n",
        "\n",
        "**For this problem, you will fill in the skeleton below to implement a training loop based on filtered BC. Firstly, implement the reweighed BC loss given above. Secondly, modify the BC loop you implemented before to make use of this new loss function.**"
      ],
      "metadata": {
        "id": "XQEw9avBKoCl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ReweighedBCLossI(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.base_loss = nn.CrossEntropyLoss(reduction='none')   # reduction='none' is very important here!\n",
        "\n",
        "  def forward(self, batch_predictions, batch_targets, batch_weights):\n",
        "    return torch.dot(self.base_loss(batch_predictions, batch_targets), batch_weights)\n",
        "\n",
        "# Alternative implementation that passes in the action distribution directly\n",
        "class ReweighedBCLossII(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "  def forward(self, batch_predictions, batch_targets, batch_weights):\n",
        "    return -torch.dot(batch_predictions.log_prob(batch_targets), batch_weights)"
      ],
      "metadata": {
        "id": "jm2LYrx9RA6i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_policy_by_filtered_bc(policy, dataset, weights, n_steps, batch_size) -> DiscretePolicy:\n",
        "  \"\"\"Trains the provided policy by filtered behavioral cloning, by taking n_steps training steps with the\n",
        "  provided optimizer with the provided weights. During training, training batches of size batch_size are sampled from the dataset\n",
        "  to compute the loss.\n",
        "\n",
        "  Args:\n",
        "    policy: policy of class DiscretePolicy.\n",
        "    dataset: The dataset, represented as a dictionary containing keys \"obs\"\n",
        "      and \"actions\", mapping to arrays with observations and corresponding actions to\n",
        "      clone, respectively. Arrays are of shape [dataset_size, ob_dim] and [dataset_size].\n",
        "    weights: Weights used in the filtered BC loss.\n",
        "    optimizer: An instance of torch.optim.Optimizer.\n",
        "    n_steps: Number of training steps to take.\n",
        "    batch_size: Size of the sampled batch for each training step.\n",
        "\n",
        "  Returns:\n",
        "    A policy trained according to the parameters above.\n",
        "  \"\"\"\n",
        "  dataset_size = len(dataset[\"obs\"])\n",
        "  bc_loss1 = ReweighedBCLossI()   # Having both here to demonstrate how to use both implementations above\n",
        "  bc_loss2 = ReweighedBCLossII()\n",
        "  optimizer = torch.optim.Adam(policy.parameters(), lr=0.001)\n",
        "  for _ in range(n_steps):\n",
        "    # Sample dataset\n",
        "    batch_idxs = np.random.randint(dataset_size, size=(batch_size,))\n",
        "    batch_ob = torch.Tensor(dataset[\"obs\"][batch_idxs])\n",
        "    batch_ac = torch.Tensor(dataset[\"actions\"][batch_idxs]).type(torch.LongTensor)\n",
        "    batch_weights = torch.Tensor(weights[batch_idxs])\n",
        "\n",
        "    # Zero out optimizer gradients\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Get predictions\n",
        "    action_dists = policy(batch_ob)\n",
        "\n",
        "    # Compute loss and backpropagate gradients\n",
        "    mean_loss1 = bc_loss1(action_dists.logits, batch_ac, batch_weights)    # Implementation I\n",
        "    mean_loss2 = bc_loss2(action_dists, batch_ac, batch_weights)           # Implementation II\n",
        "    mean_loss2.backward()                                                  # Gonna use second here, but both should work.\n",
        "\n",
        "    # Adjust weights\n",
        "    optimizer.step()\n",
        "\n",
        "  return policy"
      ],
      "metadata": {
        "id": "TjRrH-RNTgYG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### (c) Filtering Strategy I: Trajectory-Level Reweighing???\n",
        "\n",
        "For this problem, we will try a simple strategy: all of the $(s, a)$ pairs obtained from one trajectory will have the same weight, determined by a function of the trajectory return. To formally define this weighing scheme, let $\\tau_1, \\tau_2, \\dots, \\tau_M$ denote the trajectories found in the dataset. Then, for any $(s, a)$ contained in $\\tau_i$, the corresponding weight $w$ is given by\n",
        "\n",
        "$$w = \\text{Softmax}_i\\left[\\frac{1}{\\alpha}R(\\tau_1)\\ , \\frac{1}{\\alpha}R(\\tau_2)\\ , \\dots\\ , \\frac{1}{\\alpha}R(\\tau_m)\\right],$$\n",
        "\n",
        "where $R(\\tau_i)$ is the discounted return of trajectory $i$ and $\\alpha > 0$ is a hyperparameter referred to as the _temperature_."
      ],
      "metadata": {
        "id": "ZNskDXL_UfM4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Implement the function which computes the weights as defined above in the cell below, and performed filtered behavioral cloning with the computed weights.** Feel free to tune the temperature as necessary. How does the trained policy compare to that of the earlier policies in Problems 1(d) and 2(a)?\n",
        "\n",
        "Hint: Remember that the dataset includes an array `is_rollout_start` indicating whether each datapoint is the start of a new trajectory."
      ],
      "metadata": {
        "id": "eNKCudD6Em00"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.special import softmax\n",
        "\n",
        "def trajectory_level_return_weights(dataset, temp, discount) -> np.ndarray:\n",
        "  \"\"\"Computes an array of weights for each point in the provided dataset according to the trajectory-level reweighing scheme.\n",
        "\n",
        "  Args:\n",
        "    dataset: Input dataset.\n",
        "    temp: Temperature used in softmax.\n",
        "    discount: Discount used to compute return.\n",
        "\n",
        "  Returns:\n",
        "    An array of weights for each BC datapoint in the dataset.\n",
        "  \"\"\"\n",
        "  # Useful constants\n",
        "  n_trajs = np.count_nonzero(dataset[\"is_rollout_start\"])\n",
        "  n_pts = len(dataset[\"is_rollout_start\"])\n",
        "\n",
        "  # Compute returns for each trajectory\n",
        "  traj_returns = [0 for _ in range(n_trajs)]\n",
        "  traj_idx = -1\n",
        "  cur_traj_len = 0\n",
        "  for idx in range(n_pts):\n",
        "    if dataset[\"is_rollout_start\"][idx]:\n",
        "      traj_idx += 1\n",
        "      cur_traj_len = 0\n",
        "    traj_returns[traj_idx] += (discount ** cur_traj_len) * dataset[\"rewards\"][idx]\n",
        "    cur_traj_len += 1\n",
        "\n",
        "  # Compute softmax weights\n",
        "  traj_weights = softmax(np.array(traj_returns) / temp)\n",
        "\n",
        "  # Assign to each datapoint\n",
        "  traj_idx = -1\n",
        "  final_weights = [0 for _ in range(n_pts)]\n",
        "  for idx in range(n_pts):\n",
        "    if dataset[\"is_rollout_start\"][idx]:\n",
        "      traj_idx += 1\n",
        "    final_weights[idx] = traj_weights[traj_idx]\n",
        "\n",
        "  return np.array(final_weights)"
      ],
      "metadata": {
        "id": "ApVKsC9w4ezF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = loadmat(\"/content/vandalized_notes.mat\")\n",
        "dataset = {\n",
        "    \"obs\": dataset[\"observations\"],\n",
        "    \"actions\": dataset[\"actions\"][:, 0],\n",
        "    \"rewards\": dataset[\"rewards\"][:, 0],\n",
        "    \"is_rollout_start\": dataset[\"is_rollout_start\"][:, 0]\n",
        "}\n",
        "weights = trajectory_level_return_weights(dataset, 3.0, 0.999)\n",
        "\n",
        "# Train policy by Filtered BC\n",
        "policy = DiscretePolicy(env.observation_space.shape[0], env.action_space.n)\n",
        "policy = train_policy_by_filtered_bc(policy, dataset, weights, 20000, 64)\n",
        "\n",
        "returns = [evaluate_policy(env, policy, 0.999) for _ in range(50)]\n",
        "print(np.mean(returns))"
      ],
      "metadata": {
        "id": "sjxtmbjeICST"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Short Answer: Why does the choice of weighing function work here? How does it affect what the behavioral cloning loss is doing?**"
      ],
      "metadata": {
        "id": "YLygI3gm3X21"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since we are working here under the assumption that Floppy the Sloppy is a worse flier (and thus achieves worse return), the weighing scheme above will focus on cloning behaviors from trajectories from Flappy Sr. which have higher returns.\n",
        "\n"
      ],
      "metadata": {
        "id": "ih-skNSV3nx1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Short Answer: What is the effect of the temperature on the weighing scheme?**\n",
        "\n",
        "Hint: As a starting point, think about what happens to the softmax function as $\\alpha \\to 0$ (to make it even easier to think about, consider applying the softmax to two fixed values $a, b$ with $a > b$ as you do this)."
      ],
      "metadata": {
        "id": "G93L_6PM4KBG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As $\\alpha \\to 0$, the softmax function places more and more weight on cloning the high-return trajectories, becoming more and more like a hard maximum. For very large $\\alpha$, trajectories are weighed equally."
      ],
      "metadata": {
        "id": "8pDPvdPV4Q-j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Short Answer: One could consider a version of this reweighing scheme where we remove the softmax and simply define the weight for $\\tau_i$ as $R(\\tau_i)$. What are the potential pitfalls of such a scheme?**\n",
        "\n",
        "Hint: Think about the values the reward function could take in all kinds of environments."
      ],
      "metadata": {
        "id": "PZys54zQKtyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that in some environments, the return might be negative if the reward function can take negative values, resulting in a nonsensical weighing scheme.\n",
        "\n",
        "Furthermore, such a scheme is sensitive to the scale of rewards, while the softmax only takes into account the relative differences between different trajectories."
      ],
      "metadata": {
        "id": "YNv-fcldLcjz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### (d) Filtering Strategy II: Truncated Future Return???\n",
        "\n",
        "Another spark of insight just hit Flappy Jr.: rather than just defining weights at the level of trajectories, he can also weight each datapoint separately! Floppy's flying motions aren't that sloppy all the time after all, just like how a broken clock is correct twice a day. For any datapoint $(s, a)$ and its reward $r_0$, he looks at the reward $(r_1, \\dots, r_{T - 1})$ of the $T - 1$ following timesteps in the same trajectory\\* and computes\n",
        "\n",
        "$$R_{T}(s, a) := \\sum_{t = 0}^{T - 1}\\gamma^tr_t.$$\n",
        "\n",
        "Then, if the computed values are $R_1, \\dots, R_N$ for all points in the dataset, then the weight $w_i$ of the $i^{\\text{th}}$ point is given by\n",
        "\n",
        "$$w_i = N\\cdot \\text{Softmax}_i\\left[\\frac{1}{\\alpha}R_1\\ , \\frac{1}{\\alpha}R_2\\ , \\dots\\ , \\frac{1}{\\alpha}R_N\\right],$$\n",
        "\n",
        "where $\\alpha$ is the temperature hyperparameter (as in the previous part). Note that there are now two hyperparameters, the truncation horizon $T$, and the temperature $\\alpha$.\n",
        "\n",
        "\\* **If there are fewer than $T - 1$ timesteps after $(s, a)$ in the trajectory that $(s,a)$ belongs to, the remaining timestep rewards in the sum are set to $0$.**"
      ],
      "metadata": {
        "id": "IiSPXTGq5wnM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Implement the function which computes the weights as defined above in the cell below, and performed filtered behavioral cloning with the computed weights.** Feel free to tune the temperature as necessary. How does the trained policy compare to that of the earlier policies in Problems 1(d) and 2(a)?"
      ],
      "metadata": {
        "id": "Q3c8EbBlHJG1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.special import softmax\n",
        "\n",
        "def truncated_future_return_weights(dataset, truncation_horizon, temp, discount) -> np.ndarray:\n",
        "  \"\"\"Computes an array of weights for each point in the provided dataset according to the truncated future return reweighing scheme.\n",
        "\n",
        "  Args:\n",
        "    dataset: Input dataset.\n",
        "    truncation_horizon: How many timesteps to consider for computing future return.\n",
        "    temp: Temperature used in softmax.\n",
        "    discount: Discount used to compute return.\n",
        "\n",
        "  Returns:\n",
        "    An array of weights for each BC datapoint in the dataset.\n",
        "  \"\"\"\n",
        "  n_pts = len(dataset[\"is_rollout_start\"])\n",
        "\n",
        "  # Compute truncated returns from each datapoint\n",
        "  # Note: Not the most efficient implementation - can use numpy's sliding window function with some\n",
        "  #   clever transformation of a sliding window view of is_rollout_start array to vectorize this.\n",
        "  all_returns = [0 for _ in range(n_pts)]\n",
        "  for idx in range(n_pts):\n",
        "    for offset in range(truncation_horizon):\n",
        "      if offset > 0 and (idx + offset >= n_pts or dataset[\"is_rollout_start\"][idx + offset]):\n",
        "        break\n",
        "      all_returns[idx] += (discount ** offset) * dataset[\"rewards\"][idx + offset]\n",
        "\n",
        "  softmax_weights = softmax(np.array(all_returns) / temp)\n",
        "  return n_pts * softmax_weights"
      ],
      "metadata": {
        "id": "YiKkaRVNHHe6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.io import loadmat\n",
        "\n",
        "dataset = loadmat(\"/content/vandalized_notes.mat\")\n",
        "dataset = {\n",
        "    \"obs\": dataset[\"observations\"],\n",
        "    \"actions\": dataset[\"actions\"][:, 0],\n",
        "    \"rewards\": dataset[\"rewards\"][:, 0],\n",
        "    \"is_rollout_start\": dataset[\"is_rollout_start\"][:, 0]\n",
        "}\n",
        "weights = truncated_future_return_weights(dataset, 100, 3.0, 0.999)\n",
        "\n",
        "# Train policy by Filtered BC\n",
        "policy = DiscretePolicy(env.observation_space.shape[0], env.action_space.n)\n",
        "policy = train_policy_by_filtered_bc(policy, dataset, weights, 20000, 64)\n",
        "\n",
        "returns = [evaluate_policy(env, policy, 0.999) for _ in range(50)]\n",
        "print(np.mean(returns))"
      ],
      "metadata": {
        "id": "socBfHCRIy7C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Short Answer: Let us explore the effect of the hyperparameter $T$ on the weighing scheme. Give a succinct description of the weights when $T = 1$. Do you expect this to work well in general? Why or why not?**"
      ],
      "metadata": {
        "id": "gI2hxo6lIfsz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "When $T = 1$, each timestep is weighed by the corresponding reward at that timestep. This would not work well in general since this would result in \"myopic\" policies that only focus on immediate rewards rather than long-term returns."
      ],
      "metadata": {
        "id": "dTqP8VprJbGq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Short Answer: Can you think of a reason why one can set $T$ relatively small in Flappy Bird and still obtain decent performance?**\n",
        "\n",
        "Hint: Think about the structure of the problem the agent is trying to solve."
      ],
      "metadata": {
        "id": "lAacgrVZJufo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Small $T$ can work well for this environment due to the periodic nature of the game: as long as an action is helpful for clearing the next pipe, whatever happens in the future after that is not as important for determining whether an action is \"useful\"."
      ],
      "metadata": {
        "id": "v11Pva51Jsh1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### (e) Optional: Other Reweighing Schemes???\n",
        "\n",
        "Feel free to try implementing other reweighing schemes here! Options to try:\n",
        "\n",
        "1. Only training on the top $q\\%$ of trajectories ranked by return.\n",
        "2. Using the future return within the trajectory from the current timestep."
      ],
      "metadata": {
        "id": "uv0yRwQiOTmt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def my_reweighing_scheme(dataset, *args, **kwargs) -> np.ndarray:\n",
        "  \"\"\"Computes an array of weights for each point in the provided dataset according to the truncated future return reweighing scheme.\n",
        "\n",
        "  Args:\n",
        "    dataset: Input dataset.\n",
        "    *args: Replace with your scheme's hyperparameters.\n",
        "    **kwargs: Replace with your scheme's hyperparameters.\n",
        "\n",
        "  Returns:\n",
        "    An array of weights for each BC datapoint in the dataset.\n",
        "  \"\"\"\n",
        "  ### YOUR CODE HERE!\n",
        "  pass\n",
        "\n",
        "\n",
        "### YOUR CODE HERE: RUN FILTERED BC BELOW!"
      ],
      "metadata": {
        "id": "4dGOfDWePWy8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**You should run this assignment on Google Colab and use a GPU for fast results. Each algorithm should take ~ 10 mins.**\n",
        "\n",
        "Make sure to run every cell in the file in the correct order."
      ],
      "metadata": {
        "id": "pOqaqSFFcuUg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fUDWvikdkd2J"
      },
      "outputs": [],
      "source": [
        "!pip3 install swig > /dev/null 2>&1\n",
        "!pip3 uninstall box2d-py -y > /dev/null 2>&1\n",
        "!pip3 install box2d-py > /dev/null 2>&1\n",
        "!pip3 install box2d box2d-kengz > /dev/null 2>&1\n",
        "!apt install xvfb > /dev/null 2>&1\n",
        "!pip3 install pyvirtualdisplay > /dev/null 2>&1\n",
        "\n",
        "# Important: In this assignment we specify numpy and gym versions explicitly to fix a compatibility issue. See below.\n",
        "!pip install numpy==1.23 gym==0.25.0 > /dev/null 2>&1"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# IMPORTANT: Now numpy==1.23 is installed but is not activated in the current runtime. Therefore, NOW you need to RESTART runtime (Runtime -> Restart runtime (Ctrl+M)) so that desired numpy and gym versions are being used."
      ],
      "metadata": {
        "id": "EB6gBUkoFZqa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZlGiM8WGkzsi"
      },
      "outputs": [],
      "source": [
        "# Imports (after restarting runtime)\n",
        "\n",
        "import gym\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.distributions import Categorical\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "import sys\n",
        "from pyvirtualdisplay import Display\n",
        "from IPython import display as disp\n",
        "import copy\n",
        "from typing import Tuple\n",
        "%matplotlib inline\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# IMPORTANT: Now, let's verify that we have correct installations with command below. Make sure you see numpy==1.23 gym==0.25.0 before you can start with this notebook!"
      ],
      "metadata": {
        "id": "dXQ1uzk6Fhvg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\" Current numpy version: {np.__version__}, gym version: {gym.__version__}\")"
      ],
      "metadata": {
        "id": "B2j8c4xsFgNl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i4slgsZK2HNr"
      },
      "source": [
        "#  Actor-critic variants for continuous action space: DDPG & TD3\n",
        "\n",
        "In this assignment, we will introduce two modern RL algorithms that aim to tackle MDPs with continuous action space: **Deep Deterministic Policy Gradient** (DDPG) and **Twin Delayed DDPG** (TD3).\n",
        "\n",
        "The workflow for the rest of this assignment is as follows: we will initially implement DDPG  algorithms, and then evaluate their performance on the Gym environment ```MountainCarContinuous```. The implementation for TD3 is directly related to DDPG. By studying these two algorithms, you will gain insight into reinforcement learning in continuous action spaces and become familiar with various techniques in RL.\n",
        "\n",
        "## Question 1. Deep Deterministic Policy Gradient(DDPG)\n",
        "#### **1. Introduction: DDPG v.s. original actor critic**\n",
        "Regular actor-critic algorithms work well when we have only finite actions, as it is essentially learning a mapping from $\\mathcal{S}$ to $\\mathbb{R}^{\\mathcal{A}}$, which can be well-approximated by a neural network with an $\\mathcal{A}$-dimensional output.  However, when it comes to the case that we have infinite candidate actions, e.g. in the scenario of robotic control, how can we learn the optimal action given the current state?\n",
        "\n",
        "In the question, we introduce an actor-critic type algorithm called Deep Deterministic Policy Gradient (DDPG). DDPG maintains 4 neural networks: an estimation $Q_\\phi$ as the critic, a policy network $\\mu_\\theta: \\mathcal{S} \\rightarrow \\mathcal{A}$ as the actor, and two corresponding target networks, $Q_{\\phi'}$ and $\\mu_{\\theta'}$. By deterministic, we mean $\\mu$ does not include randomness, as it is hard to approximate a probability with a continuous support with a simple neural network.\n",
        "\n",
        " As an actor-critic style algorithm in continuous setting, DDPG has the following special properties:\n",
        "\n",
        "(1) **Actor update.** To adapt to continuous action space and deterministic policy, in DDPG, the actor is updated by the continuous version of policy gradient: $$\n",
        "\\Delta \\theta = \\eta\\cdot \\text{mean}_t \\big(\\nabla_a Q_\\phi(s_t, \\mu_\\theta(s_t)) \\cdot \\nabla_\\theta \\mu_\\theta (s_t)\\big),\n",
        "$$\n",
        "also refer to the lecture notes for details.\n",
        "\n",
        "(2) **Replay buffer & Exploration strategy.** To mitigate the catastrophic forgetting issue in actor-critic, DDPG \"borrows\" the concept of replay buffer from DQN, and correspondingly an exploratory strategy when choosing action. In every step, DDPG samples a $(s_t, a_t, s_{t+1}, r_t)$ from the replay buffer, and update the policy.\n",
        "\n",
        "Similar to the $\\epsilon$-greedy exploration in DQN, DDPG adds noise to the chosen action given by the actor to improve exploration: $$\n",
        "a_t = \\mu_\\theta(s_t) + \\epsilon_t,\n",
        "$$\n",
        "where $\\epsilon_t$ is a Gaussian noise.\n",
        "Then the agent receives the new next_state with reward, and stores it in the replay buffer.\n",
        "\n",
        "(3) **Target network & Critic update.** To mitigate inconsistency during temporal difference backups, DDPG borrows the concept of target network from double DQN. Specifically, in every episode, DDPG updates the critic $Q_\\phi$ by performing gradient descent with the following improved Bellman loss: $$\n",
        "L(\\phi) = \\text{mean}_t f(y_t, Q_\\phi(s_t,a_t)),\n",
        "$$\n",
        "where $y_t = r(s_t, a_t) + \\gamma \\cdot Q_{\\phi'}(s_{t+1}, \\mu_{\\theta'}(s_{t+1}))$. $f$ can either be $x^2$ or $\\ell_1$-smooth function. Note that $y_t$ is calculated by the target networks $Q_{\\phi'}$, and $Q_{\\phi'}$ should not be trained here! This would decrease inconsistency during training. However, different from DQN, which update the target network with a slower frequency,  DDPG chooses to update $Q_{\\phi'}$ and $\\mu_{\\theta'}$ in every episode but with a slower rate after updating $\\phi$ and $\\theta$:\n",
        "$$\n",
        "\\phi' = \\tau \\phi + (1 - \\tau) \\phi', \\qquad \\theta' = \\tau \\theta + (1 - \\tau) \\theta'.\n",
        "$$\n",
        "Usually $\\tau$ is set to be some small constant, e.g. 0.005, to prevent rapid updates to the target networks.\n",
        "\n",
        "Now you must be ready to implement DDPG by yourselves!\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We provide you with the following utility functions for this assignment: (1) the replay buffer for experience replay, and (2) a policy evaluation function which evaluates a policy with Monte Carlo."
      ],
      "metadata": {
        "id": "89mle70cfClJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1bX7EbSNod3a"
      },
      "outputs": [],
      "source": [
        "# Replay buffer\n",
        "class ReplayBuffer(object):\n",
        "\tdef __init__(self, state_dim, action_dim, max_size=int(1e6)):\n",
        "\t\tself.max_size = max_size\n",
        "\t\tself.ptr = 0\n",
        "\t\tself.size = 0\n",
        "\n",
        "\t\tself.state = np.zeros((max_size, state_dim))\n",
        "\t\tself.action = np.zeros((max_size, action_dim))\n",
        "\t\tself.next_state = np.zeros((max_size, state_dim))\n",
        "\t\tself.reward = np.zeros((max_size, 1))\n",
        "\t\tself.not_done = np.zeros((max_size, 1))\n",
        "\n",
        "\t\tself.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "\tdef add(self, state, action, next_state, reward, done):\n",
        "\t\tself.state[self.ptr] = state\n",
        "\t\tself.action[self.ptr] = action\n",
        "\t\tself.next_state[self.ptr] = next_state\n",
        "\t\tself.reward[self.ptr] = reward\n",
        "\t\tself.not_done[self.ptr] = 1. - done\n",
        "\n",
        "\t\tself.ptr = (self.ptr + 1) % self.max_size\n",
        "\t\tself.size = min(self.size + 1, self.max_size)\n",
        "\n",
        "\n",
        "\tdef sample(self, batch_size):\n",
        "\t\tind = np.random.randint(0, self.size, size=batch_size)\n",
        "\n",
        "\t\treturn (\n",
        "\t\t\ttorch.FloatTensor(self.state[ind]).to(self.device),\n",
        "\t\t\ttorch.FloatTensor(self.action[ind]).to(self.device),\n",
        "\t\t\ttorch.FloatTensor(self.next_state[ind]).to(self.device),\n",
        "\t\t\ttorch.FloatTensor(self.reward[ind]).to(self.device),\n",
        "\t\t\ttorch.FloatTensor(self.not_done[ind]).to(self.device)\n",
        "\t\t)\n",
        "\n",
        "# policy evaluation with Monte Carlo\n",
        "def eval_policy(policy, env_name, seed, eval_episodes=10):\n",
        "\teval_env = gym.make(env_name)\n",
        "\teval_env.seed(seed + 100)\n",
        "\n",
        "\tavg_reward = 0.\n",
        "\tfor _ in range(eval_episodes):\n",
        "\t\tstate, done = eval_env.reset(), False\n",
        "\t\twhile not done:\n",
        "\t\t\taction = policy.select_action(np.array(state))\n",
        "\t\t\tstate, reward, done, _= eval_env.step(action)\n",
        "\t\t\tavg_reward += reward\n",
        "\n",
        "\tavg_reward /= eval_episodes\n",
        "\n",
        "\tprint(\"---------------------------------------\")\n",
        "\tprint(f\"Evaluation over {eval_episodes} episodes: {avg_reward:.3f}\")\n",
        "\tprint(\"---------------------------------------\")\n",
        "\treturn avg_reward\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **2. Implementation: Create your own DDPG agent!**\n",
        "\n",
        "You now need to complete the following implementation of DDPG. First, you want to build your ``Actor_DDPG`` and ``Critic_DDPG`` networks. For our task, two three-layer neural network with ReLU activations suffice. Note that  ``Critic_DDPG`` approximates the Q-function, thus takes both the state and action as inputs. Here we assume the action space is a symmetric interval ``[- max_action, max_action]``, so please normalize your output with e.g. ``torch.tanh``."
      ],
      "metadata": {
        "id": "-yn9JbTCuG1h"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hn3nFpLqk3wR"
      },
      "outputs": [],
      "source": [
        "# Implementation of Deep Deterministic Policy Gradients (DDPG)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "class Actor_DDPG(nn.Module):\n",
        "\tdef __init__(self, state_dim: int, action_dim: int, max_action: torch.tensor):\n",
        "\t\tsuper(Actor_DDPG, self).__init__()\n",
        "\t  \"\"\"\n",
        "\t\tInputs:\n",
        "    state_dim: int, dimension of state\n",
        "\t\taction_dim: int, dimension of action\n",
        "\t\tOutputs:\n",
        "\t\ta torch.Tensor that represents future action\n",
        "\t\t\"\"\"\n",
        "\t\t# [HINT] Construct a neural network as the actor. Return its value using forward You need to write down three linear layers.\n",
        "\t\t# 1. l1: state_dim → 400\n",
        "\t\t# 2. l2: 400 → 300\n",
        "    # 3. l3: 300 → action_dim\n",
        "\t\t############################\n",
        "\t\t# YOUR IMPLEMENTATION HERE #\n",
        "\t\tpass\n",
        "\t\t############################\n",
        "\n",
        "\t\tself.max_action = max_action\n",
        "\n",
        "\tdef forward(self, state):\n",
        "\t\t# [HINT] Write down forward function with the following descriptions.\n",
        "\t\t# 1. Apply ReLU activation after layer l1\n",
        "\t\t# 2. Apply ReLU activation after layer l2\n",
        "\t\t# 3. Apply tanh activation after layer l3, then scale by self.max_action\n",
        "\t\t```\n",
        "\t\t############################\n",
        "\t\t# YOUR IMPLEMENTATION HERE #\n",
        "\t\tpass\n",
        "\t\t############################\n",
        "\n",
        "class Critic_DDPG(nn.Module):\n",
        "\tdef __init__(self, state_dim: int, action_dim: int):\n",
        "\t\tsuper(Critic_DDPG, self).__init__()\n",
        "\t  \"\"\"\n",
        "    Inputs:\n",
        "\t\tsame as the actor\n",
        "\t\tOutputs:\n",
        "\t\ttorch.Tensor that represents the Q-values\n",
        "\t\t\"\"\"\n",
        "\t\t# [HINT] Construct a neural network as the critic. Return its value using forward. You need to write down three linear layers.\n",
        "\t\t# 1. l1: state_dim+action_dim → 400\n",
        "\t\t# 2. l2: 400 → 300\n",
        "    # 3. l3: 300 → 1\n",
        "\t\t############################\n",
        "\t\t# YOUR IMPLEMENTATION HERE #\n",
        "\t\tpass\n",
        "\t\t############################\n",
        "\n",
        "\tdef forward(self, state: torch.Tensor, action: torch.Tensor)->torch.Tensor:\n",
        "\t\tsa = torch.cat([state, action], 1)\n",
        "\n",
        "\t\t# [HINT] Write down forward function with the following descriptions.\n",
        "\t\t# 1. Apply ReLU activation after layer l1\n",
        "\t\t# 2. Apply ReLU activation after layer l2\n",
        "\t\t# 3. Return output from layer l3\n",
        "\n",
        "\t\t############################\n",
        "\t\t# YOUR IMPLEMENTATION HERE #\n",
        "\t\tpass\n",
        "\t\t############################"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we are ready to construct a DDPG trainer! In the following cell you will need to:\n",
        "\n",
        "**(1)** Calculate the TD value using target_Q network and update the critic;\n",
        "\n",
        "**(2)** Calculate the deterministic policy gradient and update the actor;\n",
        "\n",
        "**(3)** Update the target networks."
      ],
      "metadata": {
        "id": "wgrUCZUv0Irw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DDPG(object):\n",
        "\tdef __init__(self, state_dim: int, action_dim: int, max_action: float, discount=0.99, tau=0.001):\n",
        "\t\t# Initialize the models and the target models\n",
        "\t\tself.actor = Actor_DDPG(state_dim, action_dim, max_action).to(device)\n",
        "\t\tself.actor_target = copy.deepcopy(self.actor)\n",
        "\t\tself.actor_optimizer = torch.optim.Adam(self.actor.parameters(), lr=3e-4)\n",
        "\n",
        "\t\tself.critic = Critic_DDPG(state_dim, action_dim).to(device)\n",
        "\t\tself.critic_target = copy.deepcopy(self.critic)\n",
        "\t\tself.critic_optimizer = torch.optim.Adam(self.critic.parameters(), lr=3e-4)\n",
        "\n",
        "\t\tself.discount = discount\n",
        "\t\tself.tau = tau\n",
        "\n",
        "\n",
        "\tdef select_action(self, state):\n",
        "\t\tstate = torch.FloatTensor(state.reshape(1, -1)).to(device)\n",
        "\t\treturn self.actor(state).cpu().data.numpy().flatten()\n",
        "\n",
        "\n",
        "\tdef train(self, replay_buffer, batch_size=64):\n",
        "\t\t# Sample from replay buffer\n",
        "\t\tstate, action, next_state, reward, not_done = replay_buffer.sample(batch_size)\n",
        "\n",
        "\t\t# Compute the target_Q value with critic_target for the batch\n",
        "\t\t# [HINT] target_Q = reward + discount * Q_target(next_state) * not_done\n",
        "\t\t# [HINT] Remember that we should not update the target Q here! Detach the TD target to prevent gradient backprop\n",
        "\n",
        "\t\t############################\n",
        "\t\t# YOUR IMPLEMENTATION HERE #\n",
        "\t\tpass\n",
        "    ############################\n",
        "\n",
        "\t\t# Get current Q estimate\n",
        "\t\tcurrent_Q = self.critic(state, action)\n",
        "\n",
        "\t\t# Compute critic loss\n",
        "\t\t# [HINT] It should be the an mse_loss or a mean of smooth_l1 function between current Q and target Q\n",
        "\t\t############################\n",
        "\t\t# YOUR IMPLEMENTATION HERE #\n",
        "\t\tcritic_loss =  None\n",
        "    ############################\n",
        "\n",
        "\t\t# Optimize the critic\n",
        "\t\tself.critic_optimizer.zero_grad()\n",
        "\t\tcritic_loss.backward()\n",
        "\t\tself.critic_optimizer.step()\n",
        "\n",
        "\t\t# Compute actor loss\n",
        "\t\t# [HINT] You can easily compute the loss by taking mean of critic(state, actor(state))\n",
        "\t\t############################\n",
        "\t\t# YOUR IMPLEMENTATION HERE #\n",
        "\t\tactor_loss = None\n",
        " \t\t############################\n",
        "\n",
        "\t\t# Optimize the actor\n",
        "\t\tself.actor_optimizer.zero_grad()\n",
        "\t\tactor_loss.backward()\n",
        "\t\tself.actor_optimizer.step()\n",
        "\n",
        "\t\t# Update the target models\n",
        "\t\t# [HINT] You should calculate the weighted mean of every parameters between critic and critic_target, and the weighted mean of actor and actor_target. Store their values in new_target_params.\n",
        "\t\t# [HINT] The weight for the weighted mean is self.tau\n",
        "\t\tfor param, target_param in zip(self.critic.parameters(), self.critic_target.parameters()):\n",
        "      ############################\n",
        "\t\t\t# YOUR IMPLEMENTATION HERE #\n",
        "      new_target_params = None\n",
        "\t\t\ttarget_param.data.copy_(new_target_params)\n",
        "\t \t\t############################\n",
        "\n",
        "    for param, target_param in zip(self.actor.parameters(), self.actor_target.parameters()):\n",
        "\t\t\t############################\n",
        "\t\t\t# YOUR IMPLEMENTATION HERE #\n",
        "      new_target_params = None\n",
        "\t\t\ttarget_param.data.copy_(new_target_params)\n",
        "\t \t\t############################"
      ],
      "metadata": {
        "id": "RfZWG4RL0H7F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **3. See how it works!**\n",
        "Now we are coming to the most exciting part--- try out DDPG and TD3 on ``MountainCarContinuous`` environment! The default hyperparameters are provided to you. Note that they apply for both DDPG and TD3 and should be able to work.\n",
        "\n",
        "To make the training process more stable, we will first randomly sample from the action space to fullfill our replay buffer. So don't panick if you find the reward in the innitial episodes is super low!\n",
        "\n",
        "If you find it helpful, you are welcome to change anything in the training process. But please keep in mind that performances are sensitive to training hyperparameters so be careful.\n",
        "\n"
      ],
      "metadata": {
        "id": "9QL9inx6Aw93"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JWg4ZB6XlrwR"
      },
      "outputs": [],
      "source": [
        "def init_flags():\n",
        "\n",
        "    flags = {\n",
        "        \"env\": \"MountainCarContinuous\",\n",
        "        \"seed\":0,\n",
        "        \"start_timesteps\": 25e3,\n",
        "        \"max_timesteps\": 8e4,\n",
        "        \"expl_noise\": 0.1,\n",
        "        \"batch_size\": 256,\n",
        "        \"discount\":0.99,\n",
        "        \"tau\": 0.005,\n",
        "        \"policy_noise\": 0.2,\n",
        "        \"noise_clip\":0.5,\n",
        "        \"policy_freq\": 2,\n",
        "        \"save_model\": \"store_true\"\n",
        "    }\n",
        "\n",
        "    return flags\n",
        "\n",
        "# The following main() function is provided to you. It can a run for both DDPG and TD3..\n",
        "def main(policy_name = 'DDPG'):\n",
        "\n",
        "    args = init_flags()\n",
        "    env = gym.make(args[\"env\"])\n",
        "    env.seed(args[\"seed\"] + 100)\n",
        "    env.action_space.seed(args[\"seed\"])\n",
        "    torch.manual_seed(args[\"seed\"])\n",
        "    np.random.seed(args[\"seed\"])\n",
        "\n",
        "    state_dim = env.observation_space.shape[0]\n",
        "    action_dim = env.action_space.shape[0]\n",
        "    max_action = float(env.action_space.high[0])\n",
        "    kwargs = {\n",
        "        \"state_dim\": state_dim,\n",
        "        \"action_dim\": action_dim,\n",
        "        \"max_action\": max_action,\n",
        "        \"discount\": args[\"discount\"],\n",
        "        \"tau\": args[\"tau\"],}\n",
        "    if policy_name == \"TD3\":\n",
        "        # Target policy smoothing is scaled wrt the action scale\n",
        "        kwargs[\"policy_noise\"] = args[\"policy_noise\"] * max_action\n",
        "        kwargs[\"noise_clip\"] = args[\"noise_clip\"] * max_action\n",
        "        kwargs[\"policy_freq\"] = args[\"policy_freq\"]\n",
        "        policy = TD3(**kwargs)\n",
        "    elif policy_name == \"DDPG\":\n",
        "        policy = DDPG(**kwargs)\n",
        "\n",
        "    replay_buffer = ReplayBuffer(state_dim, action_dim)\n",
        "    evaluations = [eval_policy(policy, args[\"env\"], args[\"seed\"])]\n",
        "    state, done = env.reset(), False\n",
        "    episode_reward = 0\n",
        "    episode_timesteps = 0\n",
        "    episode_num = 0\n",
        "\n",
        "    for t in range(int(args[\"max_timesteps\"])):\n",
        "\n",
        "      episode_timesteps += 1\n",
        "\n",
        "      # Select action randomly or according to policy\n",
        "      if t < args[\"start_timesteps\"]:\n",
        "        action = env.action_space.sample()\n",
        "      else:\n",
        "        action = (\n",
        "          policy.select_action(np.array(state))\n",
        "          + np.random.normal(0, max_action * args[\"expl_noise\"], size=action_dim)\n",
        "        ).clip(-max_action, max_action)\n",
        "\n",
        "      # Perform action\n",
        "      next_state, reward, done, _ = env.step(action)\n",
        "      done_bool = float(done) if episode_timesteps < env._max_episode_steps else 0\n",
        "\n",
        "      # Store data in replay buffer\n",
        "      replay_buffer.add(state, action, next_state, reward, done_bool)\n",
        "\n",
        "      state = next_state\n",
        "      episode_reward += reward\n",
        "\n",
        "      # Train agent after collecting sufficient data\n",
        "      if t >= args[\"start_timesteps\"]:\n",
        "        policy.train(replay_buffer, args[\"batch_size\"])\n",
        "\n",
        "      if done:\n",
        "        print(f\"Total T: {t+1} Episode Num: {episode_num+1} Episode T: {episode_timesteps} Reward: {episode_reward:.3f}\")\n",
        "\n",
        "        evaluations.append(episode_reward)\n",
        "\n",
        "        # Reset environment\n",
        "        state, done = env.reset(), False\n",
        "        episode_reward = 0\n",
        "        episode_timesteps = 0\n",
        "        episode_num += 1\n",
        "\n",
        "    return evaluations\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now run the following cell to see how it works! We would expect a reward that converges to around 90. The estimated wall time for running the whole process is around 10-20 minutes, and you should be able to see a large positive reward at around $8\\cdot 10^4$ timesteps. If the innitialization is unsuccessful, which could result in the reward being stuck at around $0$, try restart the ``main`` function or debug your trainer.\n",
        "\n",
        "Note: Even if your implementation has no errors, the performance may not be perfect every time, it is okay because there is randomness."
      ],
      "metadata": {
        "id": "VgnYEQx3FH9o"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nouYPAkXozYC"
      },
      "outputs": [],
      "source": [
        "evaluations_ddpg = main(policy_name = 'DDPG')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plot your reward v.s. training_episode curve. You can use the ``evaluations_ddpg`` above and ``plt.plot()``.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "BIYrmRtpDn-d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "############################\n",
        "# YOUR IMPLEMENTATION HERE #\n",
        "pass\n",
        "############################"
      ],
      "metadata": {
        "id": "9p--0NFnM99z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Question 3. TD3\n",
        "#### 1. Tackle over-estimation: Twin Delayed DDPG (TD3)\n",
        "Similar to DDPG, TD3 also maintains a continuous actor update with a deterministic policy gradient and a replay buffer with an exploratory strategy. The difference between TD3 and DDPG lies in the critic update. Unlike Double DQN, due to the slow update of the policy (the $\\theta$ in previous examples), the current and target values still remain similar even when using a double Q-update technique. While the implementation of an independent target network allows for less biased value estimation, even an unbiased estimate with high variance can still lead to future overestimations in local regions of state space, which in turn can negatively affect the global policy.\n",
        "\n",
        "To address this issue, TD3 updates the actor/critic network in the following way:\n",
        "\n",
        "**(1)** TD3 maintains **6 neural networks**: $Q_{\\phi_1}, Q_{\\phi_2}$ as two independent update for the Q function, $Q_{\\phi_1'}, Q_{\\phi_2'}$ as their corresponding target networks, and $\\mu_\\theta, \\mu_{\\theta'}$ as the actor and its target network. $Q_{\\phi_1}$ and $Q_{\\phi_2}$ are independently initialized, and all target networks are initialized to be the same as their corresponding counterparts. When updating Q-functions, in every epidsode, DDPG updates the critic $Q_\\phi$ by performing gradient descent with the following improved Bellamn loss: $$\n",
        "L(\\phi_i) = \\text{mean}_t f(y_t, Q_{\\phi_i}(s_t,a_t)),\n",
        "$$\n",
        "where $y_t = r(s_t, a_t) + \\gamma \\cdot \\min_{i= 1,2}Q_{\\phi_i'}(s_{t+1}, \\tilde{a}_{t+1})$, $\\tilde{a}_t =  \\mu_{\\theta'}(s_{t+1}) + \\text{clip}_{[-c,c]}(\\mathcal{N}(0,\\tilde{\\sigma}))$,  which is different from the TD target in DDPG, as it take the minimum of the two future Q-values and a \"disturbed\" future action. Here we clip $\\tilde{a}_t$ to prevent it from going too far. Intuitively, TD3 evaluates the Q-value of $s_{t+1}$ in a more \"conservative\" way.\n",
        "\n",
        "\n",
        "**(2)** When updating $\\mu_\\theta$, TD3 only utilizes $Q_{\\phi_1}$:$$\n",
        "\\Delta \\theta = \\eta\\cdot \\text{mean}_t \\big(\\nabla_a Q_{\\phi_1}(s_t, \\mu_\\theta(s_t)) \\cdot \\nabla_\\theta \\mu_\\theta (s_t)\\big).\n",
        "$$\n",
        "At the end of every episode, similar to DDPG, all parameters are updated by $$\n",
        "\\phi_i' = \\tau \\phi_i + (1 - \\tau) \\phi_i', \\qquad \\theta' = \\tau \\theta + (1 - \\tau) \\theta'.\n",
        "$$\n",
        "\n",
        "**(3)** The actor update is delayed: it only updates once every several times the critic updates.\n",
        "\n",
        "\n",
        "We understand the introduction above might be a little bit confusing due to the technical complexity. In the following, we will guide you end to end to implement a TD3 training algorithm.\n",
        "\n"
      ],
      "metadata": {
        "id": "PrWigU12Wd88"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####2. Implementation: Build your own TD3!\n",
        "\n",
        "In the following cell, we will implement the actor and critic network for TD3. For the actor and every critic (we need to maintain an additional critic), please make sure it has the same structure as the one in the previous DDPG question so that we can conduct an ablation study.\n",
        "\n",
        "Our implementation for the ``Critic_TD3`` class is slightly different from the previous critic in DDPG: the class function ``forward`` should return two values $q_1$ and $q_2$ given $(s,a)$, while the class function ``Q1`` should return only $q_1$."
      ],
      "metadata": {
        "id": "tpCc0fUfwldo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wJfEi1DHlbdD"
      },
      "outputs": [],
      "source": [
        "# Implementation of Twin Delayed Deep Deterministic Policy Gradients (TD3)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "class Actor_TD3(nn.Module):\n",
        "\tdef __init__(self, state_dim: int, action_dim: int, max_action: float):\n",
        "\t\tsuper(Actor_TD3, self).__init__()\n",
        "\t\t\"\"\"\n",
        "    Inputs: same as DDPG actor\n",
        "\t\tOutputs of forward: torch.Tensor that represents the chosen action\n",
        "\t\t\"\"\"\n",
        "\t\t# [HINT] Construct a neural network as the actor. Return its value using forward You need to write down three linear layers.\n",
        "\t\t# 1. l1: state_dim → 256\n",
        "\t\t# 2. l2: 256 → 256\n",
        "    # 3. l3: 256 → action_dim\n",
        "\t\t############################\n",
        "\t\t# YOUR IMPLEMENTATION HERE #\n",
        "\t\t# [HINT] make sure the structure is the same as DDPG\n",
        "\t\tpass\n",
        "\t\t############################\n",
        "\t\tself.max_action = max_action\n",
        "\n",
        "\n",
        "\tdef forward(self, state):\n",
        "\t\t# [HINT] Write down forward function with the following descriptions.\n",
        "\t\t# 1. Apply ReLU activation after layer l1\n",
        "\t\t# 2. Apply ReLU activation after layer l2\n",
        "\t\t# 3. Apply tanh activation after layer l3, then scale by self.max_action\n",
        "\t\t############################\n",
        "\t\t# YOUR IMPLEMENTATION HERE #\n",
        "\t\tpass\n",
        "    ############################\n",
        "\n",
        "\n",
        "class Critic_TD3(nn.Module):\n",
        "\tdef __init__(self, state_dim: int, action_dim: int):\n",
        "\t\tsuper(Critic_TD3, self).__init__()\n",
        "    \"\"\"\n",
        "    Inputs: same as the actor\n",
        "\t\tOutputs: two torch.Tensors that represent Q1 and Q2\n",
        "\t\t\"\"\"\n",
        "\t\t# Q1 architecture\n",
        "\t\t# [HINT] Construct a neural network as the actor. Return its value using forward You need to write down three linear layers.\n",
        "\t\t# 1. l1: state_dim+action_dim → 256\n",
        "\t\t# 2. l2: 256 → 256\n",
        "    # 3. l3: 256 → 1\n",
        "\t\t############################\n",
        "\t\t# YOUR IMPLEMENTATION HERE #\n",
        "\t\tpass\n",
        "\t\t############################\n",
        "\n",
        "\t\t# Q2 architecture\n",
        "\t\t# [HINT] Construct a neural network as the critic. Return its value using forward. You need to write down three linear layers.\n",
        "\t\t# 1. l4: state_dim+action_dim → 256\n",
        "\t\t# 2. l5: 256 → 256\n",
        "    # 3. l6: 256 → 1\n",
        "\t\t############################\n",
        "\t\t# YOUR IMPLEMENTATION HERE #\n",
        "\t\tpass\n",
        "\t\t############################\n",
        "\n",
        "\tdef forward(self, state: torch.Tensor, action: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "\t\tsa = torch.cat([state, action], 1)\n",
        "\n",
        "\t\t# [HINT] We use layers l1, l2, l3 to obtain q1\n",
        "\t\t# 1. Apply ReLU activation after layer l1\n",
        "\t\t# 2. Apply ReLU activation after layer l2\n",
        "\t\t# 3. Return output as q1 from layer l3\n",
        "\n",
        "\t\t# [HINT] We use layers l4, l5, l6 to obtain q2\n",
        "\t\t# 1. Apply ReLU activation after layer l4\n",
        "\t\t# 2. Apply ReLU activation after layer l5\n",
        "\t\t# 3. Return output as q2 from layer l6\n",
        "\n",
        "\t\t############################\n",
        "\t\t# YOUR IMPLEMENTATION HERE #\n",
        "\t\tpass\n",
        "\t\t############################\n",
        "\t\treturn q1, q2\n",
        "\n",
        "  # Implement a function that returns only Q1, which is helpful when calculating actor loss\n",
        "\tdef Q1(self, state: torch.Tensor, action: torch.Tensor) -> torch.Tensor:\n",
        "\t\tsa = torch.cat([state, action], 1)\n",
        "\t  # [HINT] only returns q1 for actor update using layers l1, l2, l3\n",
        "\t\t# 1. Apply ReLU activation after layer l1\n",
        "\t\t# 2. Apply ReLU activation after layer l2\n",
        "\t\t# 3. Return output as q1 from layer l3\n",
        "\t\t############################\n",
        "\t\t# YOUR IMPLEMENTATION HERE #\n",
        "\t\tpass\n",
        "\t  ############################\n",
        "\t\treturn q1\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's implement the TD3 trainer! In the following cell, you will need to implement the following:\n",
        "\n",
        "**(1)** For the critic update of TD3, when sampled a tuple from the replay buffer, recall that we need to estimate $Q(s_{t+1},\\tilde{a}_{t+1})$, where $\\tilde{a}_{t+1} = \\mu_{\\theta'}(s_{t+1})+\\text{clip}_{[-c,c]}(\\epsilon)$ .\n",
        "\n",
        "**(2)** Calculate the TD target with the networks $Q_{\\phi_i'}, i=1,2,$ and $(s_{t+1}, \\tilde{a}_{t+1})$ you obtained in **(1)**. Recall that the TD target = $r(s_t,a_t) + \\min_{i=1,2}\\{Q_{\\phi_i'}(s_{t+1}, \\tilde{a}_{t+1})\\}$.\n",
        "\n",
        "**(3)** Calculate the actor loss with $Q_{\\phi_1}$.\n",
        "\n",
        "**(4)** Update the parameters $\\phi_i'$, $\\theta'$."
      ],
      "metadata": {
        "id": "1yAS1Ue_07G3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TD3(object):\n",
        "\tdef __init__(\n",
        "\t\tself,\n",
        "\t\tstate_dim: int,\n",
        "\t\taction_dim: int,\n",
        "\t\tmax_action: float,\n",
        "\t\tdiscount=0.99,\n",
        "\t\ttau=0.005,\n",
        "\t\tpolicy_noise=0.2,\n",
        "\t\tnoise_clip=0.5,\n",
        "\t\tpolicy_freq=2\n",
        "\t):\n",
        "\n",
        "\t\tself.actor = Actor_TD3(state_dim, action_dim, max_action).to(device)\n",
        "\t\tself.actor_target = copy.deepcopy(self.actor)\n",
        "\t\tself.actor_optimizer = torch.optim.Adam(self.actor.parameters(), lr=3e-4)\n",
        "\n",
        "\t\tself.critic = Critic_TD3(state_dim, action_dim).to(device)\n",
        "\t\tself.critic_target = copy.deepcopy(self.critic)\n",
        "\t\tself.critic_optimizer = torch.optim.Adam(self.critic.parameters(), lr=3e-4)\n",
        "\n",
        "\t\tself.max_action = max_action\n",
        "\t\tself.discount = discount\n",
        "\t\tself.tau = tau\n",
        "\t\tself.policy_noise = policy_noise\n",
        "\t\tself.noise_clip = noise_clip\n",
        "\t\tself.policy_freq = policy_freq\n",
        "\n",
        "\t\tself.total_it = 0\n",
        "\n",
        "\n",
        "\tdef select_action(self, state):\n",
        "\t\tstate = torch.FloatTensor(state.reshape(1, -1)).to(device)\n",
        "\t\treturn self.actor(state).cpu().data.numpy().flatten()\n",
        "\n",
        "\n",
        "\tdef train(self, replay_buffer, batch_size=256):\n",
        "\t\tself.total_it += 1\n",
        "\n",
        "\t\t# Sample replay buffer\n",
        "\t\tstate, action, next_state, reward, not_done = replay_buffer.sample(batch_size)\n",
        "\n",
        "\t\twith torch.no_grad():\n",
        "\t\t\t# 1. Select action according to policy and add clipped noise.\n",
        "      # [HINT]: Return the action from the actor and add truncated noise.\n",
        "\t\t\t# [HINT]: The variance of the Gaussian noise is self.policy_noise for every dimension.\n",
        "\t\t\t# [HINT]: You can use ().clamp(-self.noise_clip, self.noise_clip) to clip the noise\n",
        "\t\t\t# Explanation: TD3 adds noise to the target policy to reduce overfitting and encourage exploration. This noise is clipped so the action remains near the intended policy output.\n",
        "      ############################\n",
        "      # YOUR IMPLEMENTATION HERE #\n",
        "\t\t\tpass\n",
        "      ############################\n",
        "\n",
        "\n",
        "\t\t\t# Compute the target Q value\n",
        "\t\t\ttarget_Q1, target_Q2 = self.critic_target(next_state, next_action)\n",
        "\n",
        "\t\t\t# 2. Compute the target_Q here\n",
        "\t\t\t# [HINT]: In TD3 we use min(Q_1,Q_2) as the pessimistic estimate of the next step Q.\n",
        "      ############################\n",
        "      # YOUR IMPLEMENTATION HERE #\n",
        "\t\t\tpass\n",
        "      ############################\n",
        "\n",
        "\t\t# Get current Q estimates\n",
        "\t\tcurrent_Q1, current_Q2 = self.critic(state, action)\n",
        "\n",
        "\t\t# Compute critic loss\n",
        "\t\tcritic_loss = F.mse_loss(current_Q1, target_Q) + F.mse_loss(current_Q2, target_Q)\n",
        "\n",
        "\t\t# Optimize the critic\n",
        "\t\tself.critic_optimizer.zero_grad()\n",
        "\t\tcritic_loss.backward()\n",
        "\t\tself.critic_optimizer.step()\n",
        "\n",
        "\t\t# Delayed policy updates\n",
        "\t\tif self.total_it % self.policy_freq == 0:\n",
        "\n",
        "\t\t\t# Compute actor loss\n",
        "\t\t\t# Hint: similar to DDPG actor loss, note we use Q1 here for optimization\n",
        "\t\t\t# Explanation: Q1 is used in the actor loss to guide the policy update. TD3 uses Q2 as a secondary critic to mitigate overestimation, but only Q1 is used for gradient updates.\n",
        "      ############################\n",
        "      # YOUR IMPLEMENTATION HERE #\n",
        "\t\t\tpass\n",
        "      ############################\n",
        "\n",
        "\t\t\t# Optimize the actor\n",
        "\t\t\tself.actor_optimizer.zero_grad()\n",
        "\t\t\tactor_loss.backward()\n",
        "\t\t\tself.actor_optimizer.step()\n",
        "\n",
        "\t\t\t# Update the frozen target models using weighted mean\n",
        "\t\t\t# [HINT]: the weight is given by self.tau\n",
        "\t\t\tfor param, target_param in zip(self.critic.parameters(), self.critic_target.parameters()):\n",
        "      \t############################\n",
        "      \t# YOUR IMPLEMENTATION HERE #\n",
        "\t\t\t\tnew_target_params = None\n",
        "\t\t\t\ttarget_param.data.copy_(new_target_params)\n",
        "\t\t\t\t############################\n",
        "\n",
        "\t\t\tfor param, target_param in zip(self.actor.parameters(), self.actor_target.parameters()):\n",
        "      \t############################\n",
        "\t\t\t\t# YOUR IMPLEMENTATION HERE #\n",
        "\t\t\t\tnew_target_params = None\n",
        "\t\t\t\ttarget_param.data.copy_(new_target_params)\n",
        "\t\t\t\t############################\n"
      ],
      "metadata": {
        "id": "9Lh09MrF022-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now run the following cell to see how TD3 works!"
      ],
      "metadata": {
        "id": "AjYk2RecQn_Y"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d_BG0sGGo3oY"
      },
      "outputs": [],
      "source": [
        "evaluations_td3 = main(policy_name = 'TD3')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plot your reward v.s. training_episode curve. You can use the ``evaluations_td3`` above and ``plt.plot()``.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "oCTipMM6Q_eg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "############################\n",
        "# YOUR IMPLEMENTATION HERE #\n",
        "pass\n",
        "############################"
      ],
      "metadata": {
        "id": "j72WNVo2RC87"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
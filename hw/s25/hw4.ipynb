{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ao1zxz7aDB3h"
      },
      "source": [
        "## Setup\n",
        "Make sure to run every single cell in this notebook, or some libraries might be missing. Also, if you are using Colab, make sure to **change your Runtime (change runtime type under Runtime)** to a GPU.\n",
        "\n",
        "Install the necessary libraries for rendering."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "2M_DloU_CtOI"
      },
      "outputs": [],
      "source": [
        "!apt-get install x11-utils > /dev/null 2>&1\n",
        "!pip install pyglet > /dev/null 2>&1\n",
        "!apt-get install -y xvfb python-opengl > /dev/null 2>&1\n",
        "\n",
        "!pip install gym pyvirtualdisplay > /dev/null 2>&1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Reinforcement Learning with Function Approximation\n",
        "In this notebook, we will implement three RL algorithms:\n",
        "1. **Q-Learning** (Off-policy TD control)\n",
        "2. **SARSA** (On-policy TD control)\n",
        "3. **Monte Carlo** (Episodic control)\n",
        "\n",
        "We will use a simple neural network to approximate Q(s, a) in a discrete-action environment (e.g., `CartPole-v1`).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1. Imports & Environment Setup\n",
        "import gym\n",
        "import numpy as np\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "# Create the environment\n",
        "env = gym.make('CartPole-v1')\n",
        "state_size = env.observation_space.shape[0]  # 4 for CartPole\n",
        "action_size = env.action_space.n  # 2 for CartPole\n",
        "\n",
        "# Set seeds for reproducibility (optional)\n",
        "env.seed(42)\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "random.seed(42)\n",
        "\n",
        "\n",
        "# rollout a random policy and visualize the results\n",
        "def rollout_random_policy(env, num_episodes=1):\n",
        "    for _ in range(num_episodes):\n",
        "        state = env.reset()\n",
        "        done = False\n",
        "        while not done:\n",
        "            action = env.action_space.sample()\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "            env.render()\n",
        "\n",
        "# rollout_random_policy(env, num_episodes=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hq8dOkRmV-_t"
      },
      "source": [
        "## Part 1: Implementing Q-network\n",
        "In this part, you will be filling out the code for a Q-network.\n",
        "We want a function that outputs a Q-value for each action, given the state.\n",
        "\n",
        "A 2-layer network with hidden dimension 64 is fine for this problem. Suggest to use Relu activation function.\n",
        "\n",
        "Fill in all sections labelled # FILL ME IN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mm0Lquc-qFQC"
      },
      "outputs": [],
      "source": [
        "# 2. Neural Network Definition\n",
        "# Simple 2-layer MLP that outputs Q-values for all actions given the state.\n",
        "\n",
        "class QNetwork(nn.Module):\n",
        "    # FILL ME IN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 2: Implementing the policy\n",
        "Implement the epsilon-greedy policy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def epsilon_greedy_policy(q_values, epsilon):\n",
        "    # FILL ME IN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 3: Implementing the algorithms\n",
        "\n",
        "We will create **separate training loops** for **Q-Learning**, **SARSA**, and **Monte-Carlo**.\n",
        "All three algorithms use a **neural network** to approximate \\( Q(s, a) \\), but they differ in how they compute their **targets**.\n",
        "\n",
        "### Q-Learning\n",
        "\n",
        "Update Rule (Per Time Step):\n",
        "\n",
        "Q-Learning is an **off-policy** algorithm that updates the Q-function using the **Bellman optimality equation**:\n",
        "\n",
        "$$\n",
        "y = r + \\gamma \\max_{a'} Q_{\\theta}(s', a')\n",
        "$$\n",
        "\n",
        "where:\n",
        "\n",
        "- $r$ is the reward obtained after taking action $ a $ in state $ s $,\n",
        "- $ \\gamma $ is the discount factor,\n",
        "- $ s' $ is the next state, and\n",
        "- $ \\max_{a'} Q_{\\theta}(s', a') $ is the maximum estimated Q-value for the next state.\n",
        "\n",
        "The **loss function** for Q-learning minimizes the squared difference between the **target value** $ y $ and the predicted Q-value $ Q_{\\theta}(s, a) $:\n",
        "\n",
        "$$\n",
        "L(\\theta) = \\left(y - Q_{\\theta}(s, a)\\right)^2\n",
        "$$\n",
        "\n",
        "where the parameters $ \\theta $ are updated using **gradient descent**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 4. Q-Learning Implementation\n",
        "\n",
        "def q_learning_update(network, optimizer, state, action, reward, next_state, done, gamma=0.99):\n",
        "    # The loss function for Q-learning minimizes the squared difference between the target value y and the predicted Q-value Q_{\\theta}(s, a):\n",
        "    # Take gradient descent step to update the network parameters.\n",
        "    # Return the loss.\n",
        "    # FILL ME IN\n",
        "\n",
        "def train_q_learning(env, network, optimizer, episodes=500, gamma=0.99,\n",
        "                     epsilon_start=1.0, epsilon_end=0.01, decay=500):\n",
        "    rewards_history = []\n",
        "    steps_done = 0\n",
        "    \n",
        "    for ep in range(episodes):\n",
        "        state = env.reset()\n",
        "        done = False\n",
        "        episode_reward = 0\n",
        "        \n",
        "        while not done:\n",
        "            epsilon = max(epsilon_end, epsilon_start * (1 - steps_done / decay))\n",
        "            \n",
        "            # Convert state to tensor and get Q-values\n",
        "            state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
        "            with torch.no_grad():  # Don't track gradients for action selection\n",
        "                q_values = network(state_tensor)\n",
        "            \n",
        "            # Get action from the epsilon-greedy policy\n",
        "            action = epsilon_greedy_policy(q_values, epsilon)\n",
        "            \n",
        "            next_state, reward, done, info = env.step(action)\n",
        "            loss = q_learning_update(network, optimizer, state, action, reward,\n",
        "                                     next_state, done, gamma)\n",
        "            \n",
        "            state = next_state\n",
        "            episode_reward += reward\n",
        "            steps_done += 1\n",
        "        \n",
        "        rewards_history.append(episode_reward)\n",
        "    \n",
        "    return rewards_history"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we train the Q-Learning algorithm."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Q-Learning\n",
        "q_network = QNetwork(state_size, action_size)\n",
        "q_optimizer = optim.Adam(q_network.parameters(), lr=1e-3)\n",
        "q_rewards = train_q_learning(env, q_network, q_optimizer, episodes=1000)\n",
        "\n",
        "# Plot learning curves\n",
        "plt.figure(figsize=(10,6))\n",
        "plt.plot(q_rewards, label='Q-Learning')\n",
        "plt.xlabel('Episode')\n",
        "plt.ylabel('Total Reward')\n",
        "plt.title('Comparison of Q-Learning, SARSA, and Monte Carlo')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### SARSA\n",
        "\n",
        "Update Rule (Per Time Step):\n",
        "\n",
        "SARSA (**State-Action-Reward-State-Action**) is an **on-policy** algorithm. Unlike Q-Learning, which uses the greedy **max** action for the next state, SARSA follows the same **policy** that is being learned (e.g., $\\epsilon$-greedy policy).\n",
        "\n",
        "The update rule for SARSA is:\n",
        "\n",
        "$$\n",
        "y = r + \\gamma Q_{\\theta}(s', a')\n",
        "$$\n",
        "\n",
        "The loss function remains the same:\n",
        "\n",
        "$$\n",
        "L(\\theta) = \\left(y - Q_{\\theta}(s, a)\\right)^2\n",
        "$$\n",
        "\n",
        "where the parameters $\\theta$ are updated via **gradient descent**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 5. SARSA Implementation\n",
        "\n",
        "def sarsa_update(network, optimizer, state, action, reward, next_state, next_action, done, gamma=0.99):\n",
        "    # The loss function for SARSA minimizes the squared difference between the target value y and the predicted Q-value Q_{\\theta}(s, a):\n",
        "    # Take gradient descent step to update the network parameters.\n",
        "    # Return the loss.\n",
        "    # FILL ME IN\n",
        "\n",
        "def train_sarsa(env, network, optimizer, episodes=500, gamma=0.99,\n",
        "                epsilon_start=1.0, epsilon_end=0.01, decay=500):\n",
        "    rewards_history = []\n",
        "    steps_done = 0\n",
        "    \n",
        "    for ep in range(episodes):\n",
        "        state = env.reset()\n",
        "        done = False\n",
        "        \n",
        "        # Choose initial action\n",
        "        epsilon = max(epsilon_end, epsilon_start * (1 - steps_done / decay))\n",
        "        q_values = network(torch.FloatTensor(state))\n",
        "        action = epsilon_greedy_policy(q_values, epsilon)\n",
        "        \n",
        "        episode_reward = 0\n",
        "        \n",
        "        while not done:\n",
        "            next_state, reward, done, info = env.step(action)\n",
        "            \n",
        "            epsilon = max(epsilon_end, epsilon_start * (1 - steps_done / decay))\n",
        "            \n",
        "            q_values = network(torch.FloatTensor(next_state))\n",
        "            next_action = epsilon_greedy_policy(q_values, epsilon)\n",
        "            \n",
        "            loss = sarsa_update(network, optimizer, state, action, reward,\n",
        "                                next_state, next_action, done, gamma)\n",
        "            \n",
        "            state = next_state\n",
        "            action = next_action\n",
        "            episode_reward += reward\n",
        "            steps_done += 1\n",
        "        \n",
        "        rewards_history.append(episode_reward)\n",
        "    \n",
        "    return rewards_history\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Train the SARSA algorithm."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# SARSA\n",
        "sarsa_network = QNetwork(state_size, action_size)\n",
        "sarsa_optimizer = optim.Adam(sarsa_network.parameters(), lr=1e-3)\n",
        "sarsa_rewards = train_sarsa(env, sarsa_network, sarsa_optimizer, episodes=1000)\n",
        "\n",
        "# Plot learning curves\n",
        "plt.figure(figsize=(10,6))\n",
        "plt.plot(sarsa_rewards, label='SARSA')\n",
        "plt.xlabel('Episode')\n",
        "plt.ylabel('Total Reward')\n",
        "plt.title('Comparison of Q-Learning, SARSA, and Monte Carlo')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Monte Carlo Regression\n",
        "\n",
        "Update Rule:\n",
        "\n",
        "Monte Carlo (MC) learning estimates **Q-values** by running full episodes and using the **observed returns** to update the function approximator. Unlike Q-learning or SARSA, which update at each time step, Monte Carlo updates only **at the end of an episode**.\n",
        "\n",
        "#### Steps:\n",
        "\n",
        "1. **Generate an entire episode**:\n",
        "\n",
        "   $$\n",
        "   (s_0, a_0, r_1, s_1, a_1, r_2, \\dots, s_{T-1}, a_{T-1}, r_T, s_T)\n",
        "   $$\n",
        "\n",
        "2. **For each state-action pair** \\( (s_t, a_t) \\), compute the return from that point onward:\n",
        "\n",
        "   $$\n",
        "   G_t = r_{t+1} + \\gamma r_{t+2} + \\dots + \\gamma^{T-t-1} r_T\n",
        "   $$\n",
        "\n",
        "   where:\n",
        "   - $ G_t $ is the **discounted return** from time step $ t $.\n",
        "   - $ \\gamma $ is the discount factor.\n",
        "   - $ T $ is the length of the episode.\n",
        "\n",
        "3. **Use gradient descent** to minimize the squared error between the estimated return \\( G_t \\) and the predicted Q-value \\( Q_{\\theta}(s_t, a_t) \\):\n",
        "\n",
        "   $$\n",
        "   L(\\theta) = \\left(G_t - Q_{\\theta}(s_t, a_t)\\right)^2\n",
        "   $$\n",
        "\n",
        "### First-Visit vs. Every-Visit Monte Carlo\n",
        "- **First-Visit MC**: Updates $ Q(s, a) $ **only the first time** each state-action pair is encountered in an episode.\n",
        "- **Every-Visit MC**: Updates $ Q(s, a) $ **every time** the state-action pair appears in the episode.\n",
        "\n",
        "In this implementation, we will use **every-visit MC** for simplicity.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 6. Monte Carlo Implementation\n",
        "\n",
        "def train_monte_carlo(env, network, optimizer, episodes=500, gamma=0.99,\n",
        "                      epsilon_start=1.0, epsilon_end=0.01, decay=500):\n",
        "    rewards_history = []\n",
        "    steps_done = 0\n",
        "    \n",
        "    for ep in range(episodes):\n",
        "        # Generate an entire episode\n",
        "        state = env.reset()\n",
        "        done = False\n",
        "        episode_experiences = []\n",
        "        \n",
        "        while not done:\n",
        "            epsilon = max(epsilon_end, epsilon_start * (1 - steps_done / decay))\n",
        "            # Implement the epsilon-greedy policy\n",
        "            q_values = network(torch.FloatTensor(state))\n",
        "            action = epsilon_greedy_policy(q_values, epsilon)\n",
        "            \n",
        "            next_state, reward, done, info = env.step(action)\n",
        "            episode_experiences.append((state, action, reward))\n",
        "            \n",
        "            state = next_state\n",
        "            steps_done += 1\n",
        "        \n",
        "        # Compute returns (G) for each state-action in the episode\n",
        "        G = 0\n",
        "        returns = []\n",
        "        for t in reversed(range(len(episode_experiences))):\n",
        "            s_t, a_t, r_t = episode_experiences[t]\n",
        "            G = r_t + gamma * G\n",
        "            returns.insert(0, G)\n",
        "        \n",
        "        episode_reward = sum([exp[2] for exp in episode_experiences])\n",
        "        rewards_history.append(episode_reward)\n",
        "        \n",
        "        # MC updates (every-visit)\n",
        "        for (s_t, a_t, _), G_t in zip(episode_experiences, returns):\n",
        "            state_t = torch.FloatTensor(s_t).unsqueeze(0)\n",
        "            q_values = network(state_t)\n",
        "            q_value = q_values[0, a_t]\n",
        "            \n",
        "            target = torch.tensor([G_t], dtype=torch.float32)\n",
        "            loss = (target - q_value)**2\n",
        "            \n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "    \n",
        "    return rewards_history\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Train the Monte Carlo algorithm."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Monte Carlo\n",
        "mc_network = QNetwork(state_size, action_size)\n",
        "mc_optimizer = optim.Adam(mc_network.parameters(), lr=1e-3)\n",
        "mc_rewards = train_monte_carlo(env, mc_network, mc_optimizer, episodes=1000)\n",
        "\n",
        "# Plot learning curves\n",
        "plt.figure(figsize=(10,6))\n",
        "plt.plot(mc_rewards, label='Monte Carlo')\n",
        "plt.xlabel('Episode')\n",
        "plt.ylabel('Total Reward')\n",
        "plt.title('Comparison of Q-Learning, SARSA, and Monte Carlo')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Comparing the Three Methods\n",
        "\n",
        "Please run the following cell to compare the three methods.\n",
        "Report the results and analysis in your writeup."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 7. Running & Comparing the Three Methods\n",
        "\n",
        "# Plot learning curves\n",
        "plt.figure(figsize=(10,6))\n",
        "plt.plot(q_rewards, label='Q-Learning')\n",
        "plt.plot(sarsa_rewards, label='SARSA')\n",
        "plt.plot(mc_rewards, label='Monte Carlo')\n",
        "plt.xlabel('Episode')\n",
        "plt.ylabel('Total Reward')\n",
        "plt.title('Comparison of Q-Learning, SARSA, and Monte Carlo')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "x",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

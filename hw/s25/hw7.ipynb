{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZlGiM8WGkzsi"
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.distributions import Normal\n",
        "import matplotlib.pyplot as plt\n",
        "import sys\n",
        "import copy\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i4slgsZK2HNr"
      },
      "source": [
        "We provide you with the following utility functions for this assignment: (1) the replay buffer for experience replay, and (2) a policy evaluation function which evaluates a policy with Monte Carlo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1bX7EbSNod3a"
      },
      "outputs": [],
      "source": [
        "# Replay buffer\n",
        "class ReplayBuffer(object):\n",
        "    def __init__(self, state_dim, action_dim, max_size=int(1e6)):\n",
        "        self.max_size = max_size\n",
        "        self.ptr = 0\n",
        "        self.size = 0\n",
        "\n",
        "        self.state = np.zeros((max_size, state_dim))\n",
        "        self.action = np.zeros((max_size, action_dim))\n",
        "        self.next_state = np.zeros((max_size, state_dim))\n",
        "        self.reward = np.zeros((max_size, 1))\n",
        "        self.not_done = np.zeros((max_size, 1))\n",
        "\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "    def add(self, state, action, next_state, reward, done):\n",
        "        self.state[self.ptr] = state\n",
        "        self.action[self.ptr] = action\n",
        "        self.next_state[self.ptr] = next_state\n",
        "        self.reward[self.ptr] = reward\n",
        "        self.not_done[self.ptr] = 1. - done\n",
        "\n",
        "        self.ptr = (self.ptr + 1) % self.max_size\n",
        "        self.size = min(self.size + 1, self.max_size)\n",
        "\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        ind = np.random.randint(0, self.size, size=batch_size)\n",
        "\n",
        "        return (\n",
        "            torch.FloatTensor(self.state[ind]).to(self.device),\n",
        "            torch.FloatTensor(self.action[ind]).to(self.device),\n",
        "            torch.FloatTensor(self.next_state[ind]).to(self.device),\n",
        "            torch.FloatTensor(self.reward[ind]).to(self.device),\n",
        "            torch.FloatTensor(self.not_done[ind]).to(self.device)\n",
        "        )\n",
        "\n",
        "# policy evaluation with Monte Carlo\n",
        "def eval_policy(policy, env_name, seed, eval_episodes=10):\n",
        "        eval_env = gym.make(env_name)\n",
        "        eval_env.reset(seed=seed)\n",
        "        avg_reward = 0.\n",
        "        for _ in range(eval_episodes):\n",
        "            state, _ = eval_env.reset()\n",
        "            done = False\n",
        "            step = 0\n",
        "            while not done:\n",
        "                action = policy.select_action(np.array(state))\n",
        "                state, reward, terminated, truncated, _ = eval_env.step(action)\n",
        "                avg_reward += reward\n",
        "                step += 1\n",
        "                done = terminated or truncated\n",
        "        avg_reward /= eval_episodes\n",
        "\n",
        "        print(\"---------------------------------------\")\n",
        "        print(f\"Evaluation over {eval_episodes} episodes: {avg_reward:.3f}\")\n",
        "        print(\"---------------------------------------\")\n",
        "        return avg_reward\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QklrnoACJgV5"
      },
      "source": [
        "# Twin Delayed Deep Deterministic Policy Gradients (TD3)\n",
        "\n",
        "\n",
        "Recall in last homework we have implemented a TD3 algorithm. You don't need to fill anything here for TD3, we use it as comparison with Soft Actor Critic (SAC) later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wJfEi1DHlbdD"
      },
      "outputs": [],
      "source": [
        "# Implementation of Twin Delayed Deep Deterministic Policy Gradients (TD3)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "class Actor_TD3(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim, max_action):\n",
        "        super(Actor_TD3, self).__init__()\n",
        "\n",
        "        self.l1 = nn.Linear(state_dim, 256)\n",
        "        self.l2 = nn.Linear(256, 256)\n",
        "        self.l3 = nn.Linear(256, action_dim)\n",
        "\n",
        "        self.max_action = max_action\n",
        "\n",
        "\n",
        "    def forward(self, state):\n",
        "        a = F.relu(self.l1(state))\n",
        "        a = F.relu(self.l2(a))\n",
        "        return self.max_action * torch.tanh(self.l3(a))\n",
        "\n",
        "\n",
        "class Critic_TD3(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim):\n",
        "        super(Critic_TD3, self).__init__()\n",
        "\n",
        "        # Q1 architecture\n",
        "        self.l1 = nn.Linear(state_dim + action_dim, 256)\n",
        "        self.l2 = nn.Linear(256, 256)\n",
        "        self.l3 = nn.Linear(256, 1)\n",
        "\n",
        "        # Q2 architecture\n",
        "        self.l4 = nn.Linear(state_dim + action_dim, 256)\n",
        "        self.l5 = nn.Linear(256, 256)\n",
        "        self.l6 = nn.Linear(256, 1)\n",
        "\n",
        "\n",
        "    def forward(self, state, action):\n",
        "        sa = torch.cat([state, action], 1)\n",
        "        q1 = F.relu(self.l1(sa))\n",
        "        q1 = F.relu(self.l2(q1))\n",
        "        q1 = self.l3(q1)\n",
        "\n",
        "        q2 = F.relu(self.l4(sa))\n",
        "        q2 = F.relu(self.l5(q2))\n",
        "        q2 = self.l6(q2)\n",
        "        return q1, q2\n",
        "\n",
        "\n",
        "    def Q1(self, state, action):\n",
        "        sa = torch.cat([state, action], 1)\n",
        "\n",
        "        q1 = F.relu(self.l1(sa))\n",
        "        q1 = F.relu(self.l2(q1))\n",
        "        q1 = self.l3(q1)\n",
        "        return q1\n",
        "\n",
        "\n",
        "class TD3(object):\n",
        "    def __init__(\n",
        "        self,\n",
        "        state_dim,\n",
        "        action_dim,\n",
        "        max_action,\n",
        "        discount=0.99,\n",
        "        tau=0.005,\n",
        "        policy_noise=0.2,\n",
        "        noise_clip=0.5,\n",
        "        policy_freq=2\n",
        "    ):\n",
        "\n",
        "        self.actor = Actor_TD3(state_dim, action_dim, max_action).to(device)\n",
        "        self.actor_target = copy.deepcopy(self.actor)\n",
        "        self.actor_optimizer = torch.optim.Adam(self.actor.parameters(), lr=3e-4)\n",
        "\n",
        "        self.critic = Critic_TD3(state_dim, action_dim).to(device)\n",
        "        self.critic_target = copy.deepcopy(self.critic)\n",
        "        self.critic_optimizer = torch.optim.Adam(self.critic.parameters(), lr=3e-4)\n",
        "\n",
        "        self.max_action = max_action\n",
        "        self.discount = discount\n",
        "        self.tau = tau\n",
        "        self.policy_noise = policy_noise\n",
        "        self.noise_clip = noise_clip\n",
        "        self.policy_freq = policy_freq\n",
        "        self.total_it = 0\n",
        "\n",
        "\n",
        "    def select_action(self, state):\n",
        "        state = torch.FloatTensor(state.reshape(1, -1)).to(device)\n",
        "        return self.actor(state).cpu().data.numpy().flatten()\n",
        "\n",
        "\n",
        "    def train(self, replay_buffer, batch_size=256):\n",
        "        self.total_it += 1\n",
        "\n",
        "        # Sample replay buffer\n",
        "        state, action, next_state, reward, not_done = replay_buffer.sample(batch_size)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            # Select action according to policy and add clipped noise\n",
        "            noise = (\n",
        "                torch.randn_like(action) * self.policy_noise\n",
        "            ).clamp(-self.noise_clip, self.noise_clip)\n",
        "\n",
        "            next_action = (\n",
        "                self.actor_target(next_state) + noise\n",
        "            ).clamp(-self.max_action, self.max_action)\n",
        "\n",
        "            # Compute the target Q value\n",
        "            target_Q1, target_Q2 = self.critic_target(next_state, next_action)\n",
        "\n",
        "            # Compute the target_Q here\n",
        "            target_Q = torch.min(target_Q1, target_Q2)\n",
        "            target_Q = reward + not_done * self.discount * target_Q\n",
        "\n",
        "        # Get current Q estimates\n",
        "        current_Q1, current_Q2 = self.critic(state, action)\n",
        "\n",
        "        # Compute critic loss\n",
        "        critic_loss = F.mse_loss(current_Q1, target_Q) + F.mse_loss(current_Q2, target_Q)\n",
        "\n",
        "        # Optimize the critic\n",
        "        self.critic_optimizer.zero_grad()\n",
        "        critic_loss.backward()\n",
        "        self.critic_optimizer.step()\n",
        "\n",
        "        # Delayed policy updates\n",
        "        if self.total_it % self.policy_freq == 0:\n",
        "\n",
        "            # Compute actor loss\n",
        "            actor_loss = -self.critic.Q1(state, self.actor(state)).mean()\n",
        "\n",
        "            # Optimize the actor\n",
        "            self.actor_optimizer.zero_grad()\n",
        "            actor_loss.backward()\n",
        "            self.actor_optimizer.step()\n",
        "\n",
        "            # Update the frozen target models\n",
        "            for param, target_param in zip(self.critic.parameters(), self.critic_target.parameters()):\n",
        "                target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n",
        "\n",
        "            for param, target_param in zip(self.actor.parameters(), self.actor_target.parameters()):\n",
        "                target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rHpWnqe2f_-7"
      },
      "source": [
        "# Soft Actor Critic (SAC)\n",
        "\n",
        "You will implement the SAC algorithm below.\n",
        "\n",
        "Including the actor and critic networks, and the training losses for the actor and critic.\n",
        "\n",
        "In this version, we do not use adaptive alpha, so there is no alpha loss or update."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9BDZI2r9xwhe"
      },
      "outputs": [],
      "source": [
        "LOG_STD_MIN = -20\n",
        "LOG_STD_MAX = 2\n",
        "epsilon = 1e-6\n",
        "\n",
        "class Actor_SAC(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim, max_action):\n",
        "        super(Actor_SAC, self).__init__()\n",
        "        # [HINT] Construct a neural network as the actor. Return its value using forward You need to write down three linear layers.\n",
        "        # 1. l1: state_dim → 256\n",
        "        # 2. l2: 256 → 256\n",
        "        # 3. l3: 256 → mean and log std of the action\n",
        "        ############################\n",
        "        # YOUR IMPLEMENTATION HERE #\n",
        "        pass\n",
        "        ############################\n",
        "        self.max_action = max_action\n",
        "\n",
        "    def forward(self, state):\n",
        "        # [HINT] Use the three linear layers to compute the mean and log std of the action\n",
        "        # Apply ReLU activation after layer l1 and l2\n",
        "        ############################\n",
        "        # YOUR IMPLEMENTATION HERE #\n",
        "        pass\n",
        "        ############################\n",
        "        log_std = torch.clamp(log_std, min=LOG_STD_MIN, max=LOG_STD_MAX)\n",
        "        return mean, log_std\n",
        "\n",
        "    def sample(self, state):\n",
        "        # [HINT] Use the forward method to compute the action, its log probability\n",
        "        # 1. Compute the mean and log std of the action\n",
        "        # 2. Compute the standard deviation of the action\n",
        "        # 3. Get the normal distribution of the action\n",
        "        # 4. Sample the action from the normal distribution\n",
        "        # 5. Apply tanh to the action and multiply by max_action to ensure the action is in the range of the action space\n",
        "        # 6. Compute the log probability of the action\n",
        "        \n",
        "        ############################\n",
        "        # YOUR IMPLEMENTATION HERE #\n",
        "        pass\n",
        "        ############################\n",
        "        return action, log_prob\n",
        "\n",
        "class Critic_SAC(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim):\n",
        "        super(Critic_SAC, self).__init__()\n",
        "        # Q1 architecture\n",
        "        # [HINT] Construct a neural network as the first critic. Return its value using forward You need to write down three linear layers.\n",
        "        # 1. l1: state_dim+action_dim → 256\n",
        "        # 2. l2: 256 → 256\n",
        "        # 3. l3: 256 → 1\n",
        "        ############################\n",
        "        # YOUR IMPLEMENTATION HERE #\n",
        "        pass\n",
        "        ############################\n",
        "\n",
        "        # Q2 architecture\n",
        "        # [HINT] Construct a neural network as the second critic. Return its value using forward. You need to write down three linear layers.\n",
        "        # 1. l4: state_dim+action_dim → 256\n",
        "        # 2. l5: 256 → 256\n",
        "        # 3. l6: 256 → 1\n",
        "        ############################\n",
        "        # YOUR IMPLEMENTATION HERE #\n",
        "        pass\n",
        "        ############################\n",
        "\n",
        "\n",
        "    def forward(self, state, action):\n",
        "        sa = torch.cat([state, action], 1)\n",
        "        # [HINT] We use layers l1, l2, l3 to obtain q1\n",
        "        # 1. Apply ReLU activation after layer l1\n",
        "        # 2. Apply ReLU activation after layer l2\n",
        "        # 3. Return output as q1 from layer l3\n",
        "\n",
        "        # [HINT] We use layers l4, l5, l6 to obtain q2\n",
        "        # 1. Apply ReLU activation after layer l4\n",
        "        # 2. Apply ReLU activation after layer l5\n",
        "        # 3. Return output as q2 from layer l6\n",
        "\n",
        "        ############################\n",
        "        # YOUR IMPLEMENTATION HERE #\n",
        "        pass\n",
        "        ############################\n",
        "        return q1, q2\n",
        "\n",
        "\n",
        "    def Q1(self, state, action):\n",
        "        sa = torch.cat([state, action], 1)\n",
        "        # [HINT] only returns q1 for actor update using layers l1, l2, l3\n",
        "        # 1. Apply ReLU activation after layer l1\n",
        "        # 2. Apply ReLU activation after layer l2\n",
        "        # 3. Return output as q1 from layer l3\n",
        "        ############################\n",
        "        # YOUR IMPLEMENTATION HERE #\n",
        "        pass\n",
        "        ############################\n",
        "        return q1\n",
        "\n",
        "class SAC(object):\n",
        "    def __init__(\n",
        "        self,\n",
        "        state_dim,\n",
        "        action_dim,\n",
        "        max_action,\n",
        "        discount=0.99,\n",
        "        tau=0.005,\n",
        "        alpha=0.2,\n",
        "    ):\n",
        "        self.actor = Actor_SAC(state_dim, action_dim, max_action).to(device)\n",
        "        self.actor_optimizer = torch.optim.Adam(self.actor.parameters(), lr=3e-4)\n",
        "\n",
        "        self.critic = Critic_SAC(state_dim, action_dim).to(device)\n",
        "        self.critic_target = copy.deepcopy(self.critic)\n",
        "        self.critic_optimizer = torch.optim.Adam(self.critic.parameters(), lr=3e-4)\n",
        "\n",
        "        self.max_action = max_action\n",
        "        self.discount = discount\n",
        "        self.tau = tau\n",
        "        self.alpha = alpha\n",
        "\n",
        "    def select_action(self, state, evaluate=False):\n",
        "        state = torch.FloatTensor(state.reshape(1, -1)).to(device)\n",
        "        if not evaluate:\n",
        "            action, _, _ = self.actor.sample(state)\n",
        "        else:\n",
        "            _, _, action = self.actor.sample(state)\n",
        "        return action.detach().cpu().numpy().flatten()\n",
        "\n",
        "    def train(self, replay_buffer, batch_size=256):\n",
        "        state, action, next_state, reward, not_done = replay_buffer.sample(batch_size)\n",
        "\n",
        "        # [HINT] compute the target Q value\n",
        "        # 1. Sample the next action and its log probability from the actor with next_state\n",
        "        # 2. Compute the next Q values (Q1 and Q2) using the critic_target with next_state and next_action\n",
        "        # 3. Min over the Q values: target_Q = min(Q1, Q2) - log_prob(a'|s') * alpha\n",
        "        # 4. Compute the target Q value: target_Q = reward + not_done * discount * target_Q\n",
        "\n",
        "        ############################\n",
        "        # YOUR IMPLEMENTATION HERE #\n",
        "        pass\n",
        "        ############################\n",
        "\n",
        "        self.critic_optimizer.zero_grad()\n",
        "        critic_loss.backward()\n",
        "        self.critic_optimizer.step()\n",
        "\n",
        "        # [HINT] compute the actor loss\n",
        "        # 1. Sample the action and its log probability from the actor with state\n",
        "        # 2. Compute the Q values (Q1 and Q2) using the critic with state and action\n",
        "        # 3. Min over the Q values: Q = min(Q1, Q2)\n",
        "        # 4. Compute the actor loss: actor_loss = alpha * log_prob(a|s) - Q\n",
        "\n",
        "        ############################\n",
        "        # YOUR IMPLEMENTATION HERE #\n",
        "        pass\n",
        "        ############################\n",
        "\n",
        "        self.actor_optimizer.zero_grad()\n",
        "        actor_loss.backward()\n",
        "        self.actor_optimizer.step()\n",
        "\n",
        "        for param, target_param in zip(self.critic.parameters(), self.critic_target.parameters()):\n",
        "            target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JWg4ZB6XlrwR"
      },
      "outputs": [],
      "source": [
        "def init_flags():\n",
        "\n",
        "        flags = {\n",
        "                \"env\": \"Pendulum-v1\",\n",
        "                \"seed\":0,\n",
        "                \"start_timesteps\": 1e4,\n",
        "                \"max_timesteps\": 5e4,\n",
        "                \"expl_noise\": 0.01,\n",
        "                \"batch_size\": 256,\n",
        "                \"discount\":0.99,\n",
        "                \"tau\": 0.005,\n",
        "                \"policy_noise\": 0.05,\n",
        "                \"noise_clip\":0.5,\n",
        "                \"policy_freq\": 2,\n",
        "                \"save_model\": \"store_true\"\n",
        "        }\n",
        "\n",
        "        return flags\n",
        "\n",
        "def main(policy_name = 'TD3'):\n",
        "\n",
        "        args = init_flags()\n",
        "        env = gym.make(args[\"env\"])\n",
        "        env.reset(seed=args[\"seed\"])\n",
        "        env.action_space.seed(args[\"seed\"])\n",
        "        torch.manual_seed(args[\"seed\"])\n",
        "        np.random.seed(args[\"seed\"])\n",
        "\n",
        "        state_dim = env.observation_space.shape[0]\n",
        "        action_dim = env.action_space.shape[0]\n",
        "        max_action = float(env.action_space.high[0])\n",
        "        kwargs = {\n",
        "                \"state_dim\": state_dim,\n",
        "                \"action_dim\": action_dim,\n",
        "                \"max_action\": max_action,\n",
        "                \"discount\": args[\"discount\"],\n",
        "                \"tau\": args[\"tau\"],}\n",
        "        if policy_name == \"TD3\":\n",
        "                # Target policy smoothing is scaled wrt the action scale\n",
        "                kwargs[\"policy_noise\"] = args[\"policy_noise\"] * max_action\n",
        "                kwargs[\"noise_clip\"] = args[\"noise_clip\"] * max_action\n",
        "                kwargs[\"policy_freq\"] = args[\"policy_freq\"]\n",
        "                policy = TD3(**kwargs)\n",
        "        elif policy_name == \"SAC\":\n",
        "                policy = SAC(**kwargs)\n",
        "\n",
        "        replay_buffer = ReplayBuffer(state_dim, action_dim)\n",
        "        evaluations = [eval_policy(policy, args[\"env\"], args[\"seed\"])]\n",
        "        state, _ = env.reset()\n",
        "        done = False\n",
        "        episode_reward = 0\n",
        "        episode_timesteps = 0\n",
        "        episode_num = 0\n",
        "\n",
        "        for t in range(int(args[\"max_timesteps\"])):\n",
        "            episode_timesteps += 1\n",
        "\n",
        "            # Select action randomly or according to policy\n",
        "            if t < args[\"start_timesteps\"]:\n",
        "                action = env.action_space.sample()\n",
        "            else:\n",
        "                action = (\n",
        "                    policy.select_action(np.array(state))\n",
        "                    + np.random.normal(0, max_action * args[\"expl_noise\"], size=action_dim)\n",
        "                ).clip(-max_action, max_action)\n",
        "\n",
        "            # Perform action\n",
        "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
        "            done = terminated or truncated\n",
        "            done_bool = float(done) if episode_timesteps < env._max_episode_steps else 0\n",
        "\n",
        "            # Store data in replay buffer\n",
        "            replay_buffer.add(state, action, next_state, reward, done_bool)\n",
        "\n",
        "            state = next_state\n",
        "            episode_reward += reward\n",
        "\n",
        "            # Train agent after collecting sufficient data\n",
        "            if t >= args[\"start_timesteps\"]:\n",
        "                policy.train(replay_buffer, args[\"batch_size\"])\n",
        "\n",
        "            if done:\n",
        "                # +1 to account for 0 indexing. +0 on ep_timesteps since it will increment +1 even if done=True\n",
        "                print(f\"Total T: {t+1} Episode Num: {episode_num+1} Episode T: {episode_timesteps} Reward: {episode_reward:.3f}\")\n",
        "\n",
        "                evaluations.append(episode_reward)\n",
        "\n",
        "                # Reset environment\n",
        "                state, _ = env.reset()\n",
        "                done = False\n",
        "                episode_reward = 0\n",
        "                episode_timesteps = 0\n",
        "                episode_num += 1\n",
        "\n",
        "        return evaluations\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KxQ76lj4lRsE",
        "outputId": "351d596a-3863-43a8-bd9b-09582e89ed5a"
      },
      "outputs": [],
      "source": [
        "evaluation_td3 = main(policy_name = 'TD3')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "id": "6KT2WajNt7pO",
        "outputId": "288aed6d-3f6a-431b-b725-3a6162555ad8"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(evaluation_td3)\n",
        "plt.xlabel('Episode')\n",
        "plt.ylabel('Reward')\n",
        "plt.title('Pendulum with TD3')\n",
        "plt.grid()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JpUxQR_igC0v",
        "outputId": "ec659ad8-7e5e-435b-8589-171dbb622d31"
      },
      "outputs": [],
      "source": [
        "evaluation_sac = main(policy_name = 'SAC')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "id": "EMrhusvCwTvD",
        "outputId": "5f79a1ec-9209-44cd-dd4e-66ba1a5b39aa"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(evaluation_sac)\n",
        "plt.xlabel('Episode')\n",
        "plt.ylabel('Reward')\n",
        "plt.title('Pendulum with SAC')\n",
        "plt.grid()\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

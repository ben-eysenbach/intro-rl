{"cells":[{"cell_type":"markdown","metadata":{"id":"6Q9l3TTHLd0_"},"source":["## Homework 3\n","\n","In this assignment, we will practice implementing value iteration in the [Frozen Lake environment](https://gymnasium.farama.org/environments/toy_text/frozen_lake/) from OpenAI Gym, and implementing REINFORCE policy gradient algorithm for [Cart-Pole v1](https://gymnasium.farama.org/environments/classic_control/cart_pole/) environment.\n","\n","* You should run this on Google Colab\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"pjRnCPPiKjNA"},"source":["## Bellman Update and Value Iteration"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"G84AFb9mfS7n"},"outputs":[],"source":["# IMPORTANT: Always run this cell before anything else to ensure that you are able to access the Frozen Lake environment\n","!pip install gymnasium==1.0.0\n","import gymnasium as gym\n","import argparse\n","import numpy as np\n","import time\n","from gymnasium.envs.registration import register\n","\n","# De-register environments if there is a collision\n","env_dict = gym.envs.registration.registry.copy()\n","for env in env_dict:\n","    if \"Deterministic-4x4-FrozenLake-v0\" in env:\n","        del gym.envs.registration.registry[env]\n","    elif \"Stochastic-4x4-FrozenLake-v0\" in env:\n","        del gym.envs.registration.registry[env]\n","\n","\n","register(\n","    id=\"Deterministic-4x4-FrozenLake-v0\",\n","    entry_point=\"gymnasium.envs.toy_text.frozen_lake:FrozenLakeEnv\",\n","    kwargs={\"map_name\": \"4x4\", \"is_slippery\": False},\n",")\n","\n","register(\n","    id=\"Stochastic-4x4-FrozenLake-v0\",\n","    entry_point=\"gymnasium.envs.toy_text.frozen_lake:FrozenLakeEnv\",\n","    kwargs={\"map_name\": \"4x4\", \"is_slippery\": True},\n",")\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"x3ZMIFfOlEfV"},"source":["The parameters P, nS, nA, gamma are defined as follows:\n","\n","\tP: nested dictionary of a nested lists\n","\t\tFrom gym.core.Environment\n","\t\tFor each pair of states in [1, nS] and actions in [1, nA], P[state][action] is a\n","\t\ttuple of the form (probability, nextstate, reward, terminal) where\n","\t\t\t- probability: float\n","\t\t\t\tthe probability of transitioning from \"state\" to \"nextstate\" with \"action\"\n","\t\t\t- nextstate: int\n","\t\t\t\tdenotes the state we transition to (in range [0, nS - 1])\n","\t\t\t- reward: int\n","\t\t\t\teither 0 or 1, the reward for transitioning from \"state\" to\n","\t\t\t\t\"nextstate\" with \"action\"\n","\t\t\t- terminal: bool\n","\t\t\t  True when \"nextstate\" is a terminal state (hole or goal), False otherwise\n","\tnS: int\n","\t\tnumber of states in the environment\n","\tnA: int\n","\t\tnumber of actions in the environment\n","\tgamma: float\n","\t\tDiscount factor. Number in range [0, 1)"]},{"cell_type":"code","source":["env = gym.make('Stochastic-4x4-FrozenLake-v0')\n","\n","env.unwrapped.nS = env.unwrapped.nrow * env.unwrapped.ncol\n","env.unwrapped.nA = 4"],"metadata":{"id":"1yHWkwiKVIB5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["current_state = 3 # modify between 0~15\n","current_action = 0 # modify between 0~3\n","print(env.unwrapped.P[current_state][current_action])\n","\n","# sample output: [(1.0, 2, 0.0, False)], it transits to state 2 with\n","# probabilty 1, receives reward 0, not terminating.\n","\n","# sample output: [(0.3333333333333333, 3, 0.0, False), (0.3333333333333333, 2,\n","# 0.0, False), (0.3333333333333333, 7, 0.0, True)]"],"metadata":{"id":"k0-gYwYIVSM0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Recall that if we repeatedly apply the Bellman Operator to the value function, it will converges to the optimal value function. First, we implement the Bellman Update function below."],"metadata":{"id":"8S_yFaCyUbYP"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"Af0JXOtflUmh"},"outputs":[],"source":["def Bellman_Update(P, nS, nA, value, gamma=0.9):\n","\t\t\"\"\"\n","\t\tGiven the value function, apply Bellman Operator to the value function.\n","\n","\tParameters\n","\t----------\n","\tP, nS, nA, gamma:\n","\t\tdefined at beginning of file\n","\tvalue: np.ndarray, shape = (nS, )\n","\t\tThe input value function\n","\n","\tReturns\n","\t-------\n","\tnew_value: np.ndarray, shape = (nS, )\n","\t\tthe improved value function.\n","\tgreedy_policy: np.ndarray, shape = (nS, )\n","\t\tthe greedy policy from the improved value function.\n","\t\"\"\"\n","\n","\t\tnew_value = np.zeros(nS) # TODO\n","\t\tgreedy_policy = np.zeros(nS, dtype=\"int\") # TODO\n","\n","\t\t### Q1: Bellman Update, 15 points ###\n","\t\t############################\n","\t\t# YOUR IMPLEMENTATION HERE #\n","\n","\t\t############################\n","\n","\t\treturn new_value, greedy_policy\n"]},{"cell_type":"markdown","source":["Next, we run a for loop to apply Bellman Updates repeatedly to an initial value."],"metadata":{"id":"_Dv0Ndgrdaq7"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"xL3cBRuWla19"},"outputs":[],"source":["def value_iteration(P, nS, nA, gamma=0.9, tol=1e-3):\n","\t\t\"\"\"\n","\tPerforms value iteration by repeatedly applying Bellman Updates.\n","\n","\tParameters:\n","\t----------\n","\tP, nS, nA, gamma:\n","\t\tdefined at beginning of file\n","\ttol: float\n","\t\tTerminate value iteration when\n","\t\t\tmax |value_function(s) - prev_value_function(s)| < tol\n","\tReturns:\n","\t----------\n","\tpolicy: np.ndarray[nS]\n","\t\tthe greedy policy derived from the final value function\n","\t\"\"\"\n","\n","\t\tvalue_function = np.zeros(nS) # initial value function\n","\t\tpolicy = np.zeros(nS, dtype=int) # TODO\n","\n","\t\t### Q2: Value Iteration, 5 points ###\n","\t\t############################\n","\t\t# YOUR IMPLEMENTATION HERE #\n","\n","\t\t############################\n","\t\treturn policy"]},{"cell_type":"markdown","metadata":{"id":"Z3G0x0oZlTCB"},"source":["We provide you with the following function to evaluate how good your policy is, by interfering with the environment!"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BR439ibymW6Z"},"outputs":[],"source":["def evaluate(env, policy, max_steps=100):\n","    \"\"\"\n","    This function does not need to be modified\n","    Watch your agent play!\n","\n","    Parameters\n","    ----------\n","    env: gym.core.Environment\n","      Environment to play on. Must have nS, nA, and P as\n","      attributes.\n","    Policy: np.array of shape [env.nS]\n","      The action to take at a given state\n","  \"\"\"\n","\n","    episode_reward = 0\n","    ob, _ = env.reset()\n","    for t in range(max_steps):\n","        a = policy[ob]\n","        ob, rew, done, _, _ = env.step(a)\n","        episode_reward += rew\n","        if done:\n","            break\n","    if not done:\n","        print(\n","            \"The agent didn't reach a terminal state in {} steps.\".format(\n","                max_steps\n","            )\n","        )\n","    else:\n","        print(\"Episode reward: %f\" % episode_reward)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eaMQ1w5Rmand"},"outputs":[],"source":["# Run the code below to implement value iteration on Frozen Lake!\n","# You may change the parameters in the functions below\n","np.set_printoptions(precision=3)\n","\n","# Make gym environment\n","env = gym.make('Stochastic-4x4-FrozenLake-v0')\n","\n","env.unwrapped.nS = env.unwrapped.nrow * env.unwrapped.ncol\n","env.unwrapped.nA = 4\n","\n","print(\"\\n\" + \"-\" * 25 + \"\\nBeginning Value Iteration\\n\" + \"-\" * 25)\n","\n","p_vi = value_iteration(env.unwrapped.P, env.unwrapped.nS, env.unwrapped.nA, gamma=0.9, tol=1e-3)\n","print('the learned policy:', p_vi)\n","evaluate(env, p_vi, 100)\n","\n","## successful policy should have episode reward close to 1, and the agent should terminate within 100 steps."]},{"cell_type":"markdown","metadata":{"id":"hHh-z3zAK4ew"},"source":["## Policy Gradient: REINFORCE\n","\n","In this problem, we try to implement the `REINFORCE` algorithm to learn the optimal policy on environment `CartPole-v1` of OpenAI-Gym. We will use a neural network to parametrize the policy, and then run policy gradient on it. We  provide you with the following evaluation function, to calculate the expected cumulative reward of your current policy through Monte Carlo. You need to (1) implement REINFORCE with a neural network, and (2) plot the expected reward againt the number of training epochs."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"e-AfGSVyMj0k"},"outputs":[],"source":["def evaluate_neural(env, policy, max_steps=500, trials = 100):\n","    \"\"\"\n","    This function does not need to be modified\n","    Renders policy once on environment. Watch your agent play!\n","\n","    Parameters\n","    ----------\n","    env: gym.core.Environment\n","    Policy: torch.Distribution, trained with REINFORCE\n","    trials: int, number of trials for Monte Carlo, default = 100\n","    Returns\n","    -------\n","    cum_reward_mean: estimate of the expected cumulative reward\n","  \"\"\"\n","    cum_reward = []\n","    for i in range(trials):\n","      episode_reward = 0\n","      ob,_ = env.reset()\n","      for t in range(max_steps):\n","          ob = torch.tensor(ob, dtype=torch.float32)\n","          a = Categorical(policy(ob)).sample().item()\n","          ob, rew, done, _, _ = env.step(a)\n","          episode_reward += rew\n","          if done:\n","              break\n","      cum_reward.append(episode_reward)\n","    cum_reward = np.array(cum_reward)\n","    return cum_reward.mean(), cum_reward.std()"]},{"cell_type":"markdown","metadata":{"id":"WcnwfykAPz0R"},"source":["Now you are ready to implement the algorithm! In the following we define a class `PolicyNetwork` to be 2-layer ReLU network. Recall that the (variance reduced) gradient estimate in `REINFORCE` can be defined by\n","$$ \\sum_{h\\geq 0} \\nabla_\\theta\\log \\pi_\\theta(A_h|S_h) \\cdot \\sum_{t\\geq h}\\gamma^{t-h} r(S_t, A_t).  $$\n","\n"," You have the freedom to modify the network architecture and the training procedure of the neural network, but make sure that it takes the state as input, and outputs a distrubution for the actions! Once it is implemented and trained, plot the cumulative reward with error bar."]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.distributions import Categorical\n","\n","# Define the policy network\n","class PolicyNetwork(nn.Module):\n","    def __init__(self, input_size, hidden_size, output_size):\n","        super(PolicyNetwork, self).__init__()\n","\n","        self.fc1 = nn.Linear(input_size, hidden_size)\n","        self.fc2 = nn.Linear(hidden_size, output_size)\n","\n","    def forward(self, x):\n","        x = torch.relu(self.fc1(x))\n","        x = self.fc2(x)\n","        return torch.softmax(x, dim=-1)"],"metadata":{"id":"QK3CmVBok8Tr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Initialize environment and optimizer. Feel free to change!\n","env = gym.make('CartPole-v1')\n","input_size = 4\n","output_size = env.action_space.n\n","hidden_size = 64\n","\n","\n","policy_net = PolicyNetwork(input_size, hidden_size, output_size)\n","optimizer = optim.Adam(policy_net.parameters(), lr=0.001)\n","# Training hyperparameters. Feel free to change!\n","num_episodes = 3000\n","gamma = 0.99"],"metadata":{"id":"g4AyLQU6lAfb"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PM1uT5ngK7Tj"},"outputs":[],"source":["## statistics\n","episode_ids = []\n","avg_rewards = []\n","std_rewards = []\n","\n","\n","# Training loop\n","for episode in range(num_episodes):\n","    episode_rewards = []\n","    log_probs = []\n","\n","    state,_ = env.reset()\n","    done = False\n","\n","    while not done:\n","      # Sample action from the policy network. For simplicity we only use single trajectory\n","      # to update. But feel free to change this into batch learning!\n","      state_tensor = torch.from_numpy(state).float().unsqueeze(0)\n","      action_probs = policy_net(state_tensor)\n","      action_dist = Categorical(action_probs)\n","      action = action_dist.sample()\n","      log_prob = action_dist.log_prob(action)\n","      log_probs.append(log_prob)\n","\n","      # Take action and observe next state and reward\n","      next_state, reward, done, _, _ = env.step(action.item())\n","      episode_rewards.append(reward)\n","      state = next_state\n","\n","\n","    # Now use the episode_rewards list, the log_prob list to construct a loss function, on which you can\n","    # backward propagate and optimize with optimizer.\n","\n","    # First, compute the discounted cumulative reward for every step in the trajectory.\n","    # G_t = r_t + \\gamma r_{t+1} + \\gamma^2 r_{t+2} + ...\n","    # Hint: compute this in backward direction.\n","    discounted_rewards = []\n","\n","    ### Q3: discounted rewards, 15 points ###\n","    ############################\n","\t\t# YOUR IMPLEMENTATION HERE #\n","\n","    ############################\n","\n","    discounted_rewards = torch.tensor(discounted_rewards, dtype=torch.float32)\n","    optimizer.zero_grad()\n","\n","\n","    # Compute policy loss and update policy network\n","    ## note that `log_probs` contains differentiable torch.tensor computed through the policy network\n","    ## construct a loss using `log_probs` via REINFORCE, and call loss.backward()\n","\n","    ### Q4: REINFORCE, 15 points ###\n","    ############################\n","\t\t# YOUR IMPLEMENTATION HERE #\n","\n","    ############################\n","\n","    policy_loss.backward()\n","    optimizer.step()\n","\n","    # Record the cumulative reward and its deviation once every 100 episodes\n","    if (episode + 1) % 100 == 0:\n","      avg_reward, std_reward = evaluate_neural(env, policy_net)\n","      print(f'Episode [{episode + 1}/{num_episodes}], Cumulative Reward: {avg_reward}')\n","      episode_ids.append(episode + 1)\n","      avg_rewards.append(avg_reward)\n","      std_rewards.append(std_reward)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XP__60AnU-v9"},"outputs":[],"source":["from matplotlib import pyplot as plt\n","# Plot the curve of cumulative reward v.s. number of episodes, with error bar\n","\n","plt.errorbar(episode_ids, avg_rewards, std_rewards, label='REINFORCE')\n","plt.xlabel(\"Num Episode\")\n","plt.ylabel(\"Cumulative Rewards\")\n","plt.legend()\n","\n","## optimal policy should reach near 500 of cumulative rewards."]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
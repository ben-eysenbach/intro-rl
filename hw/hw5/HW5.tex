\documentclass[12pt]{article}
\usepackage[colorinlistoftodos]{todonotes}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{bm}
\usepackage{enumerate}
\usepackage{fancyvrb}
\usepackage[top=1in, bottom=1in, left=1in, right=1in]{geometry}
\usepackage{hyperref}
\usepackage{placeins}
\usepackage{tikz}
\usepackage{tikzsymbols}
\usepackage{todonotes}
\usepackage{bbm}
\usepackage{color}
\usepackage{mathrsfs}
\usepackage{enumitem}
\usepackage{soul} % for HL
\usepackage{color} % for HL
\usepackage{minted}

\usepackage{listings}
\usepackage{xcolor}
\usepackage[
backend=biber,
style=alphabetic,
sorting=ynt
]{biblatex}
\addbibresource{papers.bib}

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}

\lstset{style=mystyle}

% \renewcommand{\theenumi}{\roman{enumi}}
\newcommand{\rmn}[1]{{\textcolor{blue}{\bf [{\sc rmn:} #1]}}}
\DeclareMathOperator*{\argmax}{arg\,max}

\usetikzlibrary{positioning,calc}
%%%%%%%%%
\usepackage[most]{tcolorbox}
\newtcolorbox[]{solution}[1][]{%
    breakable,
    enhanced,
    colback=white,
    title=Solution,
    #1
}

\newtcolorbox[]{fillme}[1][]{%
    breakable,
    enhanced,
    colback=white,
    title=Fill me in,
    #1
}

\newcommand*{\E}{\mathbb{E}}
\newcommand*{\prob}{\mathbb{P}}
\newcommand*{\F}{\mathcal{F}}
\newcommand*{\bo}{\mathbb{B}}
\newtheorem{theorem}{Theorem}
\newtheorem{observation}{Observation}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}

%%%%%%%%%%

\date{Due March 25, 2024}
\author{\begin{fillme}[width=0.3\textwidth]
 Your name here.
\end{fillme}} % Fill in your name!

\title{ECE433/COS435 Introduction to RL\\
  Assignment 3: Value Iteration and Policy Gradient\\
  Spring 2024\\
}

\begin{document}
  \maketitle
  \section*{Collaborators}
\begin{fillme}
 Please fill in the names and NetIDs of your collaborators in this section.
\end{fillme}

\section*{Instructions}

Writeups should be typesetted in Latex and submitted as PDFs. You can work with whatever tool you like for the code, but \textbf{please submit the asked-for snippet and answer in the solutions box as part of your writeup. We will only be grading your write-up.} Make sure still also to attach your notebook/code with your submission.\\


\noindent We recommend you collaborate in pairs if you find the workload of this assignment overwhelming.

\section*{Question 1. Implementation of Advantage Actor-Critic (A2C)}
For details of A2C, please refer to the accompanying .ipynb file. We also recommend you to read the introduction of  \cite{lillicrap2019continuous} for better insight into its motivation.
\subsection*{Question 1.a}
Paste the entire cell implementing the \texttt{Actor} and \texttt{Critic} classes below. 
\begin{solution}
Your solution here...
\end{solution}

\subsection*{Question 1.b}
Paste the entire cell implementing the \texttt{compute\_returns()} method below.
\begin{solution}
Your solution here...
\end{solution}

\subsection*{Question 1.c}
Paste the code for the entire \texttt{training()} method below.
\begin{solution}
Your solution here...
\end{solution}
\subsection*{Question 1.d}
Train your A2C agent on the Gym environment \texttt{CartPole-v1}. Insert your plot of the cumulative reward curve in the training process below. 
\begin{solution}
Your solution here...
\end{solution}
\section*{Question 2. Implementing DDPG and TD3}
In the question, we introduce a useful actor-critic type algorithm:  the Deep Deterministic Policy Gradient (DDPG). For a detailed introduction, please refer to the accompanying .ipynb file.
\subsection*{Question 2.a} 
Paste the entire cell implementing the \texttt{Actor\_DDPG} and \texttt{Critic\_DDPG} classes below.
\begin{solution}
Your solution here...
\end{solution}
\subsection*{Question 2.b} 
Paste the entire code of \texttt{DDPG.train} below.
\begin{solution}
Your solution here...
\end{solution}
\subsection*{Question 2.c}
Insert the reward v.s. episode curve below.  (Hint: The optimal reward for \texttt{MountainCarContinuous} is around 100.) (Hint2: If the reward is stuck at some local minimum throughout the training process, e.g. $r= -0.1$, try restarting the training process. Being stuck in a local minimum can be caused by an insufficient starting exploration. )
\begin{solution}
Your solution here...
\end{solution}
\subsection*{Question 2.d}
Try out DDPG  on some new tasks (apart from \texttt{MountainCarContinous}). Please indicate the name of this environment (if you define it yourself, please specify its reward and transition probability), and plot the reward curve against the training episodes below. Does it work out?
\begin{solution}
Your solution here...
\end{solution}

\section*{Question 3. Implementation of TD3 [Optional]}
In this question, we introduce a variant of DDPG: the Twin Delayed DDPG (TD3). For a detailed introduction, please refer to the accompanying .ipynb file. For a better understanding of how TD3 reduce the overestimation bias and the variance, we refer you to read \cite[Chapt.~4 and 5]{fujimoto2018addressing}
\subsection*{Question 3.a} 
Paste the code for \texttt{Actor\_TD3} and \texttt{Critic\_TD3} networks below.
\begin{solution}
Your solution here...
\end{solution}
\subsection*{Question 3.b} 
Paste the entire code of \texttt{TD3.train} below.
\begin{solution}
Your solution here...
\end{solution}
\subsection*{Question 3.c} 
Insert the reward v.s. episode curve below. Do both algorithms (DDPG, TD3) converge to the same cumulative reward? (Hint: The optimal reward for \texttt{MountainCarContinuous} is around 100.)
\begin{solution}
Your solution here...
\end{solution}
\end{document}


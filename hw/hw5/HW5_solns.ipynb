{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "fUDWvikdkd2J"
      },
      "outputs": [],
      "source": [
        "!pip uninstall box2d-py -y > /dev/null 2>&1\n",
        "!pip install box2d-py > /dev/null 2>&1\n",
        "!pip3 install box2d box2d-kengz > /dev/null 2>&1\n",
        "!apt install xvfb > /dev/null 2>&1\n",
        "!pip3 install pyvirtualdisplay > /dev/null 2>&1\n",
        "!pip install gym==0.25.0 > /dev/null 2>&1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ZlGiM8WGkzsi"
      },
      "outputs": [],
      "source": [
        "import gym\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "import sys\n",
        "from pyvirtualdisplay import Display\n",
        "from IPython import display as disp\n",
        "import copy\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i4slgsZK2HNr"
      },
      "source": [
        "We provide you with the following utility functions for this assignment: (1) the replay buffer for experience replay, and (2) a policy evaluation function which evaluates a policy with Monte Carlo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "1bX7EbSNod3a"
      },
      "outputs": [],
      "source": [
        "# Replay buffer\n",
        "class ReplayBuffer(object):\n",
        "\tdef __init__(self, state_dim, action_dim, max_size=int(1e6)):\n",
        "\t\tself.max_size = max_size\n",
        "\t\tself.ptr = 0\n",
        "\t\tself.size = 0\n",
        "\n",
        "\t\tself.state = np.zeros((max_size, state_dim))\n",
        "\t\tself.action = np.zeros((max_size, action_dim))\n",
        "\t\tself.next_state = np.zeros((max_size, state_dim))\n",
        "\t\tself.reward = np.zeros((max_size, 1))\n",
        "\t\tself.not_done = np.zeros((max_size, 1))\n",
        "\n",
        "\t\tself.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "\tdef add(self, state, action, next_state, reward, done):\n",
        "\t\tself.state[self.ptr] = state\n",
        "\t\tself.action[self.ptr] = action\n",
        "\t\tself.next_state[self.ptr] = next_state\n",
        "\t\tself.reward[self.ptr] = reward\n",
        "\t\tself.not_done[self.ptr] = 1. - done\n",
        "\n",
        "\t\tself.ptr = (self.ptr + 1) % self.max_size\n",
        "\t\tself.size = min(self.size + 1, self.max_size)\n",
        "\n",
        "\n",
        "\tdef sample(self, batch_size):\n",
        "\t\tind = np.random.randint(0, self.size, size=batch_size)\n",
        "\n",
        "\t\treturn (\n",
        "\t\t\ttorch.FloatTensor(self.state[ind]).to(self.device),\n",
        "\t\t\ttorch.FloatTensor(self.action[ind]).to(self.device),\n",
        "\t\t\ttorch.FloatTensor(self.next_state[ind]).to(self.device),\n",
        "\t\t\ttorch.FloatTensor(self.reward[ind]).to(self.device),\n",
        "\t\t\ttorch.FloatTensor(self.not_done[ind]).to(self.device)\n",
        "\t\t)\n",
        "\n",
        "# policy evaluation with Monte Carlo\n",
        "def eval_policy(policy, env_name, seed, eval_episodes=10):\n",
        "\teval_env = gym.make(env_name)\n",
        "\teval_env.seed(seed + 100)\n",
        "\n",
        "\tavg_reward = 0.\n",
        "\tfor _ in range(eval_episodes):\n",
        "\t\tstate, done = eval_env.reset(), False\n",
        "\t\twhile not done:\n",
        "\t\t\taction = policy.select_action(np.array(state))\n",
        "\t\t\tstate, reward, done, _= eval_env.step(action)\n",
        "\t\t\tavg_reward += reward\n",
        "\n",
        "\tavg_reward /= eval_episodes\n",
        "\n",
        "\tprint(\"---------------------------------------\")\n",
        "\tprint(f\"Evaluation over {eval_episodes} episodes: {avg_reward:.3f}\")\n",
        "\tprint(\"---------------------------------------\")\n",
        "\treturn avg_reward\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Hn3nFpLqk3wR"
      },
      "outputs": [],
      "source": [
        "# Implementation of Deep Deterministic Policy Gradients (DDPG)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "class Actor(nn.Module):\n",
        "\tdef __init__(self, state_dim, action_dim, max_action):\n",
        "\t\tsuper(Actor, self).__init__()\n",
        "\n",
        "\t\tself.l1 = nn.Linear(state_dim, 400)\n",
        "\t\tself.l2 = nn.Linear(400, 300)\n",
        "\t\tself.l3 = nn.Linear(300, action_dim)\n",
        "\n",
        "\t\tself.max_action = max_action\n",
        "\n",
        "\n",
        "\tdef forward(self, state):\n",
        "\t\ta = F.relu(self.l1(state))\n",
        "\t\ta = F.relu(self.l2(a))\n",
        "\t\treturn self.max_action * torch.tanh(self.l3(a))\n",
        "\n",
        "\n",
        "class Critic(nn.Module):\n",
        "\tdef __init__(self, state_dim, action_dim):\n",
        "\t\tsuper(Critic, self).__init__()\n",
        "\n",
        "\t\tself.l1 = nn.Linear(state_dim, 400)\n",
        "\t\tself.l2 = nn.Linear(400 + action_dim, 300)\n",
        "\t\tself.l3 = nn.Linear(300, 1)\n",
        "\n",
        "\n",
        "\tdef forward(self, state, action):\n",
        "\t\tq = F.relu(self.l1(state))\n",
        "\t\tq = F.relu(self.l2(torch.cat([q, action], 1)))\n",
        "\t\treturn self.l3(q)\n",
        "\n",
        "\n",
        "class DDPG(object):\n",
        "\tdef __init__(self, state_dim, action_dim, max_action, discount=0.99, tau=0.001):\n",
        "\t\tself.actor = Actor(state_dim, action_dim, max_action).to(device)\n",
        "\t\tself.actor_target = copy.deepcopy(self.actor)\n",
        "\t\tself.actor_optimizer = torch.optim.Adam(self.actor.parameters(), lr=1e-4)\n",
        "\n",
        "\t\tself.critic = Critic(state_dim, action_dim).to(device)\n",
        "\t\tself.critic_target = copy.deepcopy(self.critic)\n",
        "\t\tself.critic_optimizer = torch.optim.Adam(self.critic.parameters(), weight_decay=1e-2)\n",
        "\n",
        "\t\tself.discount = discount\n",
        "\t\tself.tau = tau\n",
        "\n",
        "\n",
        "\tdef select_action(self, state):\n",
        "\t\tstate = torch.FloatTensor(state.reshape(1, -1)).to(device)\n",
        "\t\treturn self.actor(state).cpu().data.numpy().flatten()\n",
        "\n",
        "\n",
        "\tdef train(self, replay_buffer, batch_size=64):\n",
        "\t\t# Sample replay buffer\n",
        "\t\tstate, action, next_state, reward, not_done = replay_buffer.sample(batch_size)\n",
        "\n",
        "\t\t# Compute the target Q value\n",
        "\t\ttarget_Q = self.critic_target(next_state, self.actor_target(next_state))\n",
        "\t\ttarget_Q = reward + (not_done * self.discount * target_Q).detach()\n",
        "\n",
        "\t\t# Get current Q estimate\n",
        "\t\tcurrent_Q = self.critic(state, action)\n",
        "\n",
        "\t\t# Compute critic loss\n",
        "\t\tcritic_loss = F.mse_loss(current_Q, target_Q)\n",
        "\n",
        "\t\t# Optimize the critic\n",
        "\t\tself.critic_optimizer.zero_grad()\n",
        "\t\tcritic_loss.backward()\n",
        "\t\tself.critic_optimizer.step()\n",
        "\n",
        "\t\t# Compute actor loss\n",
        "\t\tactor_loss = -self.critic(state, self.actor(state)).mean()\n",
        "\n",
        "\t\t# Optimize the actor\n",
        "\t\tself.actor_optimizer.zero_grad()\n",
        "\t\tactor_loss.backward()\n",
        "\t\tself.actor_optimizer.step()\n",
        "\n",
        "\t\t# Update the frozen target models\n",
        "\t\tfor param, target_param in zip(self.critic.parameters(), self.critic_target.parameters()):\n",
        "\t\t\ttarget_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n",
        "\n",
        "\t\tfor param, target_param in zip(self.actor.parameters(), self.actor_target.parameters()):\n",
        "\t\t\ttarget_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n",
        "\n",
        "\n",
        "\tdef save(self, filename):\n",
        "\t\ttorch.save(self.critic.state_dict(), filename + \"_critic\")\n",
        "\t\ttorch.save(self.critic_optimizer.state_dict(), filename + \"_critic_optimizer\")\n",
        "\n",
        "\t\ttorch.save(self.actor.state_dict(), filename + \"_actor\")\n",
        "\t\ttorch.save(self.actor_optimizer.state_dict(), filename + \"_actor_optimizer\")\n",
        "\n",
        "\n",
        "\tdef load(self, filename):\n",
        "\t\tself.critic.load_state_dict(torch.load(filename + \"_critic\"))\n",
        "\t\tself.critic_optimizer.load_state_dict(torch.load(filename + \"_critic_optimizer\"))\n",
        "\t\tself.critic_target = copy.deepcopy(self.critic)\n",
        "\n",
        "\t\tself.actor.load_state_dict(torch.load(filename + \"_actor\"))\n",
        "\t\tself.actor_optimizer.load_state_dict(torch.load(filename + \"_actor_optimizer\"))\n",
        "\t\tself.actor_target = copy.deepcopy(self.actor)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "wJfEi1DHlbdD"
      },
      "outputs": [],
      "source": [
        "# Implementation of Twin Delayed Deep Deterministic Policy Gradients (TD3)\n",
        "class Actor_TD3(nn.Module):\n",
        "\tdef __init__(self, state_dim, action_dim, max_action):\n",
        "\t\tsuper(Actor_TD3, self).__init__()\n",
        "\n",
        "\t\tself.l1 = nn.Linear(state_dim, 256)\n",
        "\t\tself.l2 = nn.Linear(256, 256)\n",
        "\t\tself.l3 = nn.Linear(256, action_dim)\n",
        "\n",
        "\t\tself.max_action = max_action\n",
        "\n",
        "\n",
        "\tdef forward(self, state):\n",
        "\t\ta = F.relu(self.l1(state))\n",
        "\t\ta = F.relu(self.l2(a))\n",
        "\t\treturn self.max_action * torch.tanh(self.l3(a))\n",
        "\n",
        "\n",
        "class Critic_TD3(nn.Module):\n",
        "\tdef __init__(self, state_dim, action_dim):\n",
        "\t\tsuper(Critic_TD3, self).__init__()\n",
        "\n",
        "\t\t# Q1 architecture\n",
        "\t\tself.l1 = nn.Linear(state_dim + action_dim, 256)\n",
        "\t\tself.l2 = nn.Linear(256, 256)\n",
        "\t\tself.l3 = nn.Linear(256, 1)\n",
        "\n",
        "\t\t# Q2 architecture\n",
        "\t\tself.l4 = nn.Linear(state_dim + action_dim, 256)\n",
        "\t\tself.l5 = nn.Linear(256, 256)\n",
        "\t\tself.l6 = nn.Linear(256, 1)\n",
        "\n",
        "\n",
        "\tdef forward(self, state, action):\n",
        "\t\tsa = torch.cat([state, action], 1)\n",
        "\n",
        "\t\tq1 = F.relu(self.l1(sa))\n",
        "\t\tq1 = F.relu(self.l2(q1))\n",
        "\t\tq1 = self.l3(q1)\n",
        "\n",
        "\t\tq2 = F.relu(self.l4(sa))\n",
        "\t\tq2 = F.relu(self.l5(q2))\n",
        "\t\tq2 = self.l6(q2)\n",
        "\t\treturn q1, q2\n",
        "\n",
        "\n",
        "\tdef Q1(self, state, action):\n",
        "\t\tsa = torch.cat([state, action], 1)\n",
        "\n",
        "\t\tq1 = F.relu(self.l1(sa))\n",
        "\t\tq1 = F.relu(self.l2(q1))\n",
        "\t\tq1 = self.l3(q1)\n",
        "\t\treturn q1\n",
        "\n",
        "\n",
        "class TD3(object):\n",
        "\tdef __init__(\n",
        "\t\tself,\n",
        "\t\tstate_dim,\n",
        "\t\taction_dim,\n",
        "\t\tmax_action,\n",
        "\t\tdiscount=0.99,\n",
        "\t\ttau=0.005,\n",
        "\t\tpolicy_noise=0.2,\n",
        "\t\tnoise_clip=0.5,\n",
        "\t\tpolicy_freq=2\n",
        "\t):\n",
        "\n",
        "\t\tself.actor = Actor_TD3(state_dim, action_dim, max_action).to(device)\n",
        "\t\tself.actor_target = copy.deepcopy(self.actor)\n",
        "\t\tself.actor_optimizer = torch.optim.Adam(self.actor.parameters(), lr=3e-4)\n",
        "\n",
        "\t\tself.critic = Critic_TD3(state_dim, action_dim).to(device)\n",
        "\t\tself.critic_target = copy.deepcopy(self.critic)\n",
        "\t\tself.critic_optimizer = torch.optim.Adam(self.critic.parameters(), lr=3e-4)\n",
        "\n",
        "\t\tself.max_action = max_action\n",
        "\t\tself.discount = discount\n",
        "\t\tself.tau = tau\n",
        "\t\tself.policy_noise = policy_noise\n",
        "\t\tself.noise_clip = noise_clip\n",
        "\t\tself.policy_freq = policy_freq\n",
        "\n",
        "\t\tself.total_it = 0\n",
        "\n",
        "\n",
        "\tdef select_action(self, state):\n",
        "\t\tstate = torch.FloatTensor(state.reshape(1, -1)).to(device)\n",
        "\t\treturn self.actor(state).cpu().data.numpy().flatten()\n",
        "\n",
        "\n",
        "\tdef train(self, replay_buffer, batch_size=256):\n",
        "\t\tself.total_it += 1\n",
        "\n",
        "\t\t# Sample replay buffer\n",
        "\t\tstate, action, next_state, reward, not_done = replay_buffer.sample(batch_size)\n",
        "\n",
        "\t\twith torch.no_grad():\n",
        "\t\t\t# Select action according to policy and add clipped noise\n",
        "\t\t\tnoise = (\n",
        "\t\t\t\ttorch.randn_like(action) * self.policy_noise\n",
        "\t\t\t).clamp(-self.noise_clip, self.noise_clip)\n",
        "\n",
        "\t\t\tnext_action = (\n",
        "\t\t\t\tself.actor_target(next_state) + noise\n",
        "\t\t\t).clamp(-self.max_action, self.max_action)\n",
        "\n",
        "\t\t\t# Compute the target Q value\n",
        "\t\t\ttarget_Q1, target_Q2 = self.critic_target(next_state, next_action)\n",
        "\t\t\ttarget_Q = torch.min(target_Q1, target_Q2)\n",
        "\t\t\ttarget_Q = reward + not_done * self.discount * target_Q\n",
        "\n",
        "\t\t# Get current Q estimates\n",
        "\t\tcurrent_Q1, current_Q2 = self.critic(state, action)\n",
        "\n",
        "\t\t# Compute critic loss\n",
        "\t\tcritic_loss = F.mse_loss(current_Q1, target_Q) + F.mse_loss(current_Q2, target_Q)\n",
        "\n",
        "\t\t# Optimize the critic\n",
        "\t\tself.critic_optimizer.zero_grad()\n",
        "\t\tcritic_loss.backward()\n",
        "\t\tself.critic_optimizer.step()\n",
        "\n",
        "\t\t# Delayed policy updates\n",
        "\t\tif self.total_it % self.policy_freq == 0:\n",
        "\n",
        "\t\t\t# Compute actor losse\n",
        "\t\t\tactor_loss = -self.critic.Q1(state, self.actor(state)).mean()\n",
        "\n",
        "\t\t\t# Optimize the actor\n",
        "\t\t\tself.actor_optimizer.zero_grad()\n",
        "\t\t\tactor_loss.backward()\n",
        "\t\t\tself.actor_optimizer.step()\n",
        "\n",
        "\t\t\t# Update the frozen target models\n",
        "\t\t\tfor param, target_param in zip(self.critic.parameters(), self.critic_target.parameters()):\n",
        "\t\t\t\ttarget_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n",
        "\n",
        "\t\t\tfor param, target_param in zip(self.actor.parameters(), self.actor_target.parameters()):\n",
        "\t\t\t\ttarget_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n",
        "\n",
        "\n",
        "\tdef save(self, filename):\n",
        "\t\ttorch.save(self.critic.state_dict(), filename + \"_critic\")\n",
        "\t\ttorch.save(self.critic_optimizer.state_dict(), filename + \"_critic_optimizer\")\n",
        "\n",
        "\t\ttorch.save(self.actor.state_dict(), filename + \"_actor\")\n",
        "\t\ttorch.save(self.actor_optimizer.state_dict(), filename + \"_actor_optimizer\")\n",
        "\n",
        "\n",
        "\tdef load(self, filename):\n",
        "\t\tself.critic.load_state_dict(torch.load(filename + \"_critic\"))\n",
        "\t\tself.critic_optimizer.load_state_dict(torch.load(filename + \"_critic_optimizer\"))\n",
        "\t\tself.critic_target = copy.deepcopy(self.critic)\n",
        "\n",
        "\t\tself.actor.load_state_dict(torch.load(filename + \"_actor\"))\n",
        "\t\tself.actor_optimizer.load_state_dict(torch.load(filename + \"_actor_optimizer\"))\n",
        "\t\tself.actor_target = copy.deepcopy(self.actor)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "JWg4ZB6XlrwR"
      },
      "outputs": [],
      "source": [
        "def init_flags():\n",
        "\n",
        "    flags = {\n",
        "        \"env\": \"MountainCarContinuous\", # Change to \"MountainCar-v0\" when needed.\n",
        "        \"seed\":0,\n",
        "        \"start_timesteps\": 25e3,\n",
        "        \"eval_freq\": 5e3,\n",
        "        \"max_timesteps\": 1e6,\n",
        "        \"expl_noise\": 0.1,\n",
        "        \"batch_size\": 256,\n",
        "        \"discount\":0.99,\n",
        "        \"tau\": 0.005,\n",
        "        \"policy_noise\": 0.2,\n",
        "        \"noise_clip\":0.5,\n",
        "        \"policy_freq\": 2,\n",
        "        \"save_model\": \"store_true\"\n",
        "    }\n",
        "\n",
        "    return flags\n",
        "\n",
        "def main(policy_name = 'DDPG'):\n",
        "\n",
        "    args = init_flags()\n",
        "    env = gym.make(args[\"env\"])\n",
        "    # env.seed(args[\"seed\"])\n",
        "    env.action_space.seed(args[\"seed\"])\n",
        "    torch.manual_seed(args[\"seed\"])\n",
        "    np.random.seed(args[\"seed\"])\n",
        "\n",
        "    state_dim = env.observation_space.shape[0]\n",
        "    action_dim = env.action_space.shape[0]\n",
        "    max_action = float(env.action_space.high[0])\n",
        "    kwargs = {\n",
        "        \"state_dim\": state_dim,\n",
        "        \"action_dim\": action_dim,\n",
        "        \"max_action\": max_action,\n",
        "        \"discount\": args[\"discount\"],\n",
        "        \"tau\": args[\"tau\"],}\n",
        "    if policy_name == \"TD3\":\n",
        "        # Target policy smoothing is scaled wrt the action scale\n",
        "        kwargs[\"policy_noise\"] = args[\"policy_noise\"] * max_action\n",
        "        kwargs[\"noise_clip\"] = args[\"noise_clip\"] * max_action\n",
        "        kwargs[\"policy_freq\"] = args[\"policy_freq\"]\n",
        "        policy = TD3(**kwargs)\n",
        "    elif policy_name == \"DDPG\":\n",
        "        policy = DDPG(**kwargs)\n",
        "\n",
        "    replay_buffer = ReplayBuffer(state_dim, action_dim)\n",
        "    evaluations = [eval_policy(policy, args[\"env\"], args[\"seed\"])]\n",
        "    state, done = env.reset(), False\n",
        "    episode_reward = 0\n",
        "    episode_timesteps = 0\n",
        "    episode_num = 0\n",
        "\n",
        "    for t in range(int(args[\"max_timesteps\"])):\n",
        "\n",
        "      episode_timesteps += 1\n",
        "\n",
        "      # Select action randomly or according to policy\n",
        "      if t < args[\"start_timesteps\"]:\n",
        "        action = env.action_space.sample()\n",
        "      else:\n",
        "        action = (\n",
        "          policy.select_action(np.array(state))\n",
        "          + np.random.normal(0, max_action * args[\"expl_noise\"], size=action_dim)\n",
        "        ).clip(-max_action, max_action)\n",
        "\n",
        "      # Perform action\n",
        "      next_state, reward, done, _ = env.step(action)\n",
        "      done_bool = float(done) if episode_timesteps < env._max_episode_steps else 0\n",
        "\n",
        "      # Store data in replay buffer\n",
        "      replay_buffer.add(state, action, next_state, reward, done_bool)\n",
        "\n",
        "      state = next_state\n",
        "      episode_reward += reward\n",
        "\n",
        "      # Train agent after collecting sufficient data\n",
        "      if t >= args[\"start_timesteps\"]:\n",
        "        policy.train(replay_buffer, args[\"batch_size\"])\n",
        "\n",
        "      if done:\n",
        "        # +1 to account for 0 indexing. +0 on ep_timesteps since it will increment +1 even if done=True\n",
        "        print(f\"Total T: {t+1} Episode Num: {episode_num+1} Episode T: {episode_timesteps} Reward: {episode_reward:.3f}\")\n",
        "        # Reset environment\n",
        "        state, done = env.reset(), False\n",
        "        episode_reward = 0\n",
        "        episode_timesteps = 0\n",
        "        episode_num += 1\n",
        "\n",
        "      # Evaluate episode\n",
        "      if (t + 1) % args[\"eval_freq\"] == 0:\n",
        "        evaluations.append(eval_policy(policy, args[\"env\"], args[\"seed\"]))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nouYPAkXozYC"
      },
      "outputs": [],
      "source": [
        "main(policy_name = 'DDPG')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "d_BG0sGGo3oY",
        "outputId": "e0c65ba6-2c71-4ddc-b86b-f959a34537c7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/envs/registration.py:600: UserWarning: \u001b[33mWARN: Using the latest versioned environment `MountainCarContinuous-v0` instead of the unversioned environment `MountainCarContinuous`.\u001b[0m\n",
            "  logger.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------------------------------------\n",
            "Evaluation over 10 episodes: -0.270\n",
            "---------------------------------------\n",
            "Total T: 999 Episode Num: 1 Episode T: 999 Reward: -32.504\n",
            "Total T: 1998 Episode Num: 2 Episode T: 999 Reward: -35.036\n",
            "Total T: 2997 Episode Num: 3 Episode T: 999 Reward: -32.692\n",
            "Total T: 3996 Episode Num: 4 Episode T: 999 Reward: -32.465\n",
            "Total T: 4995 Episode Num: 5 Episode T: 999 Reward: -35.142\n",
            "---------------------------------------\n",
            "Evaluation over 10 episodes: -0.270\n",
            "---------------------------------------\n",
            "Total T: 5994 Episode Num: 6 Episode T: 999 Reward: -32.234\n",
            "Total T: 6993 Episode Num: 7 Episode T: 999 Reward: -34.059\n",
            "Total T: 7099 Episode Num: 8 Episode T: 106 Reward: 96.532\n",
            "Total T: 8098 Episode Num: 9 Episode T: 999 Reward: -33.525\n",
            "Total T: 9097 Episode Num: 10 Episode T: 999 Reward: -33.559\n",
            "---------------------------------------\n",
            "Evaluation over 10 episodes: -0.270\n",
            "---------------------------------------\n",
            "Total T: 10096 Episode Num: 11 Episode T: 999 Reward: -32.441\n",
            "Total T: 11095 Episode Num: 12 Episode T: 999 Reward: -32.332\n",
            "Total T: 12094 Episode Num: 13 Episode T: 999 Reward: -34.181\n",
            "Total T: 13093 Episode Num: 14 Episode T: 999 Reward: -34.245\n",
            "Total T: 14092 Episode Num: 15 Episode T: 999 Reward: -32.436\n",
            "---------------------------------------\n",
            "Evaluation over 10 episodes: -0.270\n",
            "---------------------------------------\n",
            "Total T: 15091 Episode Num: 16 Episode T: 999 Reward: -32.275\n",
            "Total T: 16090 Episode Num: 17 Episode T: 999 Reward: -33.297\n",
            "Total T: 17089 Episode Num: 18 Episode T: 999 Reward: -31.481\n",
            "Total T: 18088 Episode Num: 19 Episode T: 999 Reward: -32.966\n",
            "Total T: 19087 Episode Num: 20 Episode T: 999 Reward: -32.432\n",
            "---------------------------------------\n",
            "Evaluation over 10 episodes: -0.270\n",
            "---------------------------------------\n",
            "Total T: 20086 Episode Num: 21 Episode T: 999 Reward: -31.544\n",
            "Total T: 21085 Episode Num: 22 Episode T: 999 Reward: -33.001\n",
            "Total T: 22084 Episode Num: 23 Episode T: 999 Reward: -32.710\n",
            "Total T: 23083 Episode Num: 24 Episode T: 999 Reward: -33.195\n",
            "Total T: 24082 Episode Num: 25 Episode T: 999 Reward: -31.546\n",
            "---------------------------------------\n",
            "Evaluation over 10 episodes: -0.270\n",
            "---------------------------------------\n",
            "Total T: 25081 Episode Num: 26 Episode T: 999 Reward: -31.309\n",
            "Total T: 26080 Episode Num: 27 Episode T: 999 Reward: -3.470\n",
            "Total T: 27079 Episode Num: 28 Episode T: 999 Reward: -2.691\n",
            "Total T: 28078 Episode Num: 29 Episode T: 999 Reward: -2.082\n",
            "Total T: 29077 Episode Num: 30 Episode T: 999 Reward: -1.719\n",
            "---------------------------------------\n",
            "Evaluation over 10 episodes: -1.337\n",
            "---------------------------------------\n",
            "Total T: 30076 Episode Num: 31 Episode T: 999 Reward: -2.952\n",
            "Total T: 31075 Episode Num: 32 Episode T: 999 Reward: -2.036\n",
            "Total T: 32074 Episode Num: 33 Episode T: 999 Reward: -1.793\n",
            "Total T: 33073 Episode Num: 34 Episode T: 999 Reward: -2.269\n",
            "Total T: 34072 Episode Num: 35 Episode T: 999 Reward: -2.215\n",
            "---------------------------------------\n",
            "Evaluation over 10 episodes: -1.352\n",
            "---------------------------------------\n",
            "Total T: 35071 Episode Num: 36 Episode T: 999 Reward: -2.070\n",
            "Total T: 36070 Episode Num: 37 Episode T: 999 Reward: -2.849\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-9aa8f0d82574>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpolicy_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'TD3'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-8-35460f95d429>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(policy_name)\u001b[0m\n\u001b[1;32m     78\u001b[0m       \u001b[0;31m# Train agent after collecting sufficient data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"start_timesteps\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m         \u001b[0mpolicy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplay_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"batch_size\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-17cd12ceecc2>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, replay_buffer, batch_size)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m \t\t\tnext_action = (\n\u001b[0;32m--> 107\u001b[0;31m                                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactor_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnoise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \t\t\t).clamp(-self.max_action, self.max_action)\n\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "main(policy_name = 'TD3')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
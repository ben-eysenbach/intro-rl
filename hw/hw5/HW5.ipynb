{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Make sure to run every cell in the file in the correct order."
      ],
      "metadata": {
        "id": "pOqaqSFFcuUg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fUDWvikdkd2J"
      },
      "outputs": [],
      "source": [
        "!pip3 install swig > /dev/null 2>&1\n",
        "!pip3 uninstall box2d-py -y > /dev/null 2>&1\n",
        "!pip3 install box2d-py > /dev/null 2>&1\n",
        "!pip3 install box2d box2d-kengz > /dev/null 2>&1\n",
        "!apt install xvfb > /dev/null 2>&1\n",
        "!pip3 install pyvirtualdisplay > /dev/null 2>&1\n",
        "!pip3 install gym==0.25.0 > /dev/null 2>&1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZlGiM8WGkzsi"
      },
      "outputs": [],
      "source": [
        "import gym\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.distributions import Categorical\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "import sys\n",
        "from pyvirtualdisplay import Display\n",
        "from IPython import display as disp\n",
        "import copy\n",
        "from typing import Tuple\n",
        "\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 1: Advantage actor critic (A2C) for ``CartPole``\n",
        "\n",
        "### Tutorial: How it works?\n",
        "When the action space of the environment is finite, we can use A2C to learn the optimal policy. A2C maintains two neural networks during the update: the first one acts as the \"critic\", i.e. it outputs the current value function $V_\\phi(s_t)$, and the second one is the \"actor\", which represents our policy $\\pi_\\theta(\\cdot|s_t): \\mathcal{S} â†’ \\Delta^{\\mathcal{A}}$. The difference between A2C and the original actor critic is that they use different update rules for the actor, and maintain a critic in a different form. Classical actor-critic maintains a Q-function $Q_\\phi$, and uses the following update for the actor: $$\n",
        "\\Delta\\theta = \\eta \\sum_{t} \\nabla_\\theta\\log \\pi_\\theta(a_t|s_t) \\cdot Q_\\phi(s_t,a_t)\n",
        "$$\n",
        "while A2C (in all its variants) usually maintains a V-value functions $V_\\phi$, and updates the actor as the following: $$\n",
        "\\Delta\\theta = \\eta \\sum_{t} \\nabla_\\theta\\log \\pi_\\theta(a_t|s_t) \\cdot \\big(\\hat{Q}(s_t,a_t)  - V_\\phi(s_t)\\big).\n",
        "$$\n",
        "Here $\\hat{Q}(s_t,a_t)$ is a surrogate of Q-value function constructed using the maintained $V_\\phi$, usually in the form of $\\sum_{h\\geq t} \\gamma^{h-t} r(s_h,a_h)$ or $r(s_t,a_t) + \\gamma \\cdot V_\\phi(s_{h+1})$.\n",
        "\n",
        "### Implementation\n",
        "In this exercise, we will implement A2C on the simple OpenAI Gym environment ``CartPole``. Specifically, you need to:\n",
        "\n",
        "(1) Construct a Actor / Critic network. The actor network takes the state as a input and outputs a ``torch.distributions.Categorical`` type distribution on $\\mathcal{A}$ that you can sample from. The critic network takes the state as an input and outputs a real number that represents the V-function of the current state.\n",
        "\n",
        "(2) Train the actor and critic network. For simplicity, we recommend you the following loss function for the critic network:\n",
        "$$\n",
        "\\min_\\phi \\text{mean}_{t}f\\bigg(V_\\phi(s_t, a_t) - \\sum_{h\\geq t} \\gamma^{h-t} r(s_h,a_h)\\bigg),\n",
        "$$\n",
        "where $f$ can either be $x^2$ or $\\ell_1$-smooth function provided by ```torch.nn.SmoothL1Loss```. For the actor network, we also recommend you to use the MC update for $\\hat{Q}$ mentioned above: $$\n",
        "\\Delta\\theta = \\text{mean}_{t} \\nabla_\\theta\\log \\pi_\\theta(a_t|s_t) \\cdot \\bigg(\\sum_{h\\geq t} \\gamma^{h-t} r(s_h,a_h)  - V_\\phi(s_t)\\bigg).\n",
        "$$\n",
        "However, feel free to try out any form of A2C as long as your reward in the training process can reach the threshold of around 450."
      ],
      "metadata": {
        "id": "v_nXWuLQ1Gkp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the following you will implement the actor/critic network. You can choose whatever structure you like, but a three-layer narrow neural network with ReLU activation should suffice to tackle a simple task like CartPole.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "1YMhUJXqbX8i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# [Hint] Implement actor/critic networks.\n",
        "class Actor(nn.Module):\n",
        "    \"\"\"\n",
        "    Input:\n",
        "    num_inputs: int, state size\n",
        "    num_actions: int, action size\n",
        "    Output:\n",
        "    torch.nn.distribution.Categorical that represents the policy\n",
        "    \"\"\"\n",
        "    def __init__(self, num_inputs: int, num_actions: int):\n",
        "        super(Actor, self).__init__()\n",
        "        ############################\n",
        "        # YOUR IMPLEMENTATION HERE #\n",
        "        pass\n",
        "        ############################\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        ############################\n",
        "        # YOUR IMPLEMENTATION HERE #\n",
        "        pass\n",
        "        ############################\n",
        "\n",
        "class Critic(nn.Module):\n",
        "    \"\"\"\n",
        "    Input:\n",
        "    num_inputs: int, state size\n",
        "    Output:\n",
        "    a torch.Tensor variable that represents the V-value\n",
        "    \"\"\"\n",
        "    def __init__(self, num_inputs: int):\n",
        "        super(Critic, self).__init__()\n",
        "        ############################\n",
        "        # YOUR IMPLEMENTATION HERE #\n",
        "        pass\n",
        "        ############################\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        ############################\n",
        "        # YOUR IMPLEMENTATION HERE #\n",
        "        pass\n",
        "        ############################\n"
      ],
      "metadata": {
        "id": "bNvtP0v5YZL4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4d41d4e6-c8a6-42e9-da8f-52611b6d8e6e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following utility function should return a list that stores $\\big(\\sum_{h \\geq t }\\gamma^h r(s_h, a_h)\\big)_{t}$ with $\\big(r(s_t,a_t)\\big)_t$ as the input. You can refer to the policy gradient implementation in HW3.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "2d_qN5gn4Vd9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_returns(rewards: list, gamma: float)-> list:\n",
        "    \"\"\"\n",
        "    Inputs:\n",
        "    gamma: discount factor\n",
        "    rewards: list, reward signal of a trajectory\n",
        "    Outputs:\n",
        "    a list that record the discounted cumulative reward\n",
        "    \"\"\"\n",
        "    ############################\n",
        "    # YOUR IMPLEMENTATION HERE #\n",
        "    pass\n",
        "    ############################"
      ],
      "metadata": {
        "id": "F4IGRnjz0-Ms"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we are ready to train A2C! Specifically, you need to calculate the loss function of the actor and critic in every episode, and optimize them with Adam in every episode."
      ],
      "metadata": {
        "id": "L-uVuhGffZ6v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(env_name: str, num_episodes=500, gamma=0.99, lr=0.001) -> list:\n",
        "    seed = 1234\n",
        "    env = gym.make(env_name)\n",
        "    num_inputs = env.observation_space.shape[0]\n",
        "    num_actions = env.action_space.n\n",
        "\n",
        "    # set random seeds\n",
        "    env.seed(seed);\n",
        "    np.random.seed(seed);\n",
        "    torch.manual_seed(seed);\n",
        "\n",
        "    # Initialize networks and optimizers\n",
        "    actor = Actor(num_inputs, num_actions)\n",
        "    critic = Critic(num_inputs)\n",
        "    actor_opt = optim.Adam(actor.parameters(), lr=lr)\n",
        "    critic_opt = optim.Adam(critic.parameters(), lr=lr)\n",
        "\n",
        "    # record the cumulative rewards in eval\n",
        "    eval = []\n",
        "    for episode in range(num_episodes):\n",
        "        state = env.reset()\n",
        "        log_probs = []\n",
        "        values = []\n",
        "        rewards = []\n",
        "        done = False\n",
        "\n",
        "        while not done:\n",
        "            state = torch.tensor(state).unsqueeze(0)\n",
        "            policy = actor(state)\n",
        "            value = critic(state)\n",
        "            action = policy.sample()\n",
        "            next_state, reward, done, _ = env.step(action.item())\n",
        "            # record the V-function estimate, the log_prob of current actor, and the reward\n",
        "            # for each step\n",
        "            log_prob = policy.log_prob(action)\n",
        "            log_probs.append(log_prob)\n",
        "            values.append(value)\n",
        "            rewards.append(reward)\n",
        "\n",
        "            state = next_state\n",
        "\n",
        "        returns = compute_returns(rewards, gamma)\n",
        "        log_probs = torch.cat(log_probs)\n",
        "        values = torch.cat(values)\n",
        "        returns = torch.tensor(returns)\n",
        "        # Calculate actor and critic loss for A2C.\n",
        "        # 1. Calculate actor loss with advantage version of policy gradient\n",
        "        # 2. Calculate critic loss.\n",
        "        # 3. Add them up. Backward->optimize.\n",
        "        # [HINT] Remember to detach the advantage gradient when calculating actor loss\n",
        "        #        To ensure the policy gradient is properly calculated\n",
        "        ############################\n",
        "        # YOUR IMPLEMENTATION HERE #\n",
        "        pass\n",
        "        ############################\n",
        "\n",
        "        # record the cumulative reward in this episode\n",
        "        undiscounted_returns = compute_returns(rewards, 1)\n",
        "        eval.append(undiscounted_returns[0])\n",
        "        # print every 10 episodes\n",
        "        if episode % 10 == 0:\n",
        "          print(f\"Episode {episode}, Loss: {loss.item()}, Return: {undiscounted_returns[0]}\")\n",
        "    return eval"
      ],
      "metadata": {
        "id": "tMNMrDgv07SC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now test how your implementation works on CartPole! If everything goes well, we would expect a reward of around 500. Plot the reward curve with ``matplotlib.pyplot.plot``. The expected time consumption should be around 5-10 minutes.\n",
        "\n"
      ],
      "metadata": {
        "id": "xV23ldRW6uE9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "eval = train(\"CartPole-v1\")"
      ],
      "metadata": {
        "id": "2Sb6ExTA1CaW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i4slgsZK2HNr"
      },
      "source": [
        "#  Actor-critic variants for continuous action space: DDPG & TD3\n",
        "In this section, we will introduce two modern RL algorithms that aim to tackle MDPs with continuous action space: **Deep Deterministic Policy Gradient** (DDPG) and **Twin Delayed DDPG** (TD3).\n",
        "The workflow for the rest of this assignment is as follows: we will initially implement DDPG  algorithms, and then evaluate their performance on the Gym environment ```MountainCarContinuous```. The implementation for TD3 is **optional**, but we encourage you do do it. By studying these two algorithms, you will gain insight into reinforcement learning in continuous action spaces and become familiar with various techniques in RL.\n",
        "\n",
        "## Question 2. Deep Deterministic Policy Gradient(DDPG)\n",
        "#### **1. Introduction: DDPG v.s. original actor critic**\n",
        "Regular actor-critic algorithms work well when we have only finite actions, as it is essentially learning a mapping from $\\mathcal{S}$ to $\\mathbb{R}^{\\mathcal{A}}$, which can be well-approximated by a neural network with an $\\mathcal{A}$-dimensional output.  However, when it comes to the case that we have infinite candidate actions, e.g. in the scenario of robotic control, how can we learn the optimal action given the current state?\n",
        "\n",
        "In the question, we introduce an actor-critic type algorithm called Deep Deterministic Policy Gradient (DDPG). DDPG maintains 4 neural networks: an estimation $Q_\\phi$ as the critic, a policy network $\\mu_\\theta: \\mathcal{S} \\rightarrow \\mathcal{A}$ as the actor, and two corresponding target networks, $Q_{\\phi'}$ and $\\mu_{\\theta'}$. By deterministic, we mean $\\mu$ does not include randomness, as it is hard to approximate a probability with a continuous support with a simple neural network.\n",
        "\n",
        " There are 3 major differences between DDPG and A2C:\n",
        "\n",
        "(1) **Actor update.** To adapt to continuous action space and deterministic policy, in DDPG, the actor is updated by the continuous version of policy gradient: $$\n",
        "\\Delta \\theta = \\eta\\cdot \\text{mean}_t \\big(\\nabla_a Q_\\phi(s_t, \\mu_\\theta(s_t)) \\cdot \\nabla_\\theta \\mu_\\theta (s_t)\\big),\n",
        "$$\n",
        "also refer to the lecture notes for details.\n",
        "\n",
        "(2) **Replay buffer & Exploration strategy.** To mitigate the catastrophic forgetting issue in actor-critic (as you might have already seen in previous examples), DDPG \"borrows\" the concept of replay buffer from DQN, and correspondingly an exploratory strategy when choosing action. In every step, DDPG samples a $(s_t, a_t, s_{t+1}, r_t)$ from the replay buffer, and update the policy.\n",
        "\n",
        "Similar to the $\\epsilon$-greedy exploration in DQN, DDPG adds noise to the chosen action given by the actor to improve exploration: $$\n",
        "a_t = \\mu_\\theta(s_t) + \\epsilon_t,\n",
        "$$\n",
        "where $\\epsilon_t$ is a Gaussian noise.\n",
        "Then the agent receives the new next_state with reward, and stores it in the replay buffer.\n",
        "\n",
        "(3) **Target network & Critic update.** To mitigate inconsistency during temporal difference backups, DDPG borrows the concept of target network from double DQN. Specifically, in every episode, DDPG updates the critic $Q_\\phi$ by performing gradient descent with the following improved Bellman loss: $$\n",
        "L(\\phi) = \\text{mean}_t f(y_t, Q_\\phi(s_t,a_t)),\n",
        "$$\n",
        "where $y_t = r(s_t, a_t) + \\gamma \\cdot Q_{\\phi'}(s_{t+1}, \\mu_{\\theta'}(s_{t+1}))$. $f$ can either be $x^2$ or $\\ell_1$-smooth function. Note that $y_t$ is calculated by the target networks $Q_{\\phi'}$, and $Q_{\\phi'}$ should not be trained here! This would decrease inconsistency during training. However, different from DQN, which update the target network with a slower frequency,  DDPG chooses to update $Q_{\\phi'}$ and $\\mu_{\\theta'}$ in every episode but with a slower rate after updating $\\phi$ and $\\theta$:\n",
        "$$\n",
        "\\phi' = \\tau \\phi + (1 - \\tau) \\phi', \\qquad \\theta' = \\tau \\theta + (1 - \\tau) \\theta'.\n",
        "$$\n",
        "Usually $\\tau$ is set to be some small constant, e.g. 0.005, to prevent rapid updates to the target networks.\n",
        "\n",
        "Now you must be ready to implement DDPG by yourselves!\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We provide you with the following utility function that implements the replay buffer for experience replay."
      ],
      "metadata": {
        "id": "89mle70cfClJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1bX7EbSNod3a"
      },
      "outputs": [],
      "source": [
        "# Replay buffer\n",
        "class ReplayBuffer(object):\n",
        "\tdef __init__(self, state_dim, action_dim, max_size=int(1e6)):\n",
        "\t\tself.max_size = max_size\n",
        "\t\tself.ptr = 0\n",
        "\t\tself.size = 0\n",
        "\n",
        "\t\tself.state = np.zeros((max_size, state_dim))\n",
        "\t\tself.action = np.zeros((max_size, action_dim))\n",
        "\t\tself.next_state = np.zeros((max_size, state_dim))\n",
        "\t\tself.reward = np.zeros((max_size, 1))\n",
        "\t\tself.not_done = np.zeros((max_size, 1))\n",
        "\n",
        "\t\tself.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "\tdef add(self, state, action, next_state, reward, done):\n",
        "\t\tself.state[self.ptr] = state\n",
        "\t\tself.action[self.ptr] = action\n",
        "\t\tself.next_state[self.ptr] = next_state\n",
        "\t\tself.reward[self.ptr] = reward\n",
        "\t\tself.not_done[self.ptr] = 1. - done\n",
        "\n",
        "\t\tself.ptr = (self.ptr + 1) % self.max_size\n",
        "\t\tself.size = min(self.size + 1, self.max_size)\n",
        "\n",
        "\n",
        "\tdef sample(self, batch_size):\n",
        "\t\tind = np.random.randint(0, self.size, size=batch_size)\n",
        "\n",
        "\t\treturn (\n",
        "\t\t\ttorch.FloatTensor(self.state[ind]).to(self.device),\n",
        "\t\t\ttorch.FloatTensor(self.action[ind]).to(self.device),\n",
        "\t\t\ttorch.FloatTensor(self.next_state[ind]).to(self.device),\n",
        "\t\t\ttorch.FloatTensor(self.reward[ind]).to(self.device),\n",
        "\t\t\ttorch.FloatTensor(self.not_done[ind]).to(self.device)\n",
        "\t\t)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **2. Implementation: Create your own DDPG agent!**\n",
        "\n",
        "You now need to complete the following implementation of DDPG. First, you want to build your ``Actor_DDPG`` and ``Critic_DDPG`` networks. For our task, two three-layer neural network with ReLU activations suffice. Note that different from A2C you implemented above,  ``Critic_DDPG`` approximates the Q-function, thus takes both the state and action as inputs. Here we assume the action space is a symmetric interval ``[- max_action, max_action]``, so please normalize your output with e.g. ``torch.tanh``."
      ],
      "metadata": {
        "id": "-yn9JbTCuG1h"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hn3nFpLqk3wR"
      },
      "outputs": [],
      "source": [
        "# Implementation of Deep Deterministic Policy Gradients (DDPG)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "class Actor_DDPG(nn.Module):\n",
        "\tdef __init__(self, state_dim: int, action_dim: int, max_action: torch.tensor):\n",
        "\t\tsuper(Actor_DDPG, self).__init__()\n",
        "\t  \"\"\"\n",
        "\t\tInputs:\n",
        "    state_dim: int, dimension of state\n",
        "\t\taction_dim: int, dimension of action\n",
        "\t\tOutputs:\n",
        "\t\ta torch.Tensor that represents future action\n",
        "\t\t\"\"\"\n",
        "\t\t# [HINT] Construct a neural network as the actor. Return its value using forward\n",
        "\t\t############################\n",
        "\t\t# YOUR IMPLEMENTATION HERE #\n",
        "\t\tpass\n",
        "\t\t############################\n",
        "\t\tself.max_action = max_action\n",
        "\n",
        "\tdef forward(self, state):\n",
        "\t\t############################\n",
        "\t\t# YOUR IMPLEMENTATION HERE #\n",
        "\t\tpass\n",
        "\t\t############################\n",
        "\n",
        "class Critic_DDPG(nn.Module):\n",
        "\tdef __init__(self, state_dim: int, action_dim: int):\n",
        "\t\tsuper(Critic_DDPG, self).__init__()\n",
        "\t  \"\"\"\n",
        "    Inputs:\n",
        "\t\tsame as the actor\n",
        "\t\tOutputs:\n",
        "\t\ttorch.Tensor that represents the Q-values\n",
        "\t\t\"\"\"\n",
        "\t\t# [HINT] Construct a neural network as the critic. Return its value using forward\n",
        "\t\t############################\n",
        "\t\t# YOUR IMPLEMENTATION HERE #\n",
        "\t\tpass\n",
        "\t\t############################\n",
        "\n",
        "\tdef forward(self, state: torch.Tensor, action: torch.Tensor)->torch.Tensor:\n",
        "\t\t############################\n",
        "\t\tsa = torch.cat([state, action], 1)\n",
        "\t\t############################\n",
        "\t\t# YOUR IMPLEMENTATION HERE #\n",
        "\t\tpass\n",
        "\t\t############################\n",
        "\t\treturn q1"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we are ready to construct a DDPG trainer! In the following cell you will need to:\n",
        "\n",
        "**(1)** Calculate the TD value using target_Q network and update the critic;\n",
        "\n",
        "**(2)** Calculate the deterministic policy gradient and update the actor;\n",
        "\n",
        "**(3)** Update the target networks.\n",
        "\n",
        "Let's do it!"
      ],
      "metadata": {
        "id": "wgrUCZUv0Irw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DDPG(object):\n",
        "\tdef __init__(self, state_dim: int, action_dim: int, max_action: float, discount=0.99, tau=0.001):\n",
        "\t\t# Initialize the models and the target models\n",
        "\t\tself.actor = Actor_DDPG(state_dim, action_dim, max_action).to(device)\n",
        "\t\tself.actor_target = copy.deepcopy(self.actor)\n",
        "\t\tself.actor_optimizer = torch.optim.Adam(self.actor.parameters(), lr=3e-4)\n",
        "\n",
        "\t\tself.critic = Critic_DDPG(state_dim, action_dim).to(device)\n",
        "\t\tself.critic_target = copy.deepcopy(self.critic)\n",
        "\t\tself.critic_optimizer = torch.optim.Adam(self.critic.parameters(), lr=3e-4)\n",
        "\n",
        "\t\tself.discount = discount\n",
        "\t\tself.tau = tau\n",
        "\n",
        "\n",
        "\tdef select_action(self, state):\n",
        "\t\tstate = torch.FloatTensor(state.reshape(1, -1)).to(device)\n",
        "\t\treturn self.actor(state).cpu().data.numpy().flatten()\n",
        "\n",
        "\n",
        "\tdef train(self, replay_buffer, batch_size=64):\n",
        "\t\t# Sample from replay buffer\n",
        "\t\tstate, action, next_state, reward, not_done = replay_buffer.sample(batch_size)\n",
        "\n",
        "\t\t# Compute the target_Q value with critic_target for the batch\n",
        "\t\t# [HINT] target_Q = reward + discount * Q_target(next_state) * not_done\n",
        "\t\t############################\n",
        "\t\t# YOUR IMPLEMENTATION HERE #\n",
        "\t\tpass\n",
        "    ############################\n",
        "\n",
        "\t\t# Get current Q estimate\n",
        "\t\tcurrent_Q = self.critic(state, action)\n",
        "\n",
        "\t\t# Compute critic loss\n",
        "\t\t# [HINT] It should be the an mse_loss or a mean of smooth_l1 function between current Q and target Q\n",
        "\t\t# [HINT] Remember that we should not update the target Q here! Detach the TD target to prevent gradient backprop\n",
        "\t\t############################\n",
        "\t\t# YOUR IMPLEMENTATION HERE #\n",
        "\t\tpass\n",
        "    ############################\n",
        "\n",
        "\t\t# Optimize the critic\n",
        "\t\tself.critic_optimizer.zero_grad()\n",
        "\t\tcritic_loss.backward()\n",
        "\t\tself.critic_optimizer.step()\n",
        "\n",
        "\t\t# Compute actor loss\n",
        "\t\t# [HINT] You can easily compute the loss by critic(state, actor(state)).mean()\n",
        "\t\t############################\n",
        "\t\t# YOUR IMPLEMENTATION HERE #\n",
        "\t\tpass\n",
        " \t\t############################\n",
        "\t\t# Optimize the actor\n",
        "\t\tself.actor_optimizer.zero_grad()\n",
        "\t\tactor_loss.backward()\n",
        "\t\tself.actor_optimizer.step()\n",
        "\n",
        "\t\t# Update the target models\n",
        "\t\t# [HINT] You should calculate the weighted mean of every parameters between critic and critic_target,\n",
        "\t\t#        and the weighted mean of actor and actor_target. Store their values in new_target_params.\n",
        "\t\t# [HINT] The weight for the weighted mean is self.tau\n",
        "\t\tfor param, target_param in zip(self.critic.parameters(), self.critic_target.parameters()):\n",
        "      ############################\n",
        "\t\t\t# YOUR IMPLEMENTATION HERE #\n",
        "      pass\n",
        "\t\t\ttarget_param.data.copy_(new_target_params)\n",
        "\n",
        "\t\tfor param, target_param in zip(self.actor.parameters(), self.actor_target.parameters()):\n",
        "\t\t\t############################\n",
        "\t\t\t# YOUR IMPLEMENTATION HERE #\n",
        "      pass\n",
        "\t\t\ttarget_param.data.copy_(new_target_params)"
      ],
      "metadata": {
        "id": "RfZWG4RL0H7F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **3. See how it works!**\n",
        "Now we are coming to the most exciting part--- try out DDPG (and optionally, TD3) on ``MountainCarContinuous`` environment! The hyperparameters are provided in the following cell. Note that they apply for both DDPG and TD3. To make the training process more stable, we will first randomly sample from the action space to fullfill our replay buffer. So don't panick if you find the reward in the innitial episodes is super low! You are welcome to change angthing in the training process as long as you find it helpful!\n",
        "\n"
      ],
      "metadata": {
        "id": "9QL9inx6Aw93"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JWg4ZB6XlrwR"
      },
      "outputs": [],
      "source": [
        "def init_flags():\n",
        "\n",
        "    flags = {\n",
        "        \"env\": \"MountainCarContinuous\",\n",
        "        \"seed\":0, # random seed\n",
        "        \"start_timesteps\": 25e3, #total steps of free exploration phase\n",
        "        \"max_timesteps\": 6e4, # maximum length of time steps in training\n",
        "        \"expl_noise\": 0.1, # noise strength in exploration\n",
        "        \"batch_size\": 256,\n",
        "        \"discount\":0.99,\n",
        "        \"tau\": 0.005, # rate of target update\n",
        "        \"policy_noise\": 0.2, # policy noise variance when sampling action\n",
        "        \"noise_clip\":0.5, # noise clip rate\n",
        "        \"policy_freq\": 2, # delayed policy update frequency in TD3\n",
        "    }\n",
        "\n",
        "    return flags\n",
        "\n",
        "def main(policy_name = 'DDPG'):\n",
        "\n",
        "    args = init_flags()\n",
        "    env = gym.make(args[\"env\"])\n",
        "    env.seed(args[\"seed\"]+100)\n",
        "    env.action_space.seed(args[\"seed\"])\n",
        "    torch.manual_seed(args[\"seed\"])\n",
        "    np.random.seed(args[\"seed\"])\n",
        "\n",
        "    state_dim = env.observation_space.shape[0]\n",
        "    action_dim = env.action_space.shape[0]\n",
        "    max_action = float(env.action_space.high[0])\n",
        "    kwargs = {\n",
        "        \"state_dim\": state_dim,\n",
        "        \"action_dim\": action_dim,\n",
        "        \"max_action\": max_action,\n",
        "        \"discount\": args[\"discount\"],\n",
        "        \"tau\": args[\"tau\"],}\n",
        "    if policy_name == \"TD3\":\n",
        "        # Target policy smoothing is scaled wrt the action scale\n",
        "        kwargs[\"policy_noise\"] = args[\"policy_noise\"] * max_action\n",
        "        kwargs[\"noise_clip\"] = args[\"noise_clip\"] * max_action\n",
        "        kwargs[\"policy_freq\"] = args[\"policy_freq\"]\n",
        "        policy = TD3(**kwargs)\n",
        "    elif policy_name == \"DDPG\":\n",
        "        policy = DDPG(**kwargs)\n",
        "\n",
        "    replay_buffer = ReplayBuffer(state_dim, action_dim)\n",
        "    evaluations = []\n",
        "    state, done = env.reset(), False\n",
        "    episode_reward = 0\n",
        "    episode_timesteps = 0\n",
        "    episode_num = 0\n",
        "\n",
        "    for t in range(int(args[\"max_timesteps\"])):\n",
        "\n",
        "      episode_timesteps += 1\n",
        "\n",
        "      # Select action randomly or according to policy\n",
        "      if t < args[\"start_timesteps\"]:\n",
        "        action = env.action_space.sample()\n",
        "      else:\n",
        "        action = (\n",
        "          policy.select_action(np.array(state))\n",
        "          + np.random.normal(0, max_action * args[\"expl_noise\"], size=action_dim)\n",
        "        ).clip(-max_action, max_action)\n",
        "\n",
        "      # Perform action\n",
        "      next_state, reward, done, _ = env.step(action)\n",
        "      done_bool = float(done) if episode_timesteps < env._max_episode_steps else 0\n",
        "\n",
        "      # Store data in replay buffer\n",
        "      replay_buffer.add(state, action, next_state, reward, done_bool)\n",
        "\n",
        "      state = next_state\n",
        "      episode_reward += reward\n",
        "\n",
        "      # Train agent after collecting sufficient data\n",
        "      if t >= args[\"start_timesteps\"]:\n",
        "        policy.train(replay_buffer, args[\"batch_size\"])\n",
        "\n",
        "      if done:\n",
        "        # +1 to account for 0 indexing. +0 on ep_timesteps since it will increment +1 even if done=True\n",
        "        print(f\"Total T: {t+1} Episode Num: {episode_num+1} Episode T: {episode_timesteps} Reward: {episode_reward:.3f}\")\n",
        "        evaluations.append(episode_reward)\n",
        "        # Reset environment\n",
        "        state, done = env.reset(), False\n",
        "        episode_reward = 0\n",
        "        episode_timesteps = 0\n",
        "        episode_num += 1\n",
        "\n",
        "    return evaluations\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now run the following cell to see how it works! We would expect a reward that converges to around 90. The estimated wall time for running the whole process is around 10-20 minutes, and you should be able to see a large positive reward at around $5\\cdot 10^4$ timesteps. If the innitialization is unsuccessful, which could result in the reward being stuck at around $0$, try restart the ``main`` function or debug your trainer."
      ],
      "metadata": {
        "id": "VgnYEQx3FH9o"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nouYPAkXozYC"
      },
      "outputs": [],
      "source": [
        "evaluations_ddpg = main(policy_name = 'DDPG')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plot your reward v.s. training_episode curve. You can use the evaluations above and ``plt.plot()``.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "BIYrmRtpDn-d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Question 3. TD3 [Optional]\n",
        "#### 1. Tackle over-estimation: Twin Delayed DDPG (TD3)\n",
        "Similar to DDPG, TD3 also maintains a continuous actor update with a deterministic policy gradient and a replay buffer with an exploratory strategy. The difference between TD3 and DDPG lies in the critic update. Unlike Double DQN, due to the slow update of the policy (the $\\theta$ in previous examples), the current and target values still remain similar even when using a double Q-update technique. While the implementation of an independent target network allows for less biased value estimation, even an unbiased estimate with high variance can still lead to future overestimations in local regions of state space, which in turn can negatively affect the global policy.\n",
        "\n",
        "To address this issue, TD3 updates the actor/critic network in the following way:\n",
        "\n",
        "**(1)** TD3 maintains **6 neural networks**: $Q_{\\phi_1}, Q_{\\phi_2}$ as two independent update for the Q function, $Q_{\\phi_1'}, Q_{\\phi_2'}$ as their corresponding target networks, and $\\mu_\\theta, \\mu_{\\theta'}$ as the actor and its target network. $Q_{\\phi_1}$ and $Q_{\\phi_2}$ are independently initialized, and all target networks are initialized to be the same as their corresponding counterparts. When updating Q-functions, in every epidsode, DDPG updates the critic $Q_\\phi$ by performing gradient descent with the following improved Bellamn loss: $$\n",
        "L(\\phi_i) = \\text{mean}_t f(y_t, Q_{\\phi_i}(s_t,a_t)),\n",
        "$$\n",
        "where $y_t = r(s_t, a_t) + \\gamma \\cdot \\min_{i= 1,2}Q_{\\phi_i'}(s_{t+1}, \\tilde{a}_{t+1})$, $\\tilde{a}_t =  \\mu_{\\theta'}(s_{t+1}) + \\text{clip}_{[-c,c]}(\\mathcal{N}(0,\\tilde{\\sigma}))$,  which is different from the TD target in DDPG, as it take the minimum of the two future Q-values and a \"disturbed\" future action. Here we clip $\\tilde{a}_t$ to prevent it from going too far. Intuitively, TD3 evaluates the Q-value of $s_{t+1}$ in a more \"conservative\" way.\n",
        "\n",
        "\n",
        "**(2)** When updating $\\mu_\\theta$, TD3 only utilizes $Q_{\\phi_1}$:$$\n",
        "\\Delta \\theta = \\eta\\cdot \\text{mean}_t \\big(\\nabla_a Q_{\\phi_1}(s_t, \\mu_\\theta(s_t)) \\cdot \\nabla_\\theta \\mu_\\theta (s_t)\\big).\n",
        "$$\n",
        "At the end of every episode, similar to DDPG, all parameters are updated by $$\n",
        "\\phi_i' = \\tau \\phi_i + (1 - \\tau) \\phi_i', \\qquad \\theta' = \\tau \\theta + (1 - \\tau) \\theta'.\n",
        "$$\n",
        "\n",
        "**(3)** The actor update is delayed: it only updates once every several times the critic updates.\n",
        "\n",
        "\n",
        "We understand the introduction above might be a little bit confusing due to the technical complexity. In the following, we will guide you end to end to implement a TD3 training algorithm.\n",
        "\n"
      ],
      "metadata": {
        "id": "PrWigU12Wd88"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####2. Implementation: Build your own TD3!\n",
        "\n",
        "In the following cell, we will implement the actor and critic network for TD3. For the actor and every critic (we need to maintain an additional critic), please make sure it has the same structure as the one in the previous DDPG question so that we can conduct an ablation study.\n",
        "\n",
        "Our implementation for the ``Critic_TD3`` class is slightly different from the previous critic in DDPG: the class function ``forward`` should return two values $q_1$ and $q_2$ given $(s,a)$, while the class function ``Q1`` should return only $q_1$."
      ],
      "metadata": {
        "id": "tpCc0fUfwldo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wJfEi1DHlbdD"
      },
      "outputs": [],
      "source": [
        "# Implementation of Twin Delayed Deep Deterministic Policy Gradients (TD3)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "class Actor_TD3(nn.Module):\n",
        "\tdef __init__(self, state_dim: int, action_dim: int, max_action: float):\n",
        "\t\tsuper(Actor_TD3, self).__init__()\n",
        "\t\t\"\"\"\n",
        "    Inputs: same as DDPG actor\n",
        "\t\tOutputs of forward: torch.Tensor that represents the chosen action\n",
        "\t\t\"\"\"\n",
        "\t\t############################\n",
        "\t\t# YOUR IMPLEMENTATION HERE #\n",
        "\t\t# [HINT] make sure the structure is the same as DDPG\n",
        "\t\tpass\n",
        "\t\t############################\n",
        "\t\tself.max_action = max_action\n",
        "\n",
        "\n",
        "\tdef forward(self, state):\n",
        "\t\t############################\n",
        "\t\t# YOUR IMPLEMENTATION HERE #\n",
        "\t\tpass\n",
        "    ############################\n",
        "\n",
        "\n",
        "class Critic_TD3(nn.Module):\n",
        "\tdef __init__(self, state_dim: int, action_dim: int):\n",
        "\t\tsuper(Critic_TD3, self).__init__()\n",
        "    \"\"\"\n",
        "    Inputs: same as the actor\n",
        "\t\tOutputs: two torch.Tensors that represent Q1 and Q2\n",
        "\t\t\"\"\"\n",
        "\t\t# Q1 architecture\n",
        "\t\t############################\n",
        "\t\t# YOUR IMPLEMENTATION HERE #\n",
        "\t\tpass\n",
        "\n",
        "\t\t# Q2 architecture\n",
        "\t\t############################\n",
        "\t\t# YOUR IMPLEMENTATION HERE #\n",
        "\t\tpass\n",
        "\t\t############################\n",
        "\n",
        "\tdef forward(self, state: torch.Tensor, action: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "\t\tsa = torch.cat([state, action], 1)\n",
        "\t\t############################\n",
        "\t\t# YOUR IMPLEMENTATION HERE #\n",
        "\t\tpass\n",
        "\t\t############################\n",
        "\t\treturn q1, q2\n",
        "\n",
        "  # Implement a function that returns only Q1.\n",
        "\t# This is helpful when calculating actor loss\n",
        "\tdef Q1(self, state: torch.Tensor, action: torch.Tensor) -> torch.Tensor:\n",
        "\t\t# [HINT] only returns q1 for actor update\n",
        "\t\t############################\n",
        "\t\t# YOUR IMPLEMENTATION HERE #\n",
        "\t\tpass\n",
        "\t  ############################\n",
        "\t\treturn q1\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's implement the TD3 trainer! In the following cell, you will need to implement the following:\n",
        "\n",
        "**(1)** For the critic update of TD3, when sampled a tuple from the replay buffer, recall that we need to estimate $Q(s_{t+1},\\tilde{a}_{t+1})$, where $\\tilde{a}_{t+1} = \\mu_{\\theta'}(s_{t+1})+\\text{clip}_{[-c,c]}(\\epsilon)$ .\n",
        "\n",
        "**(2)** Calculate the TD target with the networks $Q_{\\phi_i'}, i=1,2,$ and $(s_{t+1}, \\tilde{a}_{t+1})$ you obtained in **(1)**. Recall that the TD target = $r(s_t,a_t) + \\min_{i=1,2}\\{Q_{\\phi_i'}(s_{t+1}, \\tilde{a}_{t+1})\\}$.\n",
        "\n",
        "**(3)** Calculate the actor loss with $Q_{\\phi_1}$.\n",
        "\n",
        "**(4)** Update the parameters $\\phi_i'$, $\\theta'$."
      ],
      "metadata": {
        "id": "1yAS1Ue_07G3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TD3(object):\n",
        "\tdef __init__(\n",
        "\t\tself,\n",
        "\t\tstate_dim: int,\n",
        "\t\taction_dim: int,\n",
        "\t\tmax_action: float,\n",
        "\t\tdiscount=0.99,\n",
        "\t\ttau=0.005,\n",
        "\t\tpolicy_noise=0.2,\n",
        "\t\tnoise_clip=0.5,\n",
        "\t\tpolicy_freq=2\n",
        "\t):\n",
        "\n",
        "\t\tself.actor = Actor_TD3(state_dim, action_dim, max_action).to(device)\n",
        "\t\tself.actor_target = copy.deepcopy(self.actor)\n",
        "\t\tself.actor_optimizer = torch.optim.Adam(self.actor.parameters(), lr=3e-4)\n",
        "\n",
        "\t\tself.critic = Critic_TD3(state_dim, action_dim).to(device)\n",
        "\t\tself.critic_target = copy.deepcopy(self.critic)\n",
        "\t\tself.critic_optimizer = torch.optim.Adam(self.critic.parameters(), lr=3e-4)\n",
        "\n",
        "\t\tself.max_action = max_action\n",
        "\t\tself.discount = discount\n",
        "\t\tself.tau = tau\n",
        "\t\tself.policy_noise = policy_noise\n",
        "\t\tself.noise_clip = noise_clip\n",
        "\t\tself.policy_freq = policy_freq\n",
        "\n",
        "\t\tself.total_it = 0\n",
        "\n",
        "\n",
        "\tdef select_action(self, state):\n",
        "\t\tstate = torch.FloatTensor(state.reshape(1, -1)).to(device)\n",
        "\t\treturn self.actor(state).cpu().data.numpy().flatten()\n",
        "\n",
        "\n",
        "\tdef train(self, replay_buffer, batch_size=256):\n",
        "\t\tself.total_it += 1\n",
        "\n",
        "\t\t# Sample replay buffer\n",
        "\t\tstate, action, next_state, reward, not_done = replay_buffer.sample(batch_size)\n",
        "\n",
        "\t\twith torch.no_grad():\n",
        "\t\t\t# 1. Select action according to policy and add clipped noise.\n",
        "      # [HINT]: Return the action from the actor and add truncated noise.\n",
        "\t\t\t# [HINT]: The variance of the Gaussian noise is self.policy_noise for every dimension.\n",
        "\t\t\t# [HINT]: You can use ().clamp(-self.noise_clip, self.noise_clip) to clip the noise\n",
        "      ############################\n",
        "      # YOUR IMPLEMENTATION HERE #\n",
        "\t\t\tpass\n",
        "      ############################\n",
        "\t\t\t# Compute the target Q value\n",
        "\t\t\ttarget_Q1, target_Q2 = self.critic_target(next_state, next_action)\n",
        "\n",
        "\t\t\t# 2. Compute the target_Q here\n",
        "\t\t\t# [HINT]: In TD3 we use min(Q_1,Q_2) as the estimate of the next step Q.\n",
        "      ############################\n",
        "      # YOUR IMPLEMENTATION HERE #\n",
        "\t\t\tpass\n",
        "      ############################\n",
        "\t\t# Get current Q estimates\n",
        "\t\tcurrent_Q1, current_Q2 = self.critic(state, action)\n",
        "\n",
        "\t\t# Compute critic loss\n",
        "\t\tcritic_loss = F.mse_loss(current_Q1, target_Q) + F.mse_loss(current_Q2, target_Q)\n",
        "\n",
        "\t\t# Optimize the critic\n",
        "\t\tself.critic_optimizer.zero_grad()\n",
        "\t\tcritic_loss.backward()\n",
        "\t\tself.critic_optimizer.step()\n",
        "\n",
        "\t\t# Delayed policy updates\n",
        "\t\tif self.total_it % self.policy_freq == 0:\n",
        "\n",
        "\t\t\t# Compute actor loss\n",
        "      ############################\n",
        "      # YOUR IMPLEMENTATION HERE #\n",
        "\t\t\tpass\n",
        "      ############################\n",
        "\n",
        "\t\t\t# Optimize the actor\n",
        "\t\t\tself.actor_optimizer.zero_grad()\n",
        "\t\t\tactor_loss.backward()\n",
        "\t\t\tself.actor_optimizer.step()\n",
        "\n",
        "\t\t\t# Update the frozen target models using weighted mean\n",
        "\t\t\t# [HINT]: the weight is given by self.tau\n",
        "\t\t\tfor param, target_param in zip(self.critic.parameters(), self.critic_target.parameters()):\n",
        "      ############################\n",
        "      # YOUR IMPLEMENTATION HERE #\n",
        "\t\t\t\tpass\n",
        "\t\t\t\ttarget_param.data.copy_(new_target_params)\n",
        "\n",
        "\t\t\tfor param, target_param in zip(self.actor.parameters(), self.actor_target.parameters()):\n",
        "      ############################\n",
        "\t\t\t# YOUR IMPLEMENTATION HERE #\n",
        "\t\t\t\tpass\n",
        "\t\t\t\ttarget_param.data.copy_(new_target_params)\n"
      ],
      "metadata": {
        "id": "9Lh09MrF022-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d_BG0sGGo3oY"
      },
      "outputs": [],
      "source": [
        "evaluation_td3 = main(policy_name = 'TD3')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
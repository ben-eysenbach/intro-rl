{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lbad9UM0y_Ay"
      },
      "source": [
        "# **Homework 5: Proximal Policy Optimization (PPO)**\n",
        "\n",
        "## **Objective**\n",
        "In this assignment, we will implement **Proximal Policy Optimization (PPO)** for the [CartPole-v1](https://gymnasium.farama.org/environments/classic_control/cart_pole/) environment from OpenAI Gym. Along the way, we will incorporate several important policy gradient concepts:\n",
        "\n",
        "- **REINFORCE** (Monte Carlo Policy Gradient)\n",
        "- **Importance Weighting (IW)** (For Off-Policy Learning)\n",
        "- **Generalized Advantage Estimation (GAE)** (To Balance Bias and Variance)\n",
        "\n",
        "*You should run this assignment on Google Colab.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bi0VhhMN0vzT"
      },
      "source": [
        "## **Understanding PPO with Mathematical Formulation**\n",
        "\n",
        "### **Vanilla Policy Gradient (REINFORCE)**\n",
        "The **policy gradient** method optimizes the policy by maximizing the expected cumulative reward using the gradient:\n",
        "\n",
        "$$\n",
        "\\sum_{h\\geq 0} \\nabla_\\theta\\log \\pi_\\theta(A_h|S_h) \\cdot \\sum_{t\\geq h}\\gamma^{t-h} r(S_t, A_t)\n",
        "$$\n",
        "\n",
        "Here, we compute gradients based on **Monte Carlo estimates** of rewards. This approach has high variance and slow convergence.\n",
        "\n",
        "---\n",
        "\n",
        "### **Proximal Policy Optimization (PPO)**\n",
        "#### **GAE**\n",
        "To reduce variance, we replace returns with **Generalized Advantage Estimation (GAE)**:\n",
        "\n",
        "$$\n",
        "A_t = \\sum_{l=0}^{T} (\\gamma \\lambda)^l \\delta_{t+l}\n",
        "$$\n",
        "\n",
        "where\n",
        "\n",
        "$$\n",
        "\\delta_t = r_t + \\gamma V(s_{t+1}) - V(s_t)\n",
        "$$\n",
        "\n",
        "This approach balances bias and variance in the advantage estimation.\n",
        "\n",
        "#### **main update**\n",
        "PPO refines the policy update by using a **clipped surrogate objective** that restricts how far the new policy can deviate from the old one:\n",
        "\n",
        "$$\n",
        "L^{CLIP}(\\theta) = \\mathbb{E} \\left[ \\min\\left(r_t(\\theta) A_t, \\ \\text{clip}\\left(r_t(\\theta), 1-\\epsilon, 1+\\epsilon\\right) A_t\\right) \\right]\n",
        "$$\n",
        "\n",
        "with\n",
        "\n",
        "$$\n",
        "r_t(\\theta) = \\frac{\\pi_{\\theta}(a_t | s_t)}{\\pi_{\\theta_{\\text{old}}}(a_t | s_t)}\n",
        "$$\n",
        "\n",
        "This clipping ensures that updates remain within a **trust region**, enhancing stability.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K_iLh_UdcVhb"
      },
      "source": [
        "## 1. Setup\n",
        "\n",
        "First, let's install and import the necessary packages. Run the following in a code cell:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cUn56vtUyhtQ"
      },
      "outputs": [],
      "source": [
        "!pip install gymnasium==1.0.0 torch numpy matplotlib seaborn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UaWgeDI0yrE1"
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.distributions import Categorical\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import seaborn as sns\n",
        "sns.set(style=\"darkgrid\")\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rZZSMrM6Z_Gd"
      },
      "source": [
        "The following helper function that evaluates a given policy is provided."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1zIfQVUl3OZv"
      },
      "outputs": [],
      "source": [
        "def evaluate_policy(policy, env_name, seed=42):\n",
        "    env_test = gym.make(env_name)\n",
        "    # env_test.seed(seed)\n",
        "    state, _ = env_test.reset()\n",
        "    done = False\n",
        "    total_reward = 0\n",
        "    while not done:\n",
        "        state = torch.FloatTensor(state).unsqueeze(0)\n",
        "        dist = policy(state)\n",
        "        next_state, reward, done, _, _ = env_test.step(dist.sample().item())\n",
        "        state = next_state\n",
        "        total_reward += reward\n",
        "    return total_reward"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EcFmSceAzIv_"
      },
      "source": [
        "## 2. Defining the Policy and Value Networks\n",
        "\n",
        "We start from the policy network, which decides on the actions to take, and the value network, which estimates the returns. You need to implement both neural networks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FFPEfb-PzLs8"
      },
      "outputs": [],
      "source": [
        "# TODO: Implement the PolicyNetwork class\n",
        "\n",
        "class PolicyNetwork(nn.Module):\n",
        "\n",
        "  \"\"\"\n",
        "    Implement the Policy Network.\n",
        "\n",
        "    Your task is to complete the initialization of the Policy Network, which maps states to action probabilities.\n",
        "\n",
        "    The network should consist of three fully connected layers:\n",
        "\n",
        "    1. An input layer that takes in state_dim and outputs 128 neurons.\n",
        "    2. A hidden layer with 128 neurons.\n",
        "    3. A final output layer that maps to action_dim, producing logits for each possible action.\n",
        "    4. The activation function for the hidden layers should be ReLU (torch.relu).\n",
        "    5. Implement the forward pass to return a Categorical distribution given state inputs.\n",
        "\n",
        "    Hint: The constructor takes 'state_dim' and 'action_dim' as arguments, representing the dimensions\n",
        "    of the state space and action space, respectively.\n",
        "  \"\"\"\n",
        "  def __init__(self, state_dim, action_dim):\n",
        "      super(PolicyNetwork, self).__init__()\n",
        "      ##### Code implementation here #####\n",
        "      pass\n",
        "\n",
        "  def forward(self, x):\n",
        "      ##### Code implementation here #####\n",
        "      pass\n",
        "\n",
        "# TODO: Implement the ValueNetwork class\n",
        "class ValueNetwork(nn.Module):\n",
        "  \"\"\"\n",
        "    Implement the Value Network.\n",
        "\n",
        "    Your task is to complete the initialization of the **Value Network**, which maps states to their estimated values.\n",
        "\n",
        "    Network Architecture:\n",
        "    - The network consists of **three fully connected layers**:\n",
        "    1. An **input layer** that takes `state_dim` and outputs **128 neurons**.\n",
        "    2. A **hidden layer** with **128 neurons**.\n",
        "    3. A **final output layer** that produces a **single scalar value** representing the state's estimated value.\n",
        "    4. Activation function for the hidden layers should be ReLU (torch.relu).\n",
        "\n",
        "\n",
        "    Hint: The constructor takes 'state_dim' as an argument, representing the dimension of the state space.\n",
        "  \"\"\"\n",
        "  def __init__(self, state_dim):\n",
        "      super(ValueNetwork, self).__init__()\n",
        "      ##### Code implementation here #####\n",
        "      pass\n",
        "\n",
        "  def forward(self, x):\n",
        "      ##### Code implementation here #####\n",
        "      pass\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f_En8P4u0vza"
      },
      "source": [
        "Below you need to implement GAE. The function should return the advantage estimates for each state-action pair.\n",
        "\n",
        "Hints: refer to the aforementioned TD error formula and note that the advantage is computed recursively."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FPo_hVV80vza"
      },
      "outputs": [],
      "source": [
        "# TODO: Implement the advantage function calculation\n",
        "\n",
        "\"\"\"\n",
        "Introduction: The function `compute_advantages` calculates **returns** using the **Generalized Advantage Estimation (GAE)** method.\n",
        "GAE helps reduce variance while maintaining bias efficiency in reinforcement learning algorithms like PPO.\n",
        "\n",
        "This function follows a **backward recursion** process:\n",
        "1. The function receives `rewards`, `masks`, and `values`, which are **lists** representing a **trajectory** (a sequence of states and rewards).\n",
        "2. The **masks** are used to handle terminal states (they are 0 if the episode ends and 1 otherwise).\n",
        "3. It calculates **delta**, which is the temporal difference (TD) error.\n",
        "4. The function accumulates **advantages** using the **recursive formula** for GAE.\n",
        "5. Finally, it stores the **returns** (advantage + value function) for each step.\n",
        "\n",
        "# Important Notes:\n",
        "- As the recursion computation may be a bit tricky, you can adopt a simplr **for-loop** calculation if you desire.\n",
        "- The function is named `compute_advantages`, but it actually calculates **returns** (advantage + value).\n",
        "- The use of **masks** ensures that advantage propagation stops at the end of an episode.\n",
        "\n",
        "As a reference for your understand, here is how **REINFORCE** computes returns in a recursion way:\n",
        "\n",
        "  ```python\n",
        "  def compute_returns(rewards, gamma=0.99):\n",
        "      returns = []\n",
        "      G = 0\n",
        "      for reward in reversed(rewards):\n",
        "          G = reward + gamma * G\n",
        "          returns.insert(0, G)\n",
        "      return returns\n",
        "  ```\n",
        "\"\"\"\n",
        "\n",
        "def compute_advantages(next_value, rewards, masks, values, gamma=0.99, lambda_gae=0.95):\n",
        "    values = values + [next_value]  # Append bootstrap value for last state\n",
        "    advantages = 0\n",
        "    returns = []\n",
        "\n",
        "    for step in reversed(range(len(rewards))):  # Iterate in reverse (backward pass)\n",
        "        delta = rewards[step] + gamma * values[step + 1] * masks[step] - values[step]\n",
        "\n",
        "        ##### Code implementation here #####\n",
        "        advantages = None\n",
        "\n",
        "        returns.insert(0, advantages + values[step])\n",
        "\n",
        "    return returns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VPdjUPuDzNwk"
      },
      "source": [
        "## 3. Implementing the Training Loop\n",
        "\n",
        "Training in PPO involves collecting data from the environment, computing advantages, and updating the policy and value networks.\n",
        "\n",
        "1. `ppo_iter()` This function generates mini-batches from the collected data, which are then used for gradient updates. This function is provided to you.\n",
        "\n",
        "2. `ppo_update()` This function optimizes the policy and value networks using the Proximal Policy Optimization algorithm. This function applies the core PPO algorithm, using the experiences collected from the environment to perform multiple epochs of updates on the policy and value networks.You need to implement the core parts.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GuBSX55q0vzb"
      },
      "outputs": [],
      "source": [
        "def ppo_iter(mini_batch_size, states, actions, log_probs, returns, advantage):\n",
        "    batch_size = states.size(0)\n",
        "    mini_batches = []\n",
        "\n",
        "    for _ in range(batch_size // mini_batch_size):\n",
        "        rand_ids = torch.randint(0, batch_size, (mini_batch_size,))\n",
        "        mini_batch = states[rand_ids], actions[rand_ids], log_probs[rand_ids], returns[rand_ids], advantage[rand_ids]\n",
        "        mini_batches.append(mini_batch)\n",
        "\n",
        "    return mini_batches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T3d-li082-K5"
      },
      "outputs": [],
      "source": [
        "# TODO: Implement the main PPO update function\n",
        "\n",
        "def ppo_update(policy_net, value_net, optimizer, ppo_epochs, mini_batch_size, states, actions, log_probs, returns, advantages, clip_param=0.2):\n",
        "    \"\"\"\n",
        "    Implement the PPO update algorithm.\n",
        "\n",
        "    This function should perform the optimization of the policy and value networks using the Proximal Policy Optimization (PPO) algorithm.\n",
        "    You'll need to compute the ratio of new and old policy probabilities, apply the clipping technique, and calculate the losses for both the actor (policy network) and critic (value network).\n",
        "\n",
        "    Instructions:\n",
        "    1. Iterate over the number of PPO epochs, which is the number of optimizer.step() with the current collected data.\n",
        "    2. In each epoch, iterate over the mini-batches of experiences.\n",
        "    3. Calculate the new log probabilities of the actions taken, using the policy network.\n",
        "    4. Compute the ratio of new to old probabilities.\n",
        "    5. Apply the PPO clipping technique to the computed ratios.\n",
        "    6. Calculate the actor (policy) and critic (value) losses. You need to check the consistency for variable shapes before calculating the losses.\n",
        "      6.1 Compute the actor loss:\n",
        "        - The **surrogate objective** function is:\n",
        "          \\[\n",
        "          L^{\\text{clip}} = -\\min(\\text{ratio} \\cdot \\text{advantage}, \\text{clipped_ratio} \\cdot \\text{advantage})\n",
        "          \\]\n",
        "        - - The final **actor loss** is the mean of this objective.\n",
        "\n",
        "      6.2 Compute the critic loss:\n",
        "        - The value network should minimize the difference between predicted and actual returns:\n",
        "          \\[\n",
        "          L^{\\text{critic}} = (\\text{return} - \\text{value})^2\n",
        "          \\]\n",
        "        - The final **critic loss** is the mean of this objective.\n",
        "\n",
        "    7. Combine the losses and perform a backpropagation step.\n",
        "\n",
        "    Hints:\n",
        "    - Use `policy_net(state)` to get the distribution over actions for the given states.\n",
        "    - The `dist.log_prob(action)` method calculates the log probabilities of the taken actions according to the current policy.\n",
        "    - The ratio is computed as the exponential of the difference between new and old log probabilities (`(new_log_probs - old_log_probs).exp()`).\n",
        "    - Use `torch.clamp(ratio, 1.0 - clip_param, 1.0 + clip_param)` to clip the ratio between `[1-clip_param, 1+clip_param]`.\n",
        "    - The actor loss is the negative minimum of the clipped and unclipped objective, averaged over all experiences in the mini-batch.\n",
        "    - The critic loss is the mean squared error between the returns and the value estimates from the value network.\n",
        "    - Remember to zero the gradients of the optimizer before the backpropagation step with `optimizer.zero_grad()`.\n",
        "    - After computing the loss and performing backpropagation with `loss.backward()`, take an optimization step with `optimizer.step()`.\n",
        "    \"\"\"\n",
        "    for _ in range(ppo_epochs):\n",
        "        for state, action, old_log_probs, return_, advantage in ppo_iter(mini_batch_size, states, actions, log_probs, returns, advantages):\n",
        "            dist = policy_net(state)\n",
        "            new_log_probs = dist.log_prob(action)\n",
        "\n",
        "            ##### Code implementation here #####\n",
        "            pass\n",
        "            actor_loss = None\n",
        "            critic_loss = None\n",
        "            ##### Code implementation End #####\n",
        "\n",
        "            loss = 0.5 * critic_loss + actor_loss  # You can freely adjust the weight of the critic loss\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WlYLpNVodf7v"
      },
      "source": [
        "Main training loop, which collects data from the environment, computes advantages, and updates the policy and value networks. All these parts are provided to you."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fgO1llI5zTOP"
      },
      "outputs": [],
      "source": [
        "def train(env_name='CartPole-v1', num_steps=1000, mini_batch_size=8, ppo_epochs=4, threshold=400):\n",
        "    env = gym.make(env_name)\n",
        "    state_dim = env.observation_space.shape[0]\n",
        "    action_dim = env.action_space.n\n",
        "\n",
        "    policy_net = PolicyNetwork(state_dim, action_dim)\n",
        "    value_net = ValueNetwork(state_dim)\n",
        "    optimizer = optim.Adam(list(policy_net.parameters()) + list(value_net.parameters()), lr=3e-3)\n",
        "\n",
        "    state, _ = env.reset()\n",
        "    early_stop = False\n",
        "    reward_list = []\n",
        "\n",
        "    for step in range(num_steps):\n",
        "        log_probs = []\n",
        "        values = []\n",
        "        states = []\n",
        "        actions = []\n",
        "        rewards = []\n",
        "        masks = []\n",
        "\n",
        "        # Collect samples under the current policy\n",
        "        for _ in range(2048):\n",
        "            state = torch.tensor(np.array(state), dtype=torch.float32)  # Ensure correct tensor conversion\n",
        "            dist, value = policy_net(state), value_net(state)\n",
        "\n",
        "            action = dist.sample()\n",
        "            next_state, reward, done, _, _ = env.step(int(action.item()))  # Ensure action is an int\n",
        "            log_prob = dist.log_prob(action)\n",
        "\n",
        "            log_probs.append(log_prob.unsqueeze(0))\n",
        "            values.append(value.unsqueeze(0))\n",
        "            rewards.append(torch.tensor([reward], dtype=torch.float32))\n",
        "            masks.append(torch.tensor([1 - done], dtype=torch.float32))\n",
        "            states.append(state.unsqueeze(0))\n",
        "            actions.append(action.unsqueeze(0))  # Fix for actions\n",
        "\n",
        "\n",
        "            state = next_state\n",
        "            if done:\n",
        "                state, _ = env.reset()  # Ensure proper Gym reset handling\n",
        "\n",
        "        next_state = torch.tensor(np.array(next_state), dtype=torch.float32).unsqueeze(0)  # Ensure proper conversion\n",
        "        next_value = value_net(next_state)\n",
        "        returns = compute_advantages(next_value, rewards, masks, values)\n",
        "\n",
        "        returns = torch.cat(returns).detach()\n",
        "        log_probs = torch.cat(log_probs).detach()\n",
        "        values = torch.cat(values).detach()\n",
        "        states = torch.cat(states)\n",
        "        actions = torch.cat(actions)\n",
        "        advantage = returns - values\n",
        "\n",
        "        # Run PPO update for policy and value networks\n",
        "        ppo_update(policy_net, value_net, optimizer, ppo_epochs, mini_batch_size, states, actions, log_probs, returns, advantage)\n",
        "\n",
        "        if step % 1 == 0:\n",
        "            test_reward = np.mean([evaluate_policy(policy_net, env_name) for _ in range(10)])\n",
        "            print(f'Step: {step}\\tReward: {test_reward}')\n",
        "            reward_list.append(test_reward)\n",
        "            if test_reward > threshold:\n",
        "                print(\"Solved!\")\n",
        "                early_stop = True\n",
        "                break\n",
        "    return early_stop, reward_list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MTpPrUBvbs0-"
      },
      "source": [
        "## 4. Training and evaluation\n",
        "\n",
        "You can freely adjust all hyperparameters for better performances. The provided hyperparameters, if implemented correctly, should be able to make rewards close to/higher than 400.\n",
        "\n",
        "Note: Please try several times if you think your code is correct, the learning curves can have some variances over different runs. Present the best run you can get."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aj9OOdtD8l6q"
      },
      "outputs": [],
      "source": [
        "threshold = 400\n",
        "\n",
        "early_stop, reward_list = train(env_name='CartPole-v1', num_steps=100, mini_batch_size=16, ppo_epochs=4, threshold=threshold)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nhhI3U3SbxbZ"
      },
      "source": [
        "### Plot the performance curves"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nJ0gMpE491SK"
      },
      "outputs": [],
      "source": [
        "if not early_stop:\n",
        "    print(\"Not solved in %d steps\"%len(reward_list))\n",
        "\n",
        "# Plot using Seaborn\n",
        "sns.lineplot(x=np.arange(len(reward_list)), y=reward_list, color='salmon', marker='o', linestyle='-', label='PPO training: reach reward %.1f with %d steps'%(threshold, len(reward_list)))\n",
        "\n",
        "# Optional: Adding titles and labels\n",
        "plt.title('Performance Curves')\n",
        "plt.xlabel('Step')\n",
        "plt.ylabel('Reward')\n",
        "plt.legend()\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "gymenv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# HW6: Model-Based RL\n",
        "\n",
        "In this homework assignment, you will be training a simplified version of the PETS algorithm.\n",
        "In particular, you will be implementing a version that does not perform ensembling (as all associated operations are much more computationally demanding with ensembling, requiring GPU usage).\n",
        "In the end, you will be answering a few short questions to help you develop your intuition for model-based methods in general.\n",
        "\n",
        "**Instructions**: We are no longer requiring you to copy code blocks into a LaTeX template. Instead, simply submit a PDF rendering of this .ipynb file to Gradescope. As usual, we recommend running everything on Google Colab so you will have access to a standardized environment within which this notebook has been tested.\n",
        "\n",
        "If you run into any issues with submission/rendering, please post on Ed or email your preceptor. Furthermore, when submitting, **please double check that all lines are fully visible in the rendered PDF for the code blocks you have written.**"
      ],
      "metadata": {
        "id": "BF3VivspmiP8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u0s8ZwJsmcxD"
      },
      "outputs": [],
      "source": [
        "from typing import Callable, Dict\n",
        "\n",
        "import gym\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Problem 1: Probabilistic ~~Ensembles~~ with Trajectory Sampling\n",
        "\n",
        "In this problem, we will be implementing the necessary tools for performing model-based RL, including model definition, model training, and trajectory prediction.\n",
        "Then, we will put them together to implement a simplified implementation of PETS without ensembling."
      ],
      "metadata": {
        "id": "xVBbzaXOuj6s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1(a): Model Definition: Implementing a Neural Network Dynamics Model with Heteroskedastic Noise\n",
        "\n",
        "For this problem, we will implement a neural network dynamics model with an additive heteroskedastic noise model. Remember that \"heteroskedastic\" here means that the variance of the additive noise is dependent on the input.\n",
        "\n",
        "Formally, we want you do define a model that takes in a batch of state-action pairs $(s, a)$, and outputs a batch of Gaussians $\\mathcal{N}(s + \\mu_\\theta(s, a), \\Sigma_\\theta(s, a))$ representing the distribution of the next state $s'$. **Note that the model predicts the difference between the current state and the mean of the next state!**\n",
        "\n",
        "* The functions $\\mu_\\theta$ and $\\Sigma_\\theta$ should be implemented by a single neural network acting on the input $(s, a)$, whose final output is then partitioned into the mean and covariance components.\n",
        "* $\\Sigma_\\theta$ should be a diagonal covariance matrix, and so your architecture should output a vector that will then be used to fill the diagonal. `torch.diag_embed` performs the conversion from a batch of vectors to a batch of diagonal matrices.\n",
        "\n",
        "Architecture hints:  \n",
        "* We recommend having two hidden layers of 64 neurons each, with Swish/SiLU activations.\n",
        "* Use the `scale_tril` constructor argument to parameterize the covariance of `torch.distributions.MultivariateNormal` so that the architecture computes the standard deviation, see the documentation for details.\n",
        "* The part of the output that will represent the standard deviations should be passed through a softplus (with an added small constant like `1e-9` afterwards) to ensure non-negativity.\n",
        "* Shape hint: A batched `torch.distributions.MultivariateNormal` would have a mean argument of shape `[batch_size, sample_dim]` and a covariance argument of shape `[batch_size, sample_dim, sample_dim]`.\n",
        "\n",
        "You can experiment with the architecture (we will not penalize for using something else), but this should be sufficient for training the agents in this assignment."
      ],
      "metadata": {
        "id": "IFzBIT4Dmp-u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.distributions import Distribution\n",
        "\n",
        "class NNDynModel(nn.Module):\n",
        "  def __init__(self, observation_dim, action_dim):\n",
        "    \"\"\"Initializes a neural network-based dynamics model with the provided\n",
        "    observation and action dimensions.\n",
        "\n",
        "    Args:\n",
        "      observation_dim: Dimension of the input state and the\n",
        "        state to be predicted.\n",
        "      action_dim: Dimension of the input action\n",
        "    \"\"\"\n",
        "    super().__init__()\n",
        "    self.obs_dim, self.action_dim = observation_dim, action_dim\n",
        "    #################################\n",
        "    # Construct model\n",
        "    # YOUR CODE HERE!\n",
        "    #################################\n",
        "\n",
        "  def forward(self, observations, actions) -> Distribution:\n",
        "    \"\"\"Returns a batch of predicted distributions over the next state, given\n",
        "    a batch of the current state and a batch of applied actions.\n",
        "\n",
        "    Args:\n",
        "      observations: Batch of current states of shape [batch_size, observation_dim].\n",
        "      actions: Batch of applied actions of shape [batch_size, action_dim].\n",
        "\n",
        "    Returns:\n",
        "      A Distribution object representing the predicted distributions over the next\n",
        "      state for each pair in the batch.\n",
        "    \"\"\"\n",
        "    arch_inputs = torch.concat([observations, actions], dim=-1)\n",
        "    #################################\n",
        "    # Compute model output distributions\n",
        "    # YOUR CODE HERE!\n",
        "    #################################\n"
      ],
      "metadata": {
        "id": "jLIVhAmLmlG-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1(b) Completing a Model Training Loop\n",
        "\n",
        "Having defined a model, we will now write a function that performs dynamics model training.\n",
        "We have written most of the boilerplate code for you, including batch sampling; we simply need you to **fill in the indicated code block for performing a single optimization step**.\n",
        "Assume that we are using maximum likelihood for the loss.\n",
        "\n",
        "Hint: You should not have to implement the log-likelihood function yourself; check the documentation of `torch.distributions` for a simpler alternative."
      ],
      "metadata": {
        "id": "4hDbubkStN0y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(\n",
        "    model: NNDynModel,\n",
        "    dataset: Dict,\n",
        "    batch_size: int,\n",
        "    n_steps: int,\n",
        "    optimizer: torch.optim.Optimizer\n",
        "  ) -> NNDynModel:\n",
        "  \"\"\"Trains a model by taking the specified number of optimizer steps on the given dataset.\n",
        "\n",
        "  Args:\n",
        "    model: Model to train.\n",
        "    dataset: Dataset to train model on.\n",
        "    n_steps: Number of optimization steps to take.\n",
        "    optimizer: An instance of torch.optim.Optimizer for optimizing the given\n",
        "      model.\n",
        "\n",
        "  Returns:\n",
        "    Trained model.\n",
        "  \"\"\"\n",
        "  dataset_size = len(dataset[\"obs\"])\n",
        "\n",
        "  for _ in range(n_steps):\n",
        "    batch_idxs = np.random.randint(dataset_size, size=(batch_size))\n",
        "    batch_ob = torch.Tensor(dataset[\"obs\"][batch_idxs])\n",
        "    batch_ac = torch.Tensor(dataset[\"actions\"][batch_idxs])\n",
        "    batch_next_ob = torch.Tensor(dataset[\"next_obs\"][batch_idxs])\n",
        "\n",
        "    #################################\n",
        "    # Take an optimization step\n",
        "    # YOUR CODE HERE!\n",
        "    #################################\n",
        "\n",
        "  return model"
      ],
      "metadata": {
        "id": "N1jAEDWptLXC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1(c) Trajectory Prediction\n",
        "\n",
        "For this problem, you will be implementing a trajectory prediction procedure.\n",
        "The function below, given a batch of action sequences, returns the model-predicted discounted return for each sequence, where each sequence's prediction is averaged over `preds_per_seq` particles. See the docstring for more details on the input arguments.\n",
        "\n",
        "To implement multiple particle sampling for each sequence, a trick that we can use is tiling each input action sequence to have `preds_per_seq` copies.\n",
        "This means that with an input batch of `N` action sequences, we will be performing trajectory prediction for `N * preds_per_seq` sequences simultaneously.\n",
        "We then use a reshaping trick to take the mean over the particles corresponding to each input action sequence.\n",
        "**Note that we have implemented all this for you**.\n",
        "\n",
        "**All you need to implement are the updates to the loop variables** (i.e. `cur_states`, `reward_sums`) during this trajectory prediction procedure.\n",
        "Be aware that `cur_states` is of shape `(N * preds_per_seq, state_dim)`, `reward_sums` is of shape `(N * preds_per_seq)`, and `action_seqs` is of shape `(N * preds_per_seq, sequence_length, 1)`, due to the trick described in the previous paragraph."
      ],
      "metadata": {
        "id": "E7n7Hfq8I2Lj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_trajectories(\n",
        "    model: NNDynModel,\n",
        "    initial_state: torch.Tensor,\n",
        "    action_seqs: torch.Tensor,\n",
        "    preds_per_seq: int,\n",
        "    reward_fn: Callable[[torch.Tensor, torch.Tensor], torch.Tensor],\n",
        "    discount: float\n",
        "  ):\n",
        "  \"\"\"Returns predicted sum of rewards and final state after applying action_seqs at initial state.\n",
        "  Averages over preds_per_seq predictions per action sequence, and returns preds_per_seq predictions\n",
        "  of the final state (not averaged).\n",
        "\n",
        "  Args:\n",
        "    model: The model to use for trajectory prediction.\n",
        "    initial_state: The initial state from which the input action sequences will be applied.\n",
        "    action_seqs: The action sequences to be evaluated.\n",
        "    preds_per_seq: The number of particles propagated per action sequence (alternatively,\n",
        "      the number of samples used to compute an MC estimate of the trajectory return per sequence).\n",
        "    reward_fn: Reward function used for evaluation. Must take tensors of [batch_size, state_dim]\n",
        "      and [batch_size, action_dim] (representing the batch states and actions respectively) and\n",
        "      return a tensor of shape [batch_size] (representing the computed batch rewards).\n",
        "    discount: Discount to use.\n",
        "\n",
        "  Returns:\n",
        "    Estimated discounted returns for each action sequence under the provided model.\n",
        "  \"\"\"\n",
        "  n_seqs = action_seqs.shape[0]\n",
        "  seq_len = action_seqs.shape[1]\n",
        "\n",
        "  action_seqs = action_seqs[:, None].tile((1, preds_per_seq, 1, 1)).reshape(n_seqs * preds_per_seq, seq_len, -1)   # Tiling magic\n",
        "  cur_states = initial_state[None].tile((n_seqs * preds_per_seq, 1))\n",
        "  reward_sums = torch.zeros(n_seqs * preds_per_seq)\n",
        "\n",
        "  # At this point, we have that\n",
        "  # cur_states.shape = [n_seqs * preds_per_seq, state_dim]\n",
        "  # action_seqs.shape = [n_seqs * preds_per_seq, seq_len, action_dim]\n",
        "  # reward_sums.shape = [n_seqs * preds_per_seq]\n",
        "  #\n",
        "  # Note that the batch dimension is multiplied by preds_per_seq; the trick\n",
        "  #   we are using here is duplicating everything preds_per_seq times (to create\n",
        "  #   the particles for each action sequence).\n",
        "  # At this point, just pretend that you are sampling a single trajectory prediction\n",
        "  #   for each of the n_seqs * preds_per_seq sequences when filling in the\n",
        "  #   code block below.\n",
        "\n",
        "  with torch.no_grad():    # We are not backpropagating through time, speed up by not maintaining gradients\n",
        "    for t in range(action_seqs.shape[1]):\n",
        "      #################################\n",
        "      # Update relevant loop variables\n",
        "      # YOUR CODE HERE!\n",
        "      #################################\n",
        "\n",
        "  reward_sums = reward_sums.reshape((n_seqs, preds_per_seq)).mean(dim=-1)\n",
        "\n",
        "  return reward_sums"
      ],
      "metadata": {
        "id": "SBt3USqCvw45"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1(d) Model Predictive Control\n",
        "\n",
        "For this problem, **fill in the code block to implement the MPC action selection rule described in lecture**.\n",
        "Recall that MPC samples a set of action sequences, evaluates each one, then returns only the first action of the best action sequence.\n",
        "For the action sequence proposal distribution, use a uniform distribution over the box of valid actions for the environment for every action in the sequence. We have defined the variables `center` and `half_width` for your convenience when performing sampling.\n",
        "\n",
        "Note: **Do not hard code environment-related constants**; for example, if you need the dimension of the action space, you can use relevant properties of `env.action_space` (see the Gym documentation for more information about state/action spaces)."
      ],
      "metadata": {
        "id": "sHFYK05TO7lB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_mpc_policy(env, model, reward_fn, discount, plan_length, n_samples, n_particles):\n",
        "  \"\"\"Creates a policy implementing MPC.\n",
        "\n",
        "  Args:\n",
        "    env: The environment the created policy will be used for.\n",
        "    reward_fn: The reward function used by MPC for evaluating action sequences.\n",
        "    discount: Discount used by MPC for evaluating action sequences.\n",
        "    plan_length: The length of action sequences considered by MPC.\n",
        "    n_samples: The number of candidate action sequences sampled by MPC\n",
        "    n_particles: The number of particles used by MPC to evaluate every action sequence.\n",
        "\n",
        "  Returns:\n",
        "    A policy implementing MPC, represented as a function taking in a state and\n",
        "    returning an action.\n",
        "  \"\"\"\n",
        "  # The action space is a box; center gives the center of the box, while\n",
        "  #   half_width gives the distance from the center to either end for each dimension.\n",
        "  # Ex. for an action space [-n, n]^d, center = zero vector in d dimensions,\n",
        "  #   half_width = vector of all n's in d dimensions.\n",
        "  center = torch.Tensor((env.action_space.high + env.action_space.low) / 2)\n",
        "  half_width = torch.Tensor((env.action_space.high - env.action_space.low) / 2)\n",
        "\n",
        "  def mpc_policy(state: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"Returns the action taken by MPC on the given state.\n",
        "\n",
        "    Args:\n",
        "      state: Input state to policy of shape [state_dim].\n",
        "\n",
        "    Returns:\n",
        "      Action computed by MPC.\n",
        "    \"\"\"\n",
        "    state = torch.Tensor(state)\n",
        "    #################################\n",
        "    # Implement MPC action selection rule\n",
        "    # Hint: Remember to sample actions within each sequence uniformly within the action space\n",
        "    #   (use the center and half_width variables we define above!)\n",
        "    # YOUR CODE HERE!\n",
        "    #################################\n",
        "\n",
        "  return mpc_policy"
      ],
      "metadata": {
        "id": "uahBWLDtv37E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1(e) PETS Training Loop\n",
        "\n",
        "Finally, we will put all of the components we have implemented so far together.\n",
        "**Fill in the indicated code blocks in `train` to complete the PETS training loop for the `Pendulum-v1` environment in Gym**. Here is a brief description of each code block:\n",
        "\n",
        "Code Block 1: Initialize the necessary objects for training (model, policy, model training buffer, and optimizer). We recommend AdamW, with weight decay set to 0.1.  \n",
        "Code Block 2: Obtain an action from a uniformly random policy during exploration episodes (Hint: see `env.action_space` for potentially useful methods).  \n",
        "Code Block 3: Obtain an action from an MPC-based policy.  \n",
        "Code Block 4: Add transition data to the model training buffer.  \n",
        "Code Block 5: Continue training the model.\n",
        "\n",
        "We have provided a `MBRLBuffer` class for you; the `get_training_dataset()` method returns an object that is compatible with the `train_model()` method you had implemented earlier.\n",
        "Furthermore, `pendulum_reward()` computes the same reward function as the `Pendulum-v1` environment itself, and can be used during MPC evaluation.\n",
        "\n",
        "**Note on expected returns**: Due to the randomness of the initial state, the return for this environment is highly variable.\n",
        "For debugging reasons, we have included a print statement which prints the average absolute deviation of the pendulum angle from zero for the last 100 timesteps of each rollout, in addition to the return.\n",
        "Since the objective is to balance the pendulum rod ($\\theta = 0$), your implementation can be said to have learned `Pendulum-v1` if the printed average $\\theta$ deviations are very close to zero.\n",
        "To give you a sense of the magnitude of $\\theta$, $\\theta$ is initialized uniformly in the range $[-\\pi, \\pi]$.\n",
        "\n",
        "**Note on runtime**: Our implementation on Google Colab takes at most five minutes to run five episodes of MPC, which is more than sufficient to learn the task (if not less).\n",
        "\n",
        "**Note on constants**: All necessary constants (e.g. number of MPC particles, number of candidate action sequences, etc.) are provided for you already as optional arguments with default values.\n"
      ],
      "metadata": {
        "id": "WG9jo6OM82Xx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MBRLBuffer:\n",
        "  def __init__(self, observation_dim, action_dim, buffer_size):\n",
        "    self.buffer_size = buffer_size\n",
        "\n",
        "    self.ptr = 0\n",
        "    self.length = 0\n",
        "    self.ob_buffer = np.zeros((buffer_size, observation_dim))\n",
        "    self.action_buffer = np.zeros((buffer_size, action_dim))\n",
        "    self.next_ob_buffer = np.zeros((buffer_size, observation_dim))\n",
        "\n",
        "  def add_datapoint(self, observation, action, next_observation):\n",
        "    self.ob_buffer[self.ptr] = observation\n",
        "    self.action_buffer[self.ptr] = action\n",
        "    self.next_ob_buffer[self.ptr] = next_observation\n",
        "    self.ptr = (self.ptr + 1) % self.buffer_size\n",
        "    self.length = min(self.length + 1, self.buffer_size)\n",
        "\n",
        "  def get_training_dataset(self):\n",
        "    return {\n",
        "        \"obs\": self.ob_buffer[:self.length],\n",
        "        \"actions\": self.action_buffer[:self.length],\n",
        "        \"next_obs\": self.next_ob_buffer[:self.length]\n",
        "    }"
      ],
      "metadata": {
        "id": "3cjGXKchV0zM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pendulum_reward(states: torch.Tensor, actions: torch.Tensor) -> torch.Tensor:\n",
        "  return -(torch.square(torch.atan2(states[:, 1], states[:, 0])) + 0.1 * torch.square(states[:, 2]) + 0.001 * torch.square(actions[:, 0]))\n",
        "\n",
        "def train_pendulum(\n",
        "    n_episodes=10,\n",
        "    n_exploration_episodes=1,\n",
        "    discount=0.99,\n",
        "    mpc_plan_length=15,\n",
        "    n_mpc_candidate_sequences=1000,\n",
        "    n_particles=10,\n",
        "    buffer_length=10000,\n",
        "    model_batch_size=32,\n",
        "    n_model_steps=512\n",
        "  ) -> None:\n",
        "  \"\"\"Trains PETS on the Pendulum-v1 environment.\n",
        "\n",
        "  Args:\n",
        "    n_episodes: Number of episodes collected using MPC\n",
        "    n_exploration_episodes: Number of initial episodes collected using\n",
        "      uniform random actions.\n",
        "    discount: Discount used by MPC and for evaluation.\n",
        "    mpc_plan_length: Length of action sequences considered by MPC.\n",
        "    n_mpc_candidate_sequences: Number of candidate sequences considered by MPC\n",
        "      during the selection process.\n",
        "    n_particles: Number of particles used by MPC per action sequence during evaluation.\n",
        "    buffer_length: Length of model training buffer.\n",
        "    model_batch_size: Batch size during model training.\n",
        "    n_model_steps: Number of optimization steps taken every time the model is\n",
        "      trained after an episode.\n",
        "\n",
        "  Returns:\n",
        "    None\n",
        "  \"\"\"\n",
        "  env = gym.make(\"Pendulum-v1\")\n",
        "  state_dim = env.observation_space.shape[0]\n",
        "  action_dim = env.action_space.shape[0]\n",
        "\n",
        "  #################################\n",
        "  # Create model, policy, training buffer, and optimizer\n",
        "  # YOUR CODE HERE!\n",
        "  #################################\n",
        "\n",
        "\n",
        "  for episode in range(n_exploration_episodes + n_episodes):\n",
        "    reward_sum = 0\n",
        "    deviations = []\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "    timestep = 0\n",
        "\n",
        "    while not done:\n",
        "      if episode < n_exploration_episodes:\n",
        "        #################################\n",
        "        # Select random action for initial exploration episodes\n",
        "        # YOUR CODE HERE!\n",
        "        #################################\n",
        "      else:\n",
        "        #################################\n",
        "        # Use MPC to get an action.\n",
        "        # YOUR CODE HERE!\n",
        "        #################################\n",
        "\n",
        "      next_state, reward, done, _ = env.step(action)\n",
        "      reward_sum += (discount ** timestep) * reward\n",
        "      deviations.append(np.arctan2(next_state[1], next_state[0]))\n",
        "\n",
        "      #################################\n",
        "      # Add transition data to replay buffer\n",
        "      # YOUR CODE HERE!\n",
        "      #################################\n",
        "\n",
        "      timestep += 1\n",
        "      state = next_state\n",
        "\n",
        "    if episode >= n_exploration_episodes:\n",
        "      print(\"Episode {} return: {}; average deviation over final 100 steps: {}\".format(\n",
        "          episode - n_exploration_episodes, reward_sum, np.mean(deviations[-100:])\n",
        "      ))\n",
        "\n",
        "    #################################\n",
        "    # Continue training the model\n",
        "    # YOUR CODE HERE!\n",
        "    #################################"
      ],
      "metadata": {
        "id": "HLWz1v6H8yTW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_pendulum()"
      ],
      "metadata": {
        "id": "Ydoy-E99XPeW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Problem 2: Short Answers\n",
        "\n",
        "Please keep your responses brief; **most of the justifications for these questions can be answered in one or two sentences**."
      ],
      "metadata": {
        "id": "fEnqddIlvXmd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2(a) MPC Horizon\n",
        "\n",
        "One of the hyperparameters that needs to be chosen for MPC is the length of the action sequences that are evaluated at every step (i.e. the planning horizon). We have listed four factors that should be considered when choosing this hyperparameter. Explain **how each consideration affects the choice of MPC horizon and why**."
      ],
      "metadata": {
        "id": "ZAWu01cmvc8N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### (i) Difficulty of sampling a good action sequence\n",
        "\n",
        "> YOUR ANSWER HERE!\n",
        "\n"
      ],
      "metadata": {
        "id": "kEPILh4IxqDZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### (ii) MPC evaluation method vs. actual RL objective\n",
        "\n",
        "> YOUR ANSWER HERE!\n"
      ],
      "metadata": {
        "id": "6kEb0okFtpLt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### (iii) Model accuracy\n",
        "\n",
        "> YOUR ANSWER HERE!"
      ],
      "metadata": {
        "id": "Q1GKA1dNtzrX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### (iv) Wall-clock-time, applying MPC to a non-simulated system\n",
        "\n",
        "> YOUR ANSWER HERE!"
      ],
      "metadata": {
        "id": "zA4uiD8St7Ha"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2(b) Exploration Bonuses with Ensembles, Part I\n",
        "\n",
        "Consider an agent who is running MPC, but is evaluating each action sequence differently than the procedure discussed in lecture. More concretely, for a particular action sequence, let us say that the sampled returns computed from particle-based sampling is given by $(R_{i, j})$ for $i = 1, \\dots, M$ and $j = 1, \\dots, N$, such that $R_{i, 1}, R_{i, 2}, \\dots, R_{i, N}$ are the predictions from the $i^{\\text{th}}$ model of the ensemble.\n",
        "For convenience, let us define the within-model mean $\\mu_i$ and within-model standard deviation $\\sigma_i$ for $i = 1, \\dots, M$ as\n",
        "\n",
        "$$\\mu_i := \\frac{1}{N}\\sum_{j = 1}^{N}R_{i, j} \\quad \\text{and} \\quad \\sigma_i := \\sqrt{\\frac{1}{N - 1}\\sum_{j = 1}^{N}(R_{i, j} - \\mu_i)^2}$$\n",
        "\n",
        "The standard MPC evaluation procedure computes the quantity\n",
        "\n",
        "$$\\hat{R}_{\\mathrm{basic}} := \\frac{1}{M}\\sum_{i = 1}^{M}\\mu_i,$$\n",
        "\n",
        "which is equivalently the average over all particles.\n",
        "This agent instead computes the evaluation metric\n",
        "\n",
        "$$\\hat{R}_{\\mathrm{proposed}} := \\frac{1}{M}\\sum_{i = 1}^{M}(\\mu_i + \\sigma_i)$$\n",
        "\n",
        "to add some \"exploration bonus\" by encouraging the agent to go where there is more observed noise (analogous to the UCB bonus for bandits).\n",
        "**Does this bonus make sense as an exploration bonus? Why or why not?**\n",
        "\n",
        "Hint: Think about the fact that the standard deviations are *within-model* standard deviations. Where does the noise in within-model predictions come from?"
      ],
      "metadata": {
        "id": "ZWoG5-pazCrk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> YOUR ANSWER HERE!"
      ],
      "metadata": {
        "id": "VxRO5FOKuKqL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2(c) Exploration Bonuses with Ensembles, Part II\n",
        "\n",
        "We work in the same setting as 2(b), but now let us define the across-ensemble standard deviation $\\sigma_{\\mathrm{Ens}}$ as the standard deviation of the within-model means, or written mathematically,\n",
        "\n",
        "$$\\sigma_{\\mathrm{Ens}} := \\sqrt{\\frac{1}{M - 1}\\sum_{i = 1}^{M}(\\mu_i - \\hat{R}_\\mathrm{basic})^2}.$$\n",
        "\n",
        "The agent now uses the evaluation metric\n",
        "\n",
        "$$\\hat{R}_{\\mathrm{proposed}} := \\hat{R}_{basic} + \\sigma_{\\mathrm{Ens}}$$\n",
        "\n",
        "as another form of an \"exploration bonus\".\n",
        "Does this new bonus make sense as an exploration bonus? Why or why not?"
      ],
      "metadata": {
        "id": "AjtTOLfl4Hsl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> YOUR ANSWER HERE!"
      ],
      "metadata": {
        "id": "sC_X0u9Hj2cA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2(d) What to use?\n",
        "\n",
        "We will describe several scenarios for which we might want to use RL.\n",
        "In each case, **state whether a model-based or a model-free algorithm would be preferred and why**."
      ],
      "metadata": {
        "id": "3lgjkcikPkPQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Training an expensive but fragile robotic arm for reaching tasks.\n",
        "\n",
        "> YOUR ANSWER HERE!"
      ],
      "metadata": {
        "id": "Sbn5_X-WP4fg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Training an agent to play a video game that can be run massively in parallel.\n",
        "\n",
        "> YOUR ANSWER HERE!\n"
      ],
      "metadata": {
        "id": "QrjzbwqbIvye"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Training an RC car to deliver food to college students anywhere on campus.\n",
        "\n",
        "> YOUR ANSWER HERE!"
      ],
      "metadata": {
        "id": "1q4UEP6BI0FM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2(e) Terminal Value Functions\n",
        "\n",
        "Observe that the standard MPC evaluation procedure only considers the sum of rewards within the action sequence, and does not consider anything after the sequence.\n",
        "One can instead add a terminal value function at the end that is learned alongside the model throughout the training procedure.\n",
        "**Name one pro and one con of learning and using this value function.**\n",
        "\n",
        "Hint: for the drawback, think about the number of samples needed to learn a value function."
      ],
      "metadata": {
        "id": "j6Xn66dmoqCx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **Pro:** YOUR ANSWER HERE!\n"
      ],
      "metadata": {
        "id": "l-Jk6tHgpVu4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **Con:** YOUR ANSWER HERE!"
      ],
      "metadata": {
        "id": "rac3tKBkIoqb"
      }
    }
  ]
}
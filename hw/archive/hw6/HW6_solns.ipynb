{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# HW6: Model-Based RL\n",
        "\n",
        "In this homework assignment, you will be training a simplified version of the PETS algorithm.\n",
        "In particular, you will be implementing a version that does not perform ensembling (as all associated operations are much more computationally demanding with ensembling, requiring GPU usage).\n",
        "In the end, you will be answering a few short questions to help you develop your intuition for model-based methods in general.\n",
        "\n",
        "**Instructions**: We are no longer requiring you to copy code blocks into a LaTeX template. Instead, simply submit a PDF rendering of this .ipynb file to Gradescope. As usual, we recommend running everything on Google Colab so you will have access to a standardized environment within which this notebook has been completely tested.\n",
        "\n",
        "If you run into any issues with submission/rendering, please post on Ed or email your preceptor. Furthermore, when submitting, **please double check that all lines are fully visible in the rendered PDF for the code blocks you have written.**"
      ],
      "metadata": {
        "id": "BF3VivspmiP8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u0s8ZwJsmcxD"
      },
      "outputs": [],
      "source": [
        "from typing import Callable, Dict\n",
        "\n",
        "import gym\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Problem 1: Probabilistic ~~Ensembles~~ with Trajectory Sampling\n",
        "\n",
        "In this problem, we will be implementing the necessary tools for performing model-based RL, including model definition, model training, and trajectory prediction.\n",
        "Then, we will put them together to implement a simplified implementation of PETS without ensembling."
      ],
      "metadata": {
        "id": "xVBbzaXOuj6s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1(a): Model Definition: Implementing a Neural Network Dynamics Model with Heteroskedastic Noise\n",
        "\n",
        "For this problem, we will implement a neural network dynamics model with an additive heteroskedastic noise model. Remember that \"heteroskedastic\" here means that the variance of the additive noise is dependent on the input.\n",
        "\n",
        "Formally, we want you do define a model that takes in a batch of state-action pairs $(s, a)$, and outputs a batch of Gaussians $\\mathcal{N}(s + \\mu_\\theta(s, a), \\Sigma_\\theta(s, a))$ representing the distribution of the next state $s'$. **Note that the model predicts the difference between the current and next state!**\n",
        "\n",
        "* The functions $\\mu_\\theta$ and $\\Sigma_\\theta$ should be implemented by a single neural network acting on the input $(s, a)$, whose final output is then partitioned into the mean and covariance components.\n",
        "* $\\Sigma_\\theta$ should be a diagonal covariance matrix, and so your architecture should output a vector that will then be used to fill the diagonal. `torch.diag_embed` performs the conversion from a batch of vectors to a batch of diagonal matrices.\n",
        "\n",
        "Architecture hints:  \n",
        "* We recommend having two hidden layers of 64 neurons each, with Swish/SiLU activations.\n",
        "* Use the `scale_tril` constructor argument to parameterize the covariance of `torch.distributions.MultivariateNormal` so that the architecture computes the standard deviation, see the documentation for details.\n",
        "* The part of the output that will represent the standard deviations should be passed through a softplus (with an added small constant like `1e-9` afterwards) to ensure non-negativity.\n",
        "\n",
        "You can experiment with the architecture (we will not penalize for using something else), but this should be sufficient for training the agents in this assignment."
      ],
      "metadata": {
        "id": "IFzBIT4Dmp-u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.distributions import Distribution\n",
        "\n",
        "class NNDynModel(nn.Module):\n",
        "  def __init__(self, observation_dim, action_dim):\n",
        "    \"\"\"Initializes a neural network-based dynamics model with the provided\n",
        "    observation and action dimensions.\n",
        "\n",
        "    Args:\n",
        "      observation_dim: Dimension of the input state and the\n",
        "        state to be predicted.\n",
        "      action_dim: Dimension of the input action\n",
        "    \"\"\"\n",
        "    super().__init__()\n",
        "    self.obs_dim, self.action_dim = observation_dim, action_dim\n",
        "    #################################\n",
        "    # Construct model\n",
        "    self.model = nn.Sequential(\n",
        "        nn.Linear(observation_dim + action_dim, 64),\n",
        "        nn.SiLU(),\n",
        "        nn.Linear(64, 64),\n",
        "        nn.SiLU(),\n",
        "        nn.Linear(64, 2 * observation_dim)\n",
        "    )\n",
        "    #################################\n",
        "\n",
        "  def forward(self, observations, actions) -> Distribution:\n",
        "    \"\"\"Returns a batch of predicted distribution over the next state, given\n",
        "    a batch of the current state and a batch of applied actions.\n",
        "\n",
        "    Args:\n",
        "      observations: Batch of current states of shape [N, observation_dim].\n",
        "      actions: Batch of applied actions of shape [N, action_dim].\n",
        "\n",
        "    Returns:\n",
        "      A Distribution object representing the predicted distributions over the next\n",
        "      state for each pair in the batch.\n",
        "    \"\"\"\n",
        "    arch_inputs = torch.concat([observations, actions], dim=-1)\n",
        "    #################################\n",
        "    # Compute model output distributions\n",
        "\n",
        "    arch_outputs = self.model(arch_inputs)\n",
        "\n",
        "\n",
        "    means = arch_outputs[:, 0:self.obs_dim] + observations\n",
        "    stddevs = nn.functional.softplus(arch_outputs[:, self.obs_dim:]) + 1e-9\n",
        "    return torch.distributions.MultivariateNormal(\n",
        "        loc=means,\n",
        "        scale_tril=torch.diag_embed(stddevs)\n",
        "    )\n",
        "    #################################\n"
      ],
      "metadata": {
        "id": "jLIVhAmLmlG-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1(b) Completing a Model Training Loop\n",
        "\n",
        "Having defined a model, we will now write a function that performs dynamics model training.\n",
        "We have written most of the boilerplate code for you, including batch sampling; we simply need you to **fill in the indicated code block for performing a single optimization step**.\n",
        "Assume that we are using maximum likelihood for the loss.\n",
        "\n",
        "Hint: You should not have to implement the log-likelihood function yourself; check the documentation of `torch.distributions` for a simpler alternative."
      ],
      "metadata": {
        "id": "4hDbubkStN0y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(\n",
        "    model: NNDynModel,\n",
        "    dataset: Dict,\n",
        "    batch_size: int,\n",
        "    n_steps: int,\n",
        "    optimizer: torch.optim.Optimizer\n",
        "  ) -> NNDynModel:\n",
        "  \"\"\"Trains a model by taking the specified number of optimizer steps on the given dataset.\n",
        "\n",
        "  Args:\n",
        "    model: Model to train.\n",
        "    dataset: Dataset to train model on.\n",
        "    n_steps: Number of optimization steps to take.\n",
        "    optimizer: An instance of torch.optim.Optimizer for optimizing the given\n",
        "      model.\n",
        "\n",
        "  Returns:\n",
        "    Trained model.\n",
        "  \"\"\"\n",
        "  dataset_size = len(dataset[\"obs\"])\n",
        "\n",
        "  for _ in range(n_steps):\n",
        "    batch_idxs = np.random.randint(dataset_size, size=(batch_size))\n",
        "    batch_ob = torch.Tensor(dataset[\"obs\"][batch_idxs])\n",
        "    batch_ac = torch.Tensor(dataset[\"actions\"][batch_idxs])\n",
        "    batch_next_ob = torch.Tensor(dataset[\"next_obs\"][batch_idxs])\n",
        "\n",
        "    #################################\n",
        "    # Take an optimization step\n",
        "    optimizer.zero_grad()\n",
        "    mean_loss = -torch.mean(model(batch_ob, batch_ac).log_prob(batch_next_ob))\n",
        "    mean_loss.backward()\n",
        "    optimizer.step()\n",
        "    #################################\n",
        "\n",
        "  return model"
      ],
      "metadata": {
        "id": "N1jAEDWptLXC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1(c) Trajectory Prediction\n",
        "\n",
        "For this problem, you will be implementing a trajectory prediction procedure.\n",
        "The function below, given a batch of action sequences, returns the model-predicted discounted return for each sequence, where each sequence's prediction is averaged over `preds_per_seq` particles. See the docstring for more details on the input arguments.\n",
        "\n",
        "To implement multiple particle sampling for each sequence, a trick that we can use is to tile each input action sequence to have `preds_per_seq` copies.\n",
        "This means that with an input batch of `N` action sequences, we will be performing trajectory prediction for `N * preds_per_seq` sequences simultaneously.\n",
        "We then use a reshaping trick to take the mean over the particles corresponding to each action sequence.\n",
        "**Note that we have implemented all this for you**.\n",
        "\n",
        "**All you need to implement are the updates to the loop variables** (i.e. `cur_states`, `reward_sums`) during this trajectory prediction procedure.\n",
        "Be aware that `cur_states` is of shape `(N * preds_per_seq, state_dim)`, `reward_sums` is of shape `(N * preds_per_seq)`, and `action_seqs` is of shape `(N * preds_per_seq, sequence_length, 1)`, due to the trick described in the previous paragraph."
      ],
      "metadata": {
        "id": "E7n7Hfq8I2Lj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_trajectories(\n",
        "    model: NNDynModel,\n",
        "    initial_state: torch.Tensor,\n",
        "    action_seqs: torch.Tensor,\n",
        "    preds_per_seq: int,\n",
        "    reward_fn: Callable[[torch.Tensor, torch.Tensor], torch.Tensor],\n",
        "    discount: float\n",
        "  ):\n",
        "  \"\"\"Returns predicted sum of rewards and final state after applying action_seqs at initial state.\n",
        "  Averages over preds_per_seq predictions per action sequence, and returns preds_per_seq predictions\n",
        "  of the final state (not averaged).\n",
        "\n",
        "  Args:\n",
        "    model: The model to use for trajectory prediction.\n",
        "    initial_state: The initial state from which the input action sequences will be applied.\n",
        "    action_seqs: The action sequences to be evaluated.\n",
        "    preds_per_seq: The number of particles propagated per action sequence (alternatively,\n",
        "      the number of samples used to compute an MC estimate of the trajectory return per sequence).\n",
        "    reward_fn: Reward function used for evaluation. Must take tensors of [batch_size, state_dim]\n",
        "      and [batch_size, action_dim] (representing the batch states and actions respectively) and\n",
        "      return a tensor of shape [batch_size] (representing the computed batch rewards).\n",
        "    discount: Discount to use.\n",
        "\n",
        "  Returns:\n",
        "    Estimated discounted returns for each action sequence under the provided model.\n",
        "  \"\"\"\n",
        "  n_seqs = action_seqs.shape[0]\n",
        "  seq_len = action_seqs.shape[1]\n",
        "\n",
        "  action_seqs = action_seqs[:, None].tile((1, preds_per_seq, 1, 1)).reshape(n_seqs * preds_per_seq, seq_len, -1)   # Tiling magic\n",
        "  cur_states = initial_state[None].tile((n_seqs * preds_per_seq, 1))\n",
        "  reward_sums = torch.zeros(n_seqs * preds_per_seq)\n",
        "\n",
        "  # At this point, we have that\n",
        "  # cur_states.shape = [n_seqs * preds_per_seq, state_dim]\n",
        "  # action_seqs.shape = [n_seqs * preds_per_seq, seq_len, action_dim]\n",
        "  # reward_sums.shape = [n_seqs * preds_per_seq]\n",
        "  #\n",
        "  # Note that the batch dimension is multiplied by preds_per_seq; the trick\n",
        "  #   we are using here is duplicating everything preds_per_seq times (to create\n",
        "  #   the particles for each action sequence).\n",
        "  # At this point, just pretend that you are sampling a single trajectory prediction\n",
        "  #   for each of the n_seqs * preds_per_seq sequences when filling in the\n",
        "  #   code block below.\n",
        "\n",
        "  with torch.no_grad():    # We are not backpropagating through time, speed up by not maintaining gradients\n",
        "    for t in range(action_seqs.shape[1]):\n",
        "      #################################\n",
        "      # Update relevant loop variables\n",
        "      reward_sums += (discount ** t) * reward_fn(cur_states, action_seqs[:, t])\n",
        "      cur_states = model(cur_states, action_seqs[:, t]).sample()\n",
        "      #################################\n",
        "\n",
        "  reward_sums = reward_sums.reshape((n_seqs, preds_per_seq)).mean(dim=-1)\n",
        "\n",
        "  return reward_sums"
      ],
      "metadata": {
        "id": "SBt3USqCvw45"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1(d) Model Predictive Control\n",
        "\n",
        "For this problem, **fill in the code block to implement the MPC action selection rule described in lecture**.\n",
        "Recall that MPC samples a set of action sequences, evaluates each one, then returns only the first action of the best action sequence.\n",
        "For the action sequence proposal distribution, use a uniform distribution over the box of valid actions for the environment for every action in the sequence. We have defined the variables `center` and `half_width` for your convenience when performing sampling.\n",
        "\n",
        "Note: **Do not hard code environment-related constants**; for example, if you need the dimension of the action space, you can use relevant properties of `env.action_space` (see the Gym documentation for more information about state/action spaces)."
      ],
      "metadata": {
        "id": "sHFYK05TO7lB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_mpc_policy(env, model, reward_fn, discount, plan_length, n_samples, n_particles):\n",
        "  \"\"\"Creates a policy implementing MPC.\n",
        "\n",
        "  Args:\n",
        "    env: The environment the created policy will be used for.\n",
        "    reward_fn: The reward function used by MPC for evaluating action sequences.\n",
        "    discount: Discount used by MPC for evaluating action sequences.\n",
        "    plan_length: The length of action sequences considered by MPC.\n",
        "    n_samples: The number of candidate action sequences sampled by MPC\n",
        "    n_particles: The number of particles used by MPC to evaluate every action sequence.\n",
        "\n",
        "  Returns:\n",
        "    A policy implementing MPC, represented as a function taking in a state and\n",
        "    returning an action.\n",
        "  \"\"\"\n",
        "  # The action space is a box; center gives the center of the box, while\n",
        "  #   half_width gives the distance from the center to either end for each dimension.\n",
        "  # Ex. for an action space [-n, n]^d, center = zero vector in d dimensions,\n",
        "  #   half_width = vector of all n's in d dimensions.\n",
        "  center = torch.Tensor((env.action_space.high + env.action_space.low) / 2)\n",
        "  half_width = torch.Tensor((env.action_space.high - env.action_space.low) / 2)\n",
        "\n",
        "  def mpc_policy(state: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"Returns the action taken by MPC on the given state.\n",
        "\n",
        "    Args:\n",
        "      state: Input state to policy of shape [state_dim].\n",
        "\n",
        "    Returns:\n",
        "      Action computed by MPC.\n",
        "    \"\"\"\n",
        "    state = torch.Tensor(state)\n",
        "    #################################\n",
        "    # Implement MPC action selection rule\n",
        "    # Hint: Remember to sample actions within each sequence uniformly within the action space\n",
        "    #   (use the center and half_width variables we define above!)\n",
        "    prescaled_seqs = 2 * (torch.rand((n_samples, plan_length, env.action_space.shape[0])) - 0.5)  # Uniform on [-1, 1]^action_dim\n",
        "    action_seqs = half_width * prescaled_seqs + center\n",
        "    reward_sums = predict_trajectories(model, state, action_seqs, n_particles, reward_fn, discount)\n",
        "    return action_seqs[torch.argmax(reward_sums)][0].numpy()\n",
        "    #################################\n",
        "\n",
        "  return mpc_policy"
      ],
      "metadata": {
        "id": "uahBWLDtv37E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1(e) PETS Training Loop\n",
        "\n",
        "Finally, we will put all of the components we have implemented so far together.\n",
        "**Fill in the indicated code blocks in `train` to complete the PETS training loop for the `Pendulum-v1` environment in Gym**. Here is a brief description of each code block:\n",
        "\n",
        "Code Block 1: Initialize the necessary objects for training (model, policy, model training buffer, and optimizer). We recommend AdamW, with weight decay set to 0.1.  \n",
        "Code Block 2: Implement a uniform random policy for selecting actions during exploration episodes (Hint: see `env.action_space` for potentially useful methods).  \n",
        "Code Block 3: Obtain an action from an MPC-based policy.  \n",
        "Code Block 4: Add transition data to the model training buffer.  \n",
        "Code Block 5: Continue training the model.\n",
        "\n",
        "We have provided a `MBRLBuffer` class for you; the `get_training_dataset()` method returns an object that is compatible with the `train_model()` method you had implemented earlier.\n",
        "Furthermore, `pendulum_reward()` computes the same reward function as the `Pendulum-v1` environment itself, and can be used during MPC evaluation.\n",
        "\n",
        "**Note on expected returns**: Due to the randomness of the initial state, the return for this environment is highly variable.\n",
        "For debugging reasons, we have included a print statement which prints the average absolute deviation of the pendulum angle from zero for the last 100 timesteps of each rollout, in addition to the return.\n",
        "Since the objective is to balance the pendulum rod ($\\theta = 0$), your implementation can be said to have learned `Pendulum-v1` if the printed average $\\theta$ deviations are very close to zero.\n",
        "\n",
        "**Note on runtime**: Our implementation on Google Colab takes around five minutes to run five episodes of MPC, which is sufficient for learning the task (if not less).\n",
        "\n",
        "**Note on constants**: All necessary constants (e.g. number of MPC particles, number of candidate action sequences, etc.) are provided for you already as optional arguments with default values.\n"
      ],
      "metadata": {
        "id": "WG9jo6OM82Xx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MBRLBuffer:\n",
        "  def __init__(self, observation_dim, action_dim, buffer_size):\n",
        "    self.buffer_size = buffer_size\n",
        "\n",
        "    self.ptr = 0\n",
        "    self.length = 0\n",
        "    self.ob_buffer = np.zeros((buffer_size, observation_dim))\n",
        "    self.action_buffer = np.zeros((buffer_size, action_dim))\n",
        "    self.next_ob_buffer = np.zeros((buffer_size, observation_dim))\n",
        "\n",
        "  def add_datapoint(self, observation, action, next_observation):\n",
        "    self.ob_buffer[self.ptr] = observation\n",
        "    self.action_buffer[self.ptr] = action\n",
        "    self.next_ob_buffer[self.ptr] = next_observation\n",
        "    self.ptr = (self.ptr + 1) % self.buffer_size\n",
        "    self.length = min(self.length + 1, self.buffer_size)\n",
        "\n",
        "  def get_training_dataset(self):\n",
        "    return {\n",
        "        \"obs\": self.ob_buffer[:self.length],\n",
        "        \"actions\": self.action_buffer[:self.length],\n",
        "        \"next_obs\": self.next_ob_buffer[:self.length]\n",
        "    }"
      ],
      "metadata": {
        "id": "3cjGXKchV0zM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pendulum_reward(states: torch.Tensor, actions: torch.Tensor) -> torch.Tensor:\n",
        "  return -(torch.square(torch.atan2(states[:, 1], states[:, 0])) + 0.1 * torch.square(states[:, 2]) + 0.001 * torch.square(actions[:, 0]))\n",
        "\n",
        "def train_pendulum(\n",
        "    n_episodes=10,\n",
        "    n_exploration_episodes=1,\n",
        "    discount=0.99,\n",
        "    mpc_plan_length=15,\n",
        "    n_mpc_candidate_sequences=1000,\n",
        "    n_particles=10,\n",
        "    buffer_length=10000,\n",
        "    model_batch_size=32,\n",
        "    n_model_steps=512\n",
        "  ) -> None:\n",
        "  \"\"\"Trains PETS on the Pendulum-v1 environment.\n",
        "\n",
        "  Args:\n",
        "    n_episodes: Number of episodes collected using MPC\n",
        "    n_exploration_episodes: Number of initial episodes collected using\n",
        "      uniform random actions.\n",
        "    discount: Discount used by MPC and for evaluation.\n",
        "    mpc_plan_length: Length of action sequences considered by MPC.\n",
        "    n_mpc_candidate_sequences: Number of candidate sequences considered by MPC\n",
        "      during the selection process.\n",
        "    n_particles: Number of particles used by MPC per action sequence during evaluation.\n",
        "    buffer_length: Length of model training buffer.\n",
        "    model_batch_size: Batch size during model training.\n",
        "    n_model_steps: Number of optimization steps taken every time the model is\n",
        "      trained after an episode.\n",
        "\n",
        "  Returns:\n",
        "    None\n",
        "  \"\"\"\n",
        "  env = gym.make(\"Pendulum-v1\")\n",
        "  state_dim = env.observation_space.shape[0]\n",
        "  action_dim = env.action_space.shape[0]\n",
        "\n",
        "  #################################\n",
        "  # Create model, policy, training buffer, and optimizer\n",
        "  model = NNDynModel(state_dim, action_dim)\n",
        "  policy = create_mpc_policy(\n",
        "    env, model, pendulum_reward, discount, mpc_plan_length, n_mpc_candidate_sequences, n_particles\n",
        "  )\n",
        "  buffer = MBRLBuffer(state_dim, action_dim, buffer_length)\n",
        "  optimizer = torch.optim.AdamW(model.parameters(), weight_decay=0.1)\n",
        "  #################################\n",
        "\n",
        "\n",
        "  for episode in range(n_exploration_episodes + n_episodes):\n",
        "    reward_sum = 0\n",
        "    deviations = []\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "    timestep = 0\n",
        "\n",
        "    while not done:\n",
        "      if episode < n_exploration_episodes:\n",
        "        #################################\n",
        "        # Select random action for initial exploration episodes\n",
        "        action = env.action_space.sample()\n",
        "        #################################\n",
        "      else:\n",
        "        #################################\n",
        "        # Use MPC to get an action.\n",
        "        action = policy(state)\n",
        "        #################################\n",
        "\n",
        "      next_state, reward, done, _ = env.step(action)\n",
        "      reward_sum += (discount ** timestep) * reward\n",
        "      deviations.append(np.arctan2(next_state[1], next_state[0]))\n",
        "\n",
        "      #################################\n",
        "      # Add transition data to replay buffer\n",
        "      buffer.add_datapoint(state, action, next_state)\n",
        "      #################################\n",
        "\n",
        "      timestep += 1\n",
        "      state = next_state\n",
        "\n",
        "    if episode >= n_exploration_episodes:\n",
        "      print(\"Episode {} return: {}; average deviation over final 100 steps: {}\".format(\n",
        "          episode - n_exploration_episodes, reward_sum, np.mean(deviations[-100:])\n",
        "      ))\n",
        "\n",
        "    #################################\n",
        "    # Continue training the model\n",
        "    train_model(model, buffer.get_training_dataset(), model_batch_size, n_model_steps, optimizer)\n",
        "    #################################"
      ],
      "metadata": {
        "id": "HLWz1v6H8yTW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_pendulum()"
      ],
      "metadata": {
        "id": "Ydoy-E99XPeW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "921f8e2f-21a0-404f-be23-c7cc88d4eaa4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:241: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
            "  if not isinstance(terminated, (bool, np.bool8)):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 0 return: -302.5487001707376; average deviation over final 100 steps: 0.09798536449670792\n",
            "Episode 1 return: -2.001051440750063; average deviation over final 100 steps: 0.010249728336930275\n",
            "Episode 2 return: -195.77209295631263; average deviation over final 100 steps: -0.0002570024225860834\n",
            "Episode 3 return: -199.207247786024; average deviation over final 100 steps: 0.003300018608570099\n",
            "Episode 4 return: -103.58109111925118; average deviation over final 100 steps: 0.001337804482318461\n",
            "Episode 5 return: -281.3060626422867; average deviation over final 100 steps: 0.00436615664511919\n",
            "Episode 6 return: -110.0614368586988; average deviation over final 100 steps: -0.007343442644923925\n",
            "Episode 7 return: -106.28333708455487; average deviation over final 100 steps: 0.002810687990859151\n",
            "Episode 8 return: -208.71882428577678; average deviation over final 100 steps: 0.009274815209209919\n",
            "Episode 9 return: -201.82953923556136; average deviation over final 100 steps: 0.01171807385981083\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Problem 2: Short Answers\n",
        "\n",
        "Please keep your responses brief; **most of the justifications for these questions can be answered in one or two sentences**."
      ],
      "metadata": {
        "id": "fEnqddIlvXmd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2(a) MPC Horizon\n",
        "\n",
        "One of the hyperparameters that needs to be chosen for MPC is the length of the action sequences that are evaluated at every step (i.e. the planning horizon). We have listed four factors that should be considered when choosing this hyperparameter. Explain **how each consideration affects the choice of MPC horizon and why**."
      ],
      "metadata": {
        "id": "ZAWu01cmvc8N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### (i) Action sequence optimization difficulty\n",
        "\n",
        "> Sampling a good action sequence gets harder with longer sequences due to the curse of dimensionality, and so this would encourage us to choose shorter sequences.\n",
        "\n"
      ],
      "metadata": {
        "id": "kEPILh4IxqDZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### (ii) Achieved returns\n",
        "\n",
        "> Since we are ultimately trying to solve the RL problem, we want the evaluation metric to reflect the full return as well as possible, hence we would want the horizon to be longer.\n"
      ],
      "metadata": {
        "id": "6kEb0okFtpLt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### (iii) Model accuracy\n",
        "\n",
        "> The worse the model accuracy, the harder it is to get a reliable trajectory prediction for long action sequences."
      ],
      "metadata": {
        "id": "Q1GKA1dNtzrX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### (iv) Wall-clock-time, running on a non-simulated system\n",
        "\n",
        "> Trajectory prediction is computationally expensive with respect to the horizon because the runtime is linear in the horizon, hence real systems may require a shorter horizon for responsive control."
      ],
      "metadata": {
        "id": "zA4uiD8St7Ha"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2(b) Exploration Bonuses with Ensembles, Part I\n",
        "\n",
        "Consider an agent who is running MPC, but is evaluating each action sequence differently than the procedure discussed in lecture. More concretely, for a particular action sequence, let us say that the sampled returns computed from particle-based sampling is given by $(R_{i, j})$ for $i = 1, \\dots, M$ and $j = 1, \\dots, N$, such that $R_{i, 1}, R_{i, 2}, \\dots, R_{i, N}$ are the predictions from the $i^{\\text{th}}$ model of the ensemble.\n",
        "For convenience, let us define the within-model mean $\\mu_i$ and within-model standard deviation $\\sigma_i$ for $i = 1, \\dots, M$ as\n",
        "\n",
        "$$\\mu_i := \\frac{1}{N}\\sum_{j = 1}^{N}R_{i, j} \\quad \\text{and} \\quad \\sigma_i := \\sqrt{\\frac{1}{N - 1}\\sum_{j = 1}^{N}(R_{i, j} - \\mu_i)^2}$$\n",
        "\n",
        "The standard MPC evaluation procedure computes the quantity\n",
        "\n",
        "$$\\hat{R}_{\\mathrm{basic}} := \\frac{1}{M}\\sum_{i = 1}^{M}\\mu_i,$$\n",
        "\n",
        "which is equivalently the average over all particles.\n",
        "This agent instead computes the evaluation metric\n",
        "\n",
        "$$\\hat{R}_{\\mathrm{proposed}} := \\frac{1}{M}\\sum_{i = 1}^{M}(\\mu_i + \\sigma_i)$$\n",
        "\n",
        "to add some \"exploration bonus\" by encouraging the agent to go where there is more observed noise (analogous to the UCB bonus for bandits).\n",
        "**Does this bonus make sense as an exploration bonus? Why or why not?**\n",
        "\n",
        "Hint: Think about the fact that the standard deviations are *within-model* standard deviations. Where does the noise in within-model predictions come from?"
      ],
      "metadata": {
        "id": "ZWoG5-pazCrk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> This bonus does not make sense, since the variance within each model's predictions is due to irreducible environment noise, and so this objective would encourage the agent to go to noisy parts of the environment."
      ],
      "metadata": {
        "id": "VxRO5FOKuKqL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2(c) Exploration Bonuses with Ensembles, Part II\n",
        "\n",
        "We work in the same setting as 3(b), but now let us define the across-ensemble standard deviation $\\sigma_{\\mathrm{Ens}}$ as the standard deviation of the within-model means, or written mathematically,\n",
        "\n",
        "$$\\sigma_{\\mathrm{Ens}} := \\sqrt{\\frac{1}{M - 1}\\sum_{i = 1}^{M}(\\mu_i - \\hat{R}_\\mathrm{basic})^2}.$$\n",
        "\n",
        "The agent now uses the evaluation metric\n",
        "\n",
        "$$\\hat{R}_{\\mathrm{proposed}} := \\hat{R}_{basic} + \\sigma_{\\mathrm{Ens}}$$\n",
        "\n",
        "as another form of an \"exploration bonus\".\n",
        "Does this new bonus make sense as an exploration bonus? Why or why not?"
      ],
      "metadata": {
        "id": "AjtTOLfl4Hsl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> This makes sense as a bonus, since the variance is across the bootstrapped ensemble, which is an estimate of the epistemic uncertainty of the model that can be addressed with more data. Thus, the agent would be encouraged to take actions that will help it improve its return estimates."
      ],
      "metadata": {
        "id": "sC_X0u9Hj2cA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2(d) What to use?\n",
        "\n",
        "We will describe several scenarios for which we might want to use RL.\n",
        "In each case, **state whether a model-based or a model-free algorithm would be preferred and why**."
      ],
      "metadata": {
        "id": "3lgjkcikPkPQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Training an expensive but fragile robotic arm for reaching tasks.\n",
        "\n",
        "> Model-based. Using a model is key in real environments where available data is limited by the system.\n",
        "\n",
        "2. Training an agent to play a video game that can be run massively in parallel.\n",
        "\n",
        "> Model-free. If getting data is cheap and easy, then there is no reason to use a model, as the model can limit performance.\n",
        "\n",
        "3. Training an RC car to deliver food to college students anywhere on campus.\n",
        "\n",
        "> Model-based. One can think of each possible student location as a new task represented by a different reward function, but the dynamics remains the same so the same model can be used across all these tasks."
      ],
      "metadata": {
        "id": "Sbn5_X-WP4fg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2(e) Terminal Value Functions\n",
        "\n",
        "Observe that the standard MPC evaluation procedure only considers the sum of rewards within the action sequence, and does not consider anything after the sequence.\n",
        "One can instead add a terminal value function at the end that is learned alongside the model throughout the training procedure.\n",
        "**Name one pro and one con of learning and using this value function.**\n",
        "\n",
        "Hint: for the drawback, think about the number of samples needed to learn a value function."
      ],
      "metadata": {
        "id": "j6Xn66dmoqCx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "76lurpknZGgy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **Pro:** Selected action sequence will maximize the full return rather than just the sum of returns for the action sequence, removing the mismatch with the true RL objective.\n",
        "\n",
        "> **Con:** Learning a value function requires more samples, minimizing the benefit of using a model-based approach."
      ],
      "metadata": {
        "id": "l-Jk6tHgpVu4"
      }
    }
  ]
}
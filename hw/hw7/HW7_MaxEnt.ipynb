{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pOqaqSFFcuUg"
      },
      "source": [
        "Make sure to run every cell in the file in the correct order."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fUDWvikdkd2J"
      },
      "outputs": [],
      "source": [
        "!pip3 install swig > /dev/null 2>&1\n",
        "!pip3 uninstall box2d-py -y > /dev/null 2>&1\n",
        "!pip3 install box2d-py > /dev/null 2>&1\n",
        "!pip3 install box2d box2d-kengz > /dev/null 2>&1\n",
        "!apt install xvfb > /dev/null 2>&1\n",
        "!pip3 install pyvirtualdisplay > /dev/null 2>&1\n",
        "!pip3 install gym==0.25.0 > /dev/null 2>&1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZlGiM8WGkzsi"
      },
      "outputs": [],
      "source": [
        "import gym\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.distributions import Categorical\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "import sys\n",
        "from pyvirtualdisplay import Display\n",
        "from IPython import display as disp\n",
        "import copy\n",
        "from typing import Tuple\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i4slgsZK2HNr"
      },
      "source": [
        "### Implementing TD3 with MaxEnt\n",
        "\n",
        "We will give you the reference implementation for TD3 below. Your job is to change the actor into a Gaussian policy and implement MaxEnt such that it converges on MountainCar-v0."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "89mle70cfClJ"
      },
      "source": [
        "Like before, we provide you with the following utility function that implements the replay buffer for experience replay."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1bX7EbSNod3a"
      },
      "outputs": [],
      "source": [
        "# Replay buffer\n",
        "class ReplayBuffer(object):\n",
        "\tdef __init__(self, state_dim, action_dim, max_size=int(1e6)):\n",
        "\t\tself.max_size = max_size\n",
        "\t\tself.ptr = 0\n",
        "\t\tself.size = 0\n",
        "\n",
        "\t\tself.state = np.zeros((max_size, state_dim))\n",
        "\t\tself.action = np.zeros((max_size, action_dim))\n",
        "\t\tself.next_state = np.zeros((max_size, state_dim))\n",
        "\t\tself.reward = np.zeros((max_size, 1))\n",
        "\t\tself.not_done = np.zeros((max_size, 1))\n",
        "\n",
        "\t\tself.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "\tdef add(self, state, action, next_state, reward, done):\n",
        "\t\tself.state[self.ptr] = state\n",
        "\t\tself.action[self.ptr] = action\n",
        "\t\tself.next_state[self.ptr] = next_state\n",
        "\t\tself.reward[self.ptr] = reward\n",
        "\t\tself.not_done[self.ptr] = 1. - done\n",
        "\n",
        "\t\tself.ptr = (self.ptr + 1) % self.max_size\n",
        "\t\tself.size = min(self.size + 1, self.max_size)\n",
        "\n",
        "\n",
        "\tdef sample(self, batch_size):\n",
        "\t\tind = np.random.randint(0, self.size, size=batch_size)\n",
        "\n",
        "\t\treturn (\n",
        "\t\t\ttorch.FloatTensor(self.state[ind]).to(self.device),\n",
        "\t\t\ttorch.FloatTensor(self.action[ind]).to(self.device),\n",
        "\t\t\ttorch.FloatTensor(self.next_state[ind]).to(self.device),\n",
        "\t\t\ttorch.FloatTensor(self.reward[ind]).to(self.device),\n",
        "\t\t\ttorch.FloatTensor(self.not_done[ind]).to(self.device)\n",
        "\t\t)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9QL9inx6Aw93"
      },
      "source": [
        "Below is the main function for training/evaluating on ``MountainCarContinuous`` environment! The hyperparameters are provided in the following cell. Note that they apply for both DDPG and TD3. To make the training process more stable, we will first randomly sample from the action space to fullfill our replay buffer. So don't panick if you find the reward in the initial episodes is super low! **You are welcome to change anything in the training process as long as you find it helpful!**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JWg4ZB6XlrwR"
      },
      "outputs": [],
      "source": [
        "def init_flags():\n",
        "\n",
        "    flags = {\n",
        "        \"env\": \"MountainCarContinuous\",\n",
        "        \"seed\":0, # random seed\n",
        "        \"start_timesteps\": 25e3, #total steps of free exploration phase\n",
        "        \"max_timesteps\": 8e4, # maximum length of time steps in training\n",
        "        \"expl_noise\": 0.1, # noise strength in exploration\n",
        "        \"batch_size\": 512,\n",
        "        \"discount\":0.99,\n",
        "        \"tau\": 0.005, # rate of target update\n",
        "        \"policy_noise\": 0.2, # policy noise when sampling action\n",
        "        \"noise_clip\":0.5, # noise clip rate\n",
        "        \"policy_freq\": 2, # delayed policy update frequency in TD3\n",
        "    }\n",
        "\n",
        "    return flags\n",
        "\n",
        "def main(policy_name = 'DDPG') -> list:\n",
        "    \"\"\"\n",
        "    Input:\n",
        "    policy_name: str, the method to implement\n",
        "    Output:\n",
        "    evaluations: list, the reward in every episodes\n",
        "    Call DDPG/TD3 trainer and\n",
        "    \"\"\"\n",
        "    args = init_flags()\n",
        "    env = gym.make(args[\"env\"])\n",
        "    env.seed(args[\"seed\"]+100)\n",
        "    env.action_space.seed(args[\"seed\"])\n",
        "    torch.manual_seed(args[\"seed\"])\n",
        "    np.random.seed(args[\"seed\"])\n",
        "\n",
        "    state_dim = env.observation_space.shape[0]\n",
        "    action_dim = env.action_space.shape[0]\n",
        "    max_action = float(env.action_space.high[0])\n",
        "    kwargs = {\n",
        "        \"state_dim\": state_dim,\n",
        "        \"action_dim\": action_dim,\n",
        "        \"max_action\": max_action,\n",
        "        \"discount\": args[\"discount\"],\n",
        "        \"tau\": args[\"tau\"],}\n",
        "    if policy_name == \"TD3\":\n",
        "        # Target policy smoothing is scaled wrt the action scale\n",
        "        kwargs[\"policy_noise\"] = args[\"policy_noise\"] * max_action\n",
        "        kwargs[\"noise_clip\"] = args[\"noise_clip\"] * max_action\n",
        "        kwargs[\"policy_freq\"] = args[\"policy_freq\"]\n",
        "        policy = TD3(**kwargs)\n",
        "    elif policy_name == \"DDPG\":\n",
        "        policy = DDPG(**kwargs)\n",
        "\n",
        "    replay_buffer = ReplayBuffer(state_dim, action_dim)\n",
        "    evaluations = []\n",
        "    actions_l = []\n",
        "    entropies = []\n",
        "    means = []\n",
        "    varis = []\n",
        "    state, done = env.reset(), False\n",
        "    episode_reward = 0\n",
        "    episode_timesteps = 0\n",
        "    episode_num = 0\n",
        "\n",
        "    for t in range(int(args[\"max_timesteps\"])):\n",
        "\n",
        "      episode_timesteps += 1\n",
        "\n",
        "      # Select action randomly or according to policy\n",
        "      entropy = 0\n",
        "      mean = 0\n",
        "      vari = 0\n",
        "      if t < args[\"start_timesteps\"]:\n",
        "        action = env.action_space.sample()\n",
        "      else:\n",
        "        action, entropy, mean, vari = (\n",
        "          policy.select_action(np.array(state))\n",
        "          # + np.random.normal(0, max_action * args[\"expl_noise\"], size=action_dim)\n",
        "        )# .clip(-max_action, max_action)\n",
        "\n",
        "      # Perform action\n",
        "      next_state, reward, done, _ = env.step(action)\n",
        "      done_bool = float(done) if episode_timesteps < env._max_episode_steps else 0\n",
        "\n",
        "      # Store data in replay buffer\n",
        "      replay_buffer.add(state, action, next_state, reward, done_bool)\n",
        "\n",
        "      state = next_state\n",
        "      actions_l.append(action)\n",
        "      entropies.append(entropy)\n",
        "      means.append(mean)\n",
        "      varis.append(vari)\n",
        "      episode_reward += reward\n",
        "\n",
        "      # Train agent after collecting sufficient data\n",
        "      if t >= args[\"start_timesteps\"]:\n",
        "        policy.train(replay_buffer, args[\"batch_size\"])\n",
        "\n",
        "      if done:\n",
        "        # +1 to account for 0 indexing. +0 on ep_timesteps since it will increment +1 even if done=True\n",
        "        print(f\"Total T: {t+1} Episode Num: {episode_num+1} Episode T: {episode_timesteps} Reward: {episode_reward:.3f}\")\n",
        "        evaluations.append(episode_reward)\n",
        "        # print('a', actions_l)\n",
        "        # print('e', entropies)\n",
        "        # print('mu', means)\n",
        "        # print('var', varis)\n",
        "        entropies = []\n",
        "        actions_l = []\n",
        "        means = []\n",
        "        varis = []\n",
        "        # Reset environment\n",
        "        state, done = env.reset(), False\n",
        "        episode_reward = 0\n",
        "        episode_timesteps = 0\n",
        "        episode_num += 1\n",
        "\n",
        "    return evaluations\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PrWigU12Wd88"
      },
      "source": [
        "### Question 1: Implementation of TD3\n",
        "\n",
        "Follow the steps outlined in the Overleaf. Your goal is to change the TD3 actor/policy to be a Normal distribution where you sample the action.\n",
        "\n",
        "#### Description of TD3 from HW5:\n",
        "Similar to DDPG, TD3 also maintains a continuous actor update with deterministic policy gradient, a replay buffer with an exploratory strategy. The different between TD3 and DDPG lies in the critic update. Unlike Double DQN, due to the slow update of the policy (the $\\theta$ in previous examples), the current and target value still remains similar even when using a double Q-update technique. While the implementation of an independent target network allows for a less biased value estimation, even an unbiased estimate with high variance can still lead to future overestimations in local regions of state space, which in turn can negatively affect the global policy.\n",
        "\n",
        "To address this issue, TD3 updates the actor/critic network in the following way:\n",
        "\n",
        "**(1)** TD3 maintains **6 neural networks**: $Q_{\\phi_1}, Q_{\\phi_2}$ as two independent update for the Q function, $Q_{\\phi_1'}, Q_{\\phi_2'}$ as their corresponding target networks, and $\\mu_\\theta, \\mu_{\\theta'}$ as the actor and its target network. $Q_{\\phi_1}$ and $Q_{\\phi_2}$ are independently initialized, and all target networks are initialized to be the same as their corresponding counterparts. When updating Q-functions, in every epidsode, DDPG updates the critic $Q_\\phi$ by performing gradient descent with the following improved Bellamn loss: $$\n",
        "L(\\phi_i) = \\text{mean}_t f(y_t, Q_{\\phi_i}(s_t,a_t)),\n",
        "$$\n",
        "where $y_t = r(s_t, a_t) + \\gamma \\cdot \\min_{i= 1,2}Q_{\\phi_i'}(s_{t+1}, \\tilde{a}_{t+1})$, $\\tilde{a}_t =  \\mu_{\\theta'}(s_{t+1}) + \\text{clip}_{[-c,c]}(\\mathcal{N}(0,\\tilde{\\sigma}))$,  which is different from the TD target in DDPG, as it take the minimum of the two future Q-values and a \"disturbed\" future action. Here we clip $\\tilde{a}_t$ to prevent it from going too far. Intuitively, TD3 evaluates the Q-value of $s_{t+1}$ in a more \"conservative\" way.\n",
        "\n",
        "\n",
        "**(2)** When updating $\\mu_\\theta$, TD3 only utilizes $Q_{\\phi_1}$:$$\n",
        "\\Delta \\theta = \\eta\\cdot \\text{mean}_t \\big(\\nabla_a Q_{\\phi_1}(s_t, \\mu_\\theta(s_t)) \\cdot \\nabla_\\theta \\mu_\\theta (s_t)\\big).\n",
        "$$\n",
        "At the end of every episode, similar to DDPG, all parameters are updated by $$\n",
        "\\phi_i' = \\tau \\phi_i + (1 - \\tau) \\phi_i', \\qquad \\theta' = \\tau \\theta + (1 - \\tau) \\theta'.\n",
        "$$\n",
        "\n",
        "**(3)** The actor update is delayed: it only updates once every several times the critic updates.\n",
        "\n",
        "\n",
        "We understand the introduction above might be a little bit confusing due to the technical complexity. In the following, we will guide you end to end to implement a TD3 training algorithm.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tpCc0fUfwldo"
      },
      "source": [
        "In the following cell, we will implement the actor and critic network for TD3. For the actor and every critic (we need to maintain an additional critic), please make sure it has the same structure as the one in the previous DDPG question so that we can conduct an ablation study.\n",
        "\n",
        "Our implementation for the ``Critic_TD3`` class is slightly different from the previous critic in DDPG: the class function ``forward`` should return two values $q_1$ and $q_2$ given $(s,a)$, while the class function ``Q1`` should return only $q_1$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wJfEi1DHlbdD"
      },
      "outputs": [],
      "source": [
        "# Reference Implementation of Twin Delayed Deep Deterministic Policy Gradients (TD3)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Construct the actor/critic network for TD3\n",
        "class Actor_TD3(nn.Module):\n",
        "\tdef __init__(self, state_dim: int, action_dim: int, max_action: float):\n",
        "\t\tsuper(Actor_TD3, self).__init__()\n",
        "\t\t############################\n",
        "\t\t# YOUR IMPLEMENTATION HERE #\n",
        "\t\tself.l1 = nn.Linear(state_dim, 256)\n",
        "\t\tself.l2 = nn.Linear(256, 256)\n",
        "\t\tself.l3 = nn.Linear(256, action_dim)\n",
        "\t\t############################\n",
        "\t\tself.max_action = max_action\n",
        "\n",
        "\n",
        "\tdef forward(self, state: torch.Tensor) -> torch.Tensor:\n",
        "\t\t############################\n",
        "\t\t# YOUR IMPLEMENTATION HERE #\n",
        "\t\ta = F.relu(self.l1(state))\n",
        "\t\ta = F.relu(self.l2(a))\n",
        "\n",
        "\t\t# Hint: Use torch.distributions.Normal\n",
        "\n",
        "\t\treturn self.max_action * torch.tanh(self.l3(a))\n",
        "    ############################\n",
        "\n",
        "\n",
        "class Critic_TD3(nn.Module):\n",
        "\tdef __init__(self, state_dim : int, action_dim: int):\n",
        "\t\tsuper(Critic_TD3, self).__init__()\n",
        "\n",
        "\t\t# Q1 architecture\n",
        "\t\t############################\n",
        "\t\t# YOUR IMPLEMENTATION HERE #\n",
        "\t\tself.l1 = nn.Linear(state_dim + action_dim, 256)\n",
        "\t\tself.l2 = nn.Linear(256, 256)\n",
        "\t\tself.l3 = nn.Linear(256, 1)\n",
        "\n",
        "\t\t# Please implement Q2 below\n",
        "\t\t############################\n",
        "\t\t# YOUR IMPLEMENTATION HERE #\n",
        "\t\tself.l4 = nn.Linear(state_dim + action_dim, 256)\n",
        "\t\tself.l5 = nn.Linear(256, 256)\n",
        "\t\tself.l6 = nn.Linear(256, 1)\n",
        "\t\t############################\n",
        "\n",
        "\tdef forward(self, state: torch.Tensor, action: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "\t\tsa = torch.cat([state, action], 1)\n",
        "\t\t############################\n",
        "\t\t# YOUR IMPLEMENTATION HERE #\n",
        "\t\tq1 = F.relu(self.l1(sa))\n",
        "\t\tq1 = F.relu(self.l2(q1))\n",
        "\t\tq1 = self.l3(q1)\n",
        "\n",
        "\t\tq2 = F.relu(self.l4(sa))\n",
        "\t\tq2 = F.relu(self.l5(q2))\n",
        "\t\tq2 = self.l6(q2)\n",
        "\t\t############################\n",
        "\t\treturn q1, q2\n",
        "\n",
        "\n",
        "\tdef Q1(self, state: torch.Tensor, action: torch.Tensor) -> torch.Tensor:\n",
        "\t\t# [HINT] only returns q1 for actor update\n",
        "\t\t############################\n",
        "\t\t# YOUR IMPLEMENTATION HERE #\n",
        "\t\tsa = torch.cat([state, action], 1)\n",
        "\n",
        "\t\tq1 = F.relu(self.l1(sa))\n",
        "\t\tq1 = F.relu(self.l2(q1))\n",
        "\t\tq1 = self.l3(q1)\n",
        "\t  ############################\n",
        "\t\treturn q1\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1yAS1Ue_07G3"
      },
      "source": [
        "Now let's implement the TD3 trainer! In the following cell, you will need to implement the following:\n",
        "\n",
        "**(1)** For the critic update of TD3, when sampled a tuple from the replay buffer, recall that we need to estimate $Q(s_{t+1},\\tilde{a}_{t+1})$, where $\\tilde{a}_{t+1} = \\mu_{\\theta'}(s_{t+1})+\\text{clip}_{[-c,c]}(\\epsilon)$ .\n",
        "\n",
        "**(2)** Calculate the TD target with the networks $Q_{\\phi_i'}, i=1,2,$ and $(s_{t+1}, \\tilde{a}_{t+1})$ you obtained in **(1)**. Recall that the TD target = $r(s_t,a_t) + \\min_{i=1,2}\\{Q_{\\phi_i'}(s_{t+1}, \\tilde{a}_{t+1})\\}$.\n",
        "\n",
        "**(3)** Calculate the actor loss with $Q_{\\phi_1}$.\n",
        "\n",
        "**(4)** Update the parameters $\\phi_i'$, $\\theta'$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Lh09MrF022-"
      },
      "outputs": [],
      "source": [
        "class TD3(object):\n",
        "\tdef __init__(\n",
        "\t\tself,\n",
        "\t\tstate_dim: int,\n",
        "\t\taction_dim: int,\n",
        "\t\tmax_action: float,\n",
        "\t\tdiscount=0.99,\n",
        "\t\ttau=0.005,\n",
        "\t\tpolicy_noise=0.2,\n",
        "\t\tnoise_clip=0.5,\n",
        "\t\tpolicy_freq=2\n",
        "\t):\n",
        "\n",
        "\t\tself.actor = Actor_TD3(state_dim, action_dim, max_action).to(device)\n",
        "\t\tself.actor_target = copy.deepcopy(self.actor)\n",
        "\t\tself.actor_optimizer = torch.optim.Adam(self.actor.parameters(), lr=3e-4)\n",
        "\n",
        "\t\tself.critic = Critic_TD3(state_dim, action_dim).to(device)\n",
        "\t\tself.critic_target = copy.deepcopy(self.critic)\n",
        "\t\tself.critic_optimizer = torch.optim.Adam(self.critic.parameters(), lr=3e-4)\n",
        "\n",
        "\t\tself.max_action = max_action\n",
        "\t\tself.discount = discount\n",
        "\t\tself.tau = tau\n",
        "\t\tself.policy_noise = policy_noise\n",
        "\t\tself.noise_clip = noise_clip\n",
        "\t\tself.policy_freq = policy_freq\n",
        "\n",
        "\t\tself.total_it = 0\n",
        "\n",
        "\n",
        "\tdef select_action(self, state: torch.Tensor) -> torch.Tensor:\n",
        "\t\tstate = torch.FloatTensor(state.reshape(1, -1)).to(device)\n",
        "\t\treturn self.actor(state).cpu().data.numpy().flatten()\n",
        "\n",
        "\n",
        "\tdef train(self, replay_buffer, batch_size=256):\n",
        "\t\tself.total_it += 1\n",
        "\n",
        "\t\t# Sample replay buffer\n",
        "\t\tstate, action, next_state, reward, not_done = replay_buffer.sample(batch_size)\n",
        "\n",
        "\t\twith torch.no_grad():\n",
        "\t\t\t# Select action according to the policy,\n",
        "      ############################\n",
        "      # YOUR IMPLEMENTATION HERE #\n",
        "\t\t\tnext_action = (\n",
        "\t\t\t\tself.actor_target(next_state)\n",
        "\t\t\t).clamp(-self.max_action, self.max_action)\n",
        "      ############################\n",
        "\t\t\t# Compute the target Q value\n",
        "\t\t\ttarget_Q1, target_Q2 = self.critic_target(next_state, next_action)\n",
        "\n",
        "      ############################\n",
        "      # YOUR IMPLEMENTATION HERE #\n",
        "\t\t\t# 1.Calculate the min of two target Q-functions\n",
        "\t\t\t# 2. Calculate the TD target\n",
        "\t\t\ttarget_Q = torch.min(target_Q1, target_Q2)\n",
        "\t\t\ttarget_Q = reward + not_done * self.discount * target_Q\n",
        "      ############################\n",
        "\t\t# Get current Q estimates\n",
        "\t\tcurrent_Q1, current_Q2 = self.critic(state, action)\n",
        "\n",
        "\t\t# Compute critic loss\n",
        "\t\tcritic_loss = F.mse_loss(current_Q1, target_Q) + F.mse_loss(current_Q2, target_Q)\n",
        "\n",
        "\t\t# Optimize the critic\n",
        "\t\tself.critic_optimizer.zero_grad()\n",
        "\t\tcritic_loss.backward()\n",
        "\t\tself.critic_optimizer.step()\n",
        "\n",
        "\t\t# Delayed policy updates\n",
        "\t\tif self.total_it % self.policy_freq == 0:\n",
        "\n",
        "\t\t\t# Compute actor loss\n",
        "      ############################\n",
        "      # YOUR IMPLEMENTATION HERE #\n",
        "\t\t\tactor_loss = -self.critic.Q1(state, self.actor(state)).mean()\n",
        "      ############################\n",
        "\n",
        "\t\t\t# Optimize the actor\n",
        "\t\t\tself.actor_optimizer.zero_grad()\n",
        "\t\t\tactor_loss.backward()\n",
        "\t\t\tself.actor_optimizer.step()\n",
        "\n",
        "\t\t\t# Update the frozen target models\n",
        "      ############################\n",
        "      # YOUR IMPLEMENTATION HERE #\n",
        "\t\t\tfor param, target_param in zip(self.critic.parameters(), self.critic_target.parameters()):\n",
        "\t\t\t\tnew_target_params = self.tau * param.data + (1 - self.tau) * target_param.data\n",
        "\t\t\t\ttarget_param.data.copy_(new_target_params)\n",
        "\n",
        "\t\t\tfor param, target_param in zip(self.actor.parameters(), self.actor_target.parameters()):\n",
        "\t\t\t\tnew_target_params = self.tau * param.data + (1 - self.tau) * target_param.data\n",
        "\t\t\t\ttarget_param.data.copy_(new_target_params)\n",
        "      ############################\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "d_BG0sGGo3oY"
      },
      "outputs": [],
      "source": [
        "evaluation_td3 = main(policy_name = 'TD3')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ZT4AaR_-cx4"
      },
      "outputs": [],
      "source": [
        "plt.plot(evaluation_td3, label = 'td3')\n",
        "plt.legend()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
\documentclass[12pt]{article}
\usepackage[colorinlistoftodos]{todonotes}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{bm}
\usepackage{enumerate}
\usepackage{fancyvrb}
\usepackage[top=1in, bottom=1in, left=1in, right=1in]{geometry}
\usepackage{hyperref}
\usepackage{placeins}
\usepackage{tikz}
\usepackage{tikzsymbols}
\usepackage{todonotes}
\usepackage{bbm}
\usepackage{color}
\usepackage{mathrsfs}
\usepackage{enumitem}
\usepackage{soul} % for HL
\usepackage{color} % for HL
\usepackage{minted}

\usepackage{listings}
\usepackage{xcolor}
\usepackage[
backend=biber,
style=alphabetic,
sorting=ynt
]{biblatex}
\addbibresource{papers.bib}

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}

\lstset{style=mystyle}

% \renewcommand{\theenumi}{\roman{enumi}}
\newcommand{\rmn}[1]{{\textcolor{blue}{\bf [{\sc rmn:} #1]}}}
\DeclareMathOperator*{\argmax}{arg\,max}

\usetikzlibrary{positioning,calc}
%%%%%%%%%
\usepackage[most]{tcolorbox}
\newtcolorbox[]{solution}[1][]{%
    breakable,
    enhanced,
    colback=white,
    title=Solution,
    #1
}

\newtcolorbox[]{fillme}[1][]{%
    breakable,
    enhanced,
    colback=white,
    title=Fill me in,
    #1
}

\newcommand*{\E}{\mathbb{E}}
\newcommand*{\prob}{\mathbb{P}}
\newcommand*{\F}{\mathcal{F}}
\newcommand*{\bo}{\mathbb{B}}
\newtheorem{theorem}{Theorem}
\newtheorem{observation}{Observation}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}

%%%%%%%%%%

\date{Due April 9, 2024}
\author{\begin{fillme}[width=0.3\textwidth]
 Your name here.
\end{fillme}} % Fill in your name!

\title{ECE433/COS435 Introduction to RL\\
  Assignment 7: MaxEnt\\
  Spring 2024\\
}

\begin{document}
  \maketitle
  \section*{Collaborators}
\begin{fillme}
 Please fill in the names and NetIDs of your collaborators in this section.
\end{fillme}

\section*{Instructions}

Writeups should be typesetted in Latex and submitted as PDFs. You can work with whatever tool you like for the code, but \textbf{please submit the asked-for snippet and answer in the solutions box as part of your writeup. We will only be grading your write-up.} Make sure still also to attach your notebook/code with your submission.\\


\noindent We recommend you collaborate in pairs if you find the workload of this assignment overwhelming.

\section*{Question 1. Implementing TD3 with MaxEnt}
Using the solution for TD3 in Homework 5 (we'll give it to you), we are going to re-write it using the MaxEnt framework. Your policy will now be a \textbf{Gaussian} distribution that gets sampled instead of a deterministic continuous variable. See \url{https://pytorch.org/docs/stable/distributions.html} for more information about PyTorch distributions. We will also provide a set of hints below that may be useful for you.
\begin{enumerate}
    \item Be careful to distinguish between \href{https://pytorch.org/docs/stable/distributions.html#torch.distributions.distribution.Distribution.rsample}{torch.distributions.distribution.Distribution.rsample()} and \href{https://pytorch.org/docs/stable/distributions.html#torch.distributions.distribution.Distribution.sample}{torch.distributions.distribution.Distribution.sample()}.
    \item The careful reader may notice that the Gaussian distribution is unbounded, but your environment (MountainCar) has a bounded action space. Be sure to clip your outputs using an activation function or \texttt{torch.clip}.
    \item You \textbf{do not} need to factor in this "clipping" transformation in your entropy computation. Technically, you need to perform a change of variable to compute the correct entropy, but your solution should still work by only using the entropy of the Gaussian as the entropy in your expressions.
    \item Generally, you are supposed to leave the mean and variance $\mu, \sigma^2$ unbounded. However, you may notice that our action space is bounded here, so you might find it easier to just bound these terms with an activation function to avoid exploding terms.
    \item You may want to scale your entropy with a constant $\alpha$ as a tunable hyperparameter.
\end{enumerate}

\subsection*{Question 1.a} 
Rewrite the \texttt{Actor\_TD3} class such that the Actor now outputs a \textbf{Gaussian distribution} in its forward pass. You may find it easier to handle sampling actions and computing the entropy/log probabilities \textbf{outside of the forward()} function.
Paste your class below:
\begin{solution}
\begin{lstlisting}[language=Python]
class Actor_TD3:
    pass
\end{lstlisting}
\end{solution}

\subsection*{Question 1.b} 
Rewrite the \texttt{select\_action()} function to sample from your policy distribution instead of outputting a deterministic action.
\begin{solution}
\begin{lstlisting}[language=Python]
def select_action():
    pass
\end{lstlisting}
\end{solution}

\subsection*{Question 1.c} 
Re-write your actor and critic loss to support your MaxEnt implementation. Paste the entire code of \texttt{train()} below.
\\
\textbf{Hint:} Look at your target Q and actor loss computations for DDPG and see what to change based on the MaxEnt and Bellman equations.
\begin{solution}
Your solution here...
\begin{lstlisting}[language=Python]
def train():
    pass
\end{lstlisting}
\end{solution}
\subsection*{Question 1.d}
Insert the reward v.s. episode curve below. If you notice that your model is unstable but sometimes has positive reward, try tuning some parameters. Your method should be able to converge to positive reward.
\begin{solution}
Your figure here...
\end{solution}
\end{document}

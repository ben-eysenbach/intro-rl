{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lbad9UM0y_Ay"
      },
      "source": [
        "# Homework: Implementing PPO on Gym\n",
        "\n",
        "In this homework, you will implement the Proximal Policy Optimization (PPO) algorithm and test it on an environment from OpenAI Gym. This will give you hands-on experience with one of the most influential policy gradient methods in reinforcement learning.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K_iLh_UdcVhb"
      },
      "source": [
        "## 1. Setup\n",
        "\n",
        "First, let's install and import the necessary packages. Run the following in a code cell:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cUn56vtUyhtQ",
        "outputId": "7c688377-8021-4a20-db10-8e7a4aab6302"
      },
      "outputs": [],
      "source": [
        "!pip install gym torch matplotlib seaborn\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UaWgeDI0yrE1"
      },
      "outputs": [],
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.distributions import Categorical\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import seaborn as sns\n",
        "sns.set(style=\"darkgrid\")\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rZZSMrM6Z_Gd"
      },
      "source": [
        "The following helper functions that compute advantage and evaluate a given policy are provided."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1zIfQVUl3OZv"
      },
      "outputs": [],
      "source": [
        "def compute_advantages(next_value, rewards, masks, values, gamma=0.99):\n",
        "    values = values + [next_value]\n",
        "    advantages = 0\n",
        "    returns = []\n",
        "    for step in reversed(range(len(rewards))):\n",
        "        delta = rewards[step] + gamma * values[step + 1] * masks[step] - values[step]\n",
        "        advantages = delta + gamma * masks[step] * advantages\n",
        "        returns.insert(0, advantages + values[step])\n",
        "    return returns\n",
        "\n",
        "def evaluate_policy(policy, env_name, seed=42):\n",
        "    env_test = gym.make(env_name)\n",
        "    # env_test.seed(seed)\n",
        "    state, done, total_reward = env_test.reset(), False, 0\n",
        "    while not done:\n",
        "        state = torch.FloatTensor(state).unsqueeze(0)\n",
        "        dist = policy(state)\n",
        "        next_state, reward, done, _ = env_test.step(dist.sample().numpy()[0])\n",
        "        state = next_state\n",
        "        total_reward += reward\n",
        "    return total_reward\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EcFmSceAzIv_"
      },
      "source": [
        "## 2. Defining the Policy and Value Networks\n",
        "\n",
        "We start from the policy network, which decides on the actions to take, and the value network, which estimates the returns. You need to implement both neural networks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FFPEfb-PzLs8"
      },
      "outputs": [],
      "source": [
        "# TODO: Implement the PolicyNetwork class\n",
        "\n",
        "class PolicyNetwork(nn.Module):\n",
        "\n",
        "  \"\"\"\n",
        "    Implement the Policy Network.\n",
        "\n",
        "    Your task is to complete the initialization of the policy network that maps states to action probabilities.\n",
        "    This network should consist of several fully connected layers with ReLU activation, followed by a final layer\n",
        "    that outputs logits for each action. The forward pass should return a Categorical distribution over actions.\n",
        "\n",
        "    Instructions:\n",
        "    1. Initialize the fully connected layers in the __init__ method.\n",
        "    2. Implement the forward pass to return a Categorical distribution given state inputs.\n",
        "\n",
        "    Hint: The constructor takes 'state_dim' and 'action_dim' as arguments, representing the dimensions\n",
        "    of the state space and action space, respectively.\n",
        "  \"\"\"\n",
        "  def __init__(self, state_dim, action_dim):\n",
        "      super(PolicyNetwork, self).__init__()\n",
        "      ##### Code implementation here #####\n",
        "      pass\n",
        "\n",
        "  def forward(self, x):\n",
        "      ##### Code implementation here #####\n",
        "      pass\n",
        "\n",
        "# TODO: Implement the ValueNetwork class\n",
        "class ValueNetwork(nn.Module):\n",
        "  \"\"\"\n",
        "    Implement the Value Network.\n",
        "\n",
        "    Your task is to complete the initialization of the value network that maps states to value estimates.\n",
        "    Similar to the policy network, this network should consist of several fully connected layers with ReLU activation\n",
        "    followed by a final layer that outputs a single value estimate for the input state.\n",
        "\n",
        "    Instructions:\n",
        "    1. Initialize the fully connected layers in the __init__ method.\n",
        "    2. Implement the forward pass to return the value estimate given state inputs.\n",
        "\n",
        "    Hint: The constructor takes 'state_dim' as an argument, representing the dimension of the state space.\n",
        "    \"\"\"\n",
        "  def __init__(self, state_dim):\n",
        "      super(ValueNetwork, self).__init__()\n",
        "      ##### Code implementation here #####\n",
        "      pass\n",
        "\n",
        "  def forward(self, x):\n",
        "      ##### Code implementation here #####\n",
        "      pass\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VPdjUPuDzNwk"
      },
      "source": [
        "## 3. Implementing the Training Loop\n",
        "\n",
        "Training in PPO involves collecting data from the environment, computing advantages, and updating the policy and value networks.\n",
        "\n",
        "1. `ppo_iter()` This function generates mini-batches from the collected data, which are then used for gradient updates. This function is provided to you.\n",
        "\n",
        "2. `ppo_update()` This function optimizes the policy and value networks using the Proximal Policy Optimization algorithm. This function applies the core PPO algorithm, using the experiences collected from the environment to perform multiple epochs of updates on the policy and value networks.You need to implement the core parts.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T3d-li082-K5"
      },
      "outputs": [],
      "source": [
        "def ppo_iter(mini_batch_size, states, actions, log_probs, returns, advantage):\n",
        "    batch_size = states.size(0)\n",
        "    mini_batches = []\n",
        "\n",
        "    for _ in range(batch_size // mini_batch_size):\n",
        "        rand_ids = np.random.randint(0, batch_size, mini_batch_size)\n",
        "        mini_batch = states[rand_ids, :], actions[rand_ids], log_probs[rand_ids], returns[rand_ids], advantage[rand_ids]\n",
        "        mini_batches.append(mini_batch)\n",
        "\n",
        "    return mini_batches\n",
        "\n",
        "def ppo_update(policy_net, value_net, optimizer, ppo_epochs, mini_batch_size, states, actions, log_probs, returns, advantages, clip_param=0.2):\n",
        "    \"\"\"\n",
        "    Implement the PPO update algorithm.\n",
        "\n",
        "    This function should perform the optimization of the policy and value networks using the Proximal Policy Optimization (PPO) algorithm.\n",
        "    You'll need to compute the ratio of new and old policy probabilities, apply the clipping technique, and calculate the losses for both the actor (policy network) and critic (value network).\n",
        "\n",
        "    Instructions:\n",
        "    1. Iterate over the number of PPO epochs, which is the number of optimizer.step() with the current collected data.\n",
        "    2. In each epoch, iterate over the mini-batches of experiences.\n",
        "    3. Calculate the new log probabilities of the actions taken, using the policy network.\n",
        "    4. Compute the ratio of new to old probabilities.\n",
        "    5. Apply the PPO clipping technique to the computed ratios.\n",
        "    6. Calculate the actor (policy) and critic (value) losses.\n",
        "    7. Combine the losses and perform a backpropagation step.\n",
        "\n",
        "    Hints:\n",
        "    - Use `policy_net(state)` to get the distribution over actions for the given states.\n",
        "    - The `dist.log_prob(action)` method calculates the log probabilities of the taken actions according to the current policy.\n",
        "    - The ratio is computed as the exponential of the difference between new and old log probabilities (`(new_log_probs - old_log_probs).exp()`).\n",
        "    - Use `torch.clamp(ratio, 1.0 - clip_param, 1.0 + clip_param)` to clip the ratio between `[1-clip_param, 1+clip_param]`.\n",
        "    - The actor loss is the negative minimum of the clipped and unclipped objective, averaged over all experiences in the mini-batch.\n",
        "    - The critic loss is the mean squared error between the returns and the value estimates from the value network.\n",
        "    - Remember to zero the gradients of the optimizer before the backpropagation step with `optimizer.zero_grad()`.\n",
        "    - After computing the loss and performing backpropagation with `loss.backward()`, take an optimization step with `optimizer.step()`.\n",
        "    \"\"\"\n",
        "    for _ in range(ppo_epochs):\n",
        "        for state, action, old_log_probs, return_, advantage in ppo_iter(mini_batch_size, states, actions, log_probs, returns, advantages):\n",
        "            dist = policy_net(state)\n",
        "            new_log_probs = dist.log_prob(action)\n",
        "\n",
        "            ##### Code implementation here #####\n",
        "            pass\n",
        "            actor_loss = None\n",
        "            critic_loss = None\n",
        "            ##### Code implementation End #####\n",
        "\n",
        "            loss = 0.5 * critic_loss + actor_loss\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WlYLpNVodf7v"
      },
      "source": [
        "Main training loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fgO1llI5zTOP"
      },
      "outputs": [],
      "source": [
        "def train(env_name='CartPole-v1', num_steps=1000, mini_batch_size=8, ppo_epochs=4, threshold=400):\n",
        "    env = gym.make(env_name)\n",
        "    state_dim = env.observation_space.shape[0]\n",
        "    action_dim = env.action_space.n\n",
        "\n",
        "    policy_net = PolicyNetwork(state_dim, action_dim)\n",
        "    value_net = ValueNetwork(state_dim)\n",
        "    optimizer = optim.Adam(list(policy_net.parameters()) + list(value_net.parameters()), lr=3e-3)\n",
        "\n",
        "    state = env.reset()\n",
        "    early_stop = False\n",
        "    reward_list = []\n",
        "\n",
        "    for step in range(num_steps):\n",
        "        log_probs = []\n",
        "        values = []\n",
        "        states = []\n",
        "        actions = []\n",
        "        rewards = []\n",
        "        masks = []\n",
        "        entropy = 0\n",
        "\n",
        "        # Collect samples under the current policy\n",
        "        for _ in range(2048):\n",
        "            state = torch.FloatTensor(state).unsqueeze(0)\n",
        "            dist, value = policy_net(state), value_net(state)\n",
        "\n",
        "            action = dist.sample()\n",
        "            next_state, reward, done, _ = env.step(action.numpy()[0])\n",
        "            log_prob = dist.log_prob(action)\n",
        "\n",
        "            log_probs.append(log_prob)\n",
        "            values.append(value)\n",
        "            rewards.append(torch.tensor([reward], dtype=torch.float32))\n",
        "            masks.append(torch.tensor([1-done], dtype=torch.float32))\n",
        "            states.append(state)\n",
        "            actions.append(action)\n",
        "\n",
        "            state = next_state\n",
        "            if done:\n",
        "                state = env.reset()\n",
        "\n",
        "        next_state = torch.FloatTensor(next_state).unsqueeze(0)\n",
        "        next_value = value_net(next_state)\n",
        "        returns = compute_advantages(next_value, rewards, masks, values)\n",
        "\n",
        "        returns = torch.cat(returns).detach()\n",
        "        log_probs = torch.cat(log_probs).detach()\n",
        "        values = torch.cat(values).detach()\n",
        "        states = torch.cat(states)\n",
        "        actions = torch.cat(actions)\n",
        "        advantage = returns - values\n",
        "\n",
        "        # run PPO update for policy and value networks\n",
        "        ppo_update(policy_net, value_net, optimizer, ppo_epochs, mini_batch_size, states, actions, log_probs, returns, advantage)\n",
        "\n",
        "        if step % 1 == 0:\n",
        "            test_reward = np.mean([evaluate_policy(policy_net, env_name) for _ in range(10)])\n",
        "            print(f'Step: {step}\\tReward: {test_reward}')\n",
        "            reward_list.append(test_reward)\n",
        "            if test_reward > threshold:\n",
        "                print(\"Solved!\")\n",
        "                early_stop = True\n",
        "                break\n",
        "    return early_stop, reward_list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MTpPrUBvbs0-"
      },
      "source": [
        "## 4. Training and evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aj9OOdtD8l6q",
        "outputId": "6a071707-427a-4f98-c413-dfc3bd569cff"
      },
      "outputs": [],
      "source": [
        "# Run the training function\n",
        "threshold = 400\n",
        "\n",
        "early_stop, reward_list = train(env_name='CartPole-v1', num_steps=100, mini_batch_size=16, ppo_epochs=4, threshold=threshold)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nhhI3U3SbxbZ"
      },
      "source": [
        "### Plot the performance curves"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 480
        },
        "id": "nJ0gMpE491SK",
        "outputId": "012f4ed6-c86a-4cee-aa61-e452447a380b"
      },
      "outputs": [],
      "source": [
        "if not early_stop:\n",
        "  print(\"Not solved in %d steps\"%len(reward_list))\n",
        "\n",
        "# Plot using Seaborn\n",
        "sns.lineplot(x=np.arange(len(reward_list)), y=reward_list, color='salmon', marker='o', linestyle='-', label='PPO training: reach reward %.1f with %d steps'%(threshold, len(reward_list)))\n",
        "\n",
        "# Optional: Adding titles and labels\n",
        "plt.title('Performance Curves')\n",
        "plt.xlabel('Step')\n",
        "plt.ylabel('Reward')\n",
        "plt.legend()\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

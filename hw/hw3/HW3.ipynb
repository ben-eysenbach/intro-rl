{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Q9l3TTHLd0_"
      },
      "source": [
        "## Homework 3\n",
        "\n",
        "In this assignment, we will practice implementing (1)value iteration and (2)policy gradient method in the Frozen Lake environment from OpenAI Gym. To begin with, for value iteration, there are two functions that you need to implement: (1) policy_improvement, which returns the optimal policy given a value function and environment dynamics, and (2) value_iteration, which implements value iteration.\n",
        "\n",
        "* You should run this on Google Colab\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G84AFb9mfS7n",
        "outputId": "2dec4087-746e-49ae-d320-24dd7f28e4b7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gymnasium==0.29.1\n",
            "  Downloading gymnasium-0.29.1-py3-none-any.whl (953 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m953.9/953.9 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium==0.29.1) (1.25.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium==0.29.1) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium==0.29.1) (4.9.0)\n",
            "Collecting farama-notifications>=0.0.1 (from gymnasium==0.29.1)\n",
            "  Downloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
            "Installing collected packages: farama-notifications, gymnasium\n",
            "Successfully installed farama-notifications-0.0.4 gymnasium-0.29.1\n"
          ]
        }
      ],
      "source": [
        "# IMPORTANT: Always run this cell before anything else to ensure that you are able to access the Frozen Lake environment\n",
        "!pip install gymnasium==0.29.1\n",
        "import gymnasium as gym\n",
        "import argparse\n",
        "import numpy as np\n",
        "import time\n",
        "from gymnasium.envs.registration import register\n",
        "\n",
        "# De-register environments if there is a collision\n",
        "env_dict = gym.envs.registration.registry.copy()\n",
        "for env in env_dict:\n",
        "    if \"Deterministic-4x4-FrozenLake-v0\" in env:\n",
        "        del gym.envs.registration.registry[env]\n",
        "    elif \"Stochastic-4x4-FrozenLake-v0\" in env:\n",
        "        del gym.envs.registration.registry[env]\n",
        "\n",
        "\n",
        "register(\n",
        "    id=\"Deterministic-4x4-FrozenLake-v0\",\n",
        "    entry_point=\"gymnasium.envs.toy_text.frozen_lake:FrozenLakeEnv\",\n",
        "    kwargs={\"map_name\": \"4x4\", \"is_slippery\": False},\n",
        ")\n",
        "\n",
        "register(\n",
        "    id=\"Stochastic-4x4-FrozenLake-v0\",\n",
        "    entry_point=\"gymnasium.envs.toy_text.frozen_lake:FrozenLakeEnv\",\n",
        "    kwargs={\"map_name\": \"4x4\", \"is_slippery\": True},\n",
        ")\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x3ZMIFfOlEfV"
      },
      "source": [
        "The parameters P, nS, nA, gamma are defined as follows:\n",
        "\n",
        "\tP: nested dictionary of a nested lists\n",
        "\t\tFrom gym.core.Environment\n",
        "\t\tFor each pair of states in [1, nS] and actions in [1, nA], P[state][action] is a\n",
        "\t\ttuple of the form (probability, nextstate, reward, terminal) where\n",
        "\t\t\t- probability: float\n",
        "\t\t\t\tthe probability of transitioning from \"state\" to \"nextstate\" with \"action\"\n",
        "\t\t\t- nextstate: int\n",
        "\t\t\t\tdenotes the state we transition to (in range [0, nS - 1])\n",
        "\t\t\t- reward: int\n",
        "\t\t\t\teither 0 or 1, the reward for transitioning from \"state\" to\n",
        "\t\t\t\t\"nextstate\" with \"action\"\n",
        "\t\t\t- terminal: bool\n",
        "\t\t\t  True when \"nextstate\" is a terminal state (hole or goal), False otherwise\n",
        "\tnS: int\n",
        "\t\tnumber of states in the environment\n",
        "\tnA: int\n",
        "\t\tnumber of actions in the environment\n",
        "\tgamma: float\n",
        "\t\tDiscount factor. Number in range [0, 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pjRnCPPiKjNA"
      },
      "source": [
        "## Value Iteration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Af0JXOtflUmh"
      },
      "outputs": [],
      "source": [
        "def policy_improvement(P, nS, nA, value_from_policy, policy, gamma=0.9):\n",
        "    \"\"\"Given the value function from policy improve the policy.\n",
        "\n",
        "\tParameters\n",
        "\t----------\n",
        "\tP, nS, nA, gamma:\n",
        "\t\tdefined at beginning of file\n",
        "\tvalue_from_policy: np.ndarray\n",
        "\t\tThe value calculated from the policy\n",
        "\tpolicy: np.array\n",
        "\t\tThe previous policy.\n",
        "\n",
        "\tReturns\n",
        "\t-------\n",
        "\tnew_policy: np.ndarray[nS]\n",
        "\t\tAn array of integers. Each integer is the optimal action to take\n",
        "\t\tin that state according to the environment dynamics and the\n",
        "\t\tgiven value function.\n",
        "\t\"\"\"\n",
        "\n",
        "    new_policy = np.zeros(nS, dtype=\"int\")\n",
        "\n",
        "    ############################\n",
        "    # YOUR IMPLEMENTATION HERE #\n",
        "\n",
        "    ############################\n",
        "    return new_policy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xL3cBRuWla19"
      },
      "outputs": [],
      "source": [
        "def value_iteration(P, nS, nA, gamma=0.9, tol=1e-3):\n",
        "    \"\"\"\n",
        "\tLearn value function and policy by using value iteration method for a given\n",
        "\tgamma and environment.\n",
        "\n",
        "\tParameters:\n",
        "\t----------\n",
        "\tP, nS, nA, gamma:\n",
        "\t\tdefined at beginning of file\n",
        "\ttol: float\n",
        "\t\tTerminate value iteration when\n",
        "\t\t\tmax |value_function(s) - prev_value_function(s)| < tol\n",
        "\tReturns:\n",
        "\t----------\n",
        "\tvalue_function: np.ndarray[nS]\n",
        "\tpolicy: np.ndarray[nS]\n",
        "\t\"\"\"\n",
        "\n",
        "    value_function = np.zeros(nS)\n",
        "    policy = np.zeros(nS, dtype=int)\n",
        "    ############################\n",
        "    # YOUR IMPLEMENTATION HERE #\n",
        "\n",
        "    ############################\n",
        "    return value_function, policy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z3G0x0oZlTCB"
      },
      "source": [
        "We provide you with the following function to evaluate how good your policy is, by interfering with the environment!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BR439ibymW6Z"
      },
      "outputs": [],
      "source": [
        "def evaluate(env, policy, max_steps=100):\n",
        "    \"\"\"\n",
        "    This function does not need to be modified\n",
        "    Watch your agent play!\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    env: gym.core.Environment\n",
        "      Environment to play on. Must have nS, nA, and P as\n",
        "      attributes.\n",
        "    Policy: np.array of shape [env.nS]\n",
        "      The action to take at a given state\n",
        "  \"\"\"\n",
        "\n",
        "    episode_reward = 0\n",
        "    ob, _ = env.reset()\n",
        "    for t in range(max_steps):\n",
        "        a = policy[ob]\n",
        "        ob, rew, done, _, _ = env.step(a)\n",
        "        episode_reward += rew\n",
        "        if done:\n",
        "            break\n",
        "    if not done:\n",
        "        print(\n",
        "            \"The agent didn't reach a terminal state in {} steps.\".format(\n",
        "                max_steps\n",
        "            )\n",
        "        )\n",
        "    else:\n",
        "        print(\"Episode reward: %f\" % episode_reward)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eaMQ1w5Rmand"
      },
      "outputs": [],
      "source": [
        "# Run the code below to implement value iteration on Frozen Lake!\n",
        "# You may change the parameters in the functions below\n",
        "np.set_printoptions(precision=3)\n",
        "\n",
        "# Make gym environment\n",
        "env = gym.make('Deterministic-4x4-FrozenLake-v0')\n",
        "\n",
        "env.nS = env.nrow * env.ncol\n",
        "env.nA = 4\n",
        "\n",
        "print(\"\\n\" + \"-\" * 25 + \"\\nBeginning Value Iteration\\n\" + \"-\" * 25)\n",
        "\n",
        "V_vi, p_vi = value_iteration(env.P, env.nS, env.nA, gamma=0.9, tol=1e-3)\n",
        "evaluate(env, p_vi, 100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hHh-z3zAK4ew"
      },
      "source": [
        "## Policy Gradient\n",
        "\n",
        "In this problem, we try to implement the `REINFORCE` algorithm to learn the optimal policy on environment `CartPole-v1` of OpenAI-Gym. We will use a neural network to parametrize the policy, and then run policy gradient on it. We  provide you with the following evaluation function, to calculat the expected cumulative reward of your current policy through Monte Carlo. You need to (1) implement REINFORCE with a neural network, and (2) plot the expected reward againt the number of training epochs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e-AfGSVyMj0k"
      },
      "outputs": [],
      "source": [
        "def evaluate_neural(env, policy, max_steps=1000, trials = 100):\n",
        "    \"\"\"\n",
        "    This function does not need to be modified\n",
        "    Renders policy once on environment. Watch your agent play!\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    env: gym.core.Environment\n",
        "    Policy: torch.Distribution, trained with REINFORCE\n",
        "    trials: int, number of trials for Monte Carlo, default = 100\n",
        "    Returns\n",
        "    -------\n",
        "    cum_reward_mean: estimate of the expected cumulative reward\n",
        "  \"\"\"\n",
        "    cum_reward = []\n",
        "    for i in range(trials):\n",
        "      episode_reward = 0\n",
        "      ob,_ = env.reset()\n",
        "      for t in range(max_steps):\n",
        "          ob = torch.tensor(ob, dtype=torch.float32)\n",
        "          a = Categorical(policy(ob)).sample().item()\n",
        "          ob, rew, done, _, _ = env.step(a)\n",
        "          episode_reward += rew\n",
        "          if done:\n",
        "              break\n",
        "      cum_reward.append(episode_reward)\n",
        "    cum_reward = np.array(cum_reward)\n",
        "    return cum_reward.mean(), cum_reward.std()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WcnwfykAPz0R"
      },
      "source": [
        "Now you are ready to implement the algorithm! In the following we define a class `PolicyNetwork` for the agent's policy, which you should implement by a neural network. Recall that the (variance reduced) gradient estimate in `REINFORCE` can be defined by\n",
        "$$ \\sum_{h\\geq 0} \\nabla_\\theta\\log \\pi_\\theta(A_h|S_h) \\cdot \\sum_{t\\geq h}\\gamma^t r(S_t, A_t).  $$\n",
        " You have the freedom to decide the architecture and the training procedure of the neural network, but make sure that it takes the state as input, and outputs a distrubution for the actions! Once it is implemented and trained, please plot the cumulative reward (with error bar = 2* standard deviation) against the number of epochs with `matplotlib.pyplot`, and submit it in the .tex file!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PM1uT5ngK7Tj"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.distributions import Categorical\n",
        "\n",
        "# Define the policy network\n",
        "class PolicyNetwork(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(PolicyNetwork, self).__init__()\n",
        "        ############################\n",
        "        # YOUR IMPLEMENTATION HERE #\n",
        "        pass\n",
        "        ############################\n",
        "\n",
        "    def forward(self, x):\n",
        "        ############################\n",
        "        # YOUR IMPLEMENTATION HERE #\n",
        "        pass\n",
        "        ############################\n",
        "\n",
        "# Initialize environment and optimizer. Feel free to change!\n",
        "env = gym.make('CartPole-v1')\n",
        "input_size = 4\n",
        "output_size = env.action_space.n\n",
        "hidden_size = 64\n",
        "policy_net = PolicyNetwork(input_size, hidden_size, output_size)\n",
        "optimizer = optim.Adam(policy_net.parameters(), lr=0.001)\n",
        "\n",
        "avg_rewards = []\n",
        "std_rewards = []\n",
        "\n",
        "# Training hyperparameters. Feel free to change!\n",
        "num_episodes = 3000\n",
        "gamma = 0.99\n",
        "\n",
        "# Training loop\n",
        "for episode in range(num_episodes):\n",
        "    episode_rewards = []\n",
        "    log_probs = []\n",
        "    rewards = []\n",
        "\n",
        "    state,_ = env.reset()\n",
        "    done = False\n",
        "\n",
        "    while not done:\n",
        "      # Sample action from the policy network. For simplicity we only use single trajectory\n",
        "      # to update. But feel free to change this into batch learning!\n",
        "      state_tensor = torch.from_numpy(state).float().unsqueeze(0)\n",
        "      action_probs = policy_net(state_tensor)\n",
        "      action_dist = Categorical(action_probs)\n",
        "      action = action_dist.sample()\n",
        "      log_prob = action_dist.log_prob(action)\n",
        "      log_probs.append(log_prob)\n",
        "\n",
        "      # Take action and observe next state and reward\n",
        "      next_state, reward, done, _, _ = env.step(action.item())\n",
        "      episode_rewards.append(reward)\n",
        "      state = next_state\n",
        "\n",
        "\n",
        "    # Now use the episode_rewards list, the log_prob list to construct a loss function, on which you can\n",
        "    # backward propagate and optimize with optimizer. First, try to compute the discounted cumulative reward\n",
        "    # for every step in the trajectory.\n",
        "    discounted_reward = []\n",
        "    ############################\n",
        "    # YOUR IMPLEMENTATION HERE #\n",
        "    pass\n",
        "    ############################\n",
        "    discounted_rewards = torch.tensor(discounted_reward, dtype = torch.float32)\n",
        "\n",
        "    # Now compute the policy loss with discounted_rewards and log_prob. Use this loss to run policy gradient.\n",
        "    ############################\n",
        "    # YOUR IMPLEMENTATION HERE #\n",
        "    pass\n",
        "    ############################\n",
        "\n",
        "    # Record the cumulative reward and its deviation once every 100 episodes\n",
        "    if (episode + 1) % 100 == 0:\n",
        "      avg_reward, std_reward = evaluate_neural(env, policy_net)\n",
        "      print(f'Episode [{episode + 1}/{num_episodes}], Cumulative Reward: {avg_reward}')\n",
        "      avg_rewards.append(avg_reward)\n",
        "      std_rewards.append(std_reward)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XP__60AnU-v9"
      },
      "outputs": [],
      "source": [
        "from matplotlib import pyplot as plt\n",
        "# Plot the curve of cumulative reward v.s. number of episodes, with error bar = 2* std_reward\n",
        "############################\n",
        "# YOUR IMPLEMENTATION HERE #\n",
        "pass\n",
        "############################\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KIhdWRNKdGWi"
      },
      "source": [
        "## Optional: Visualizing Your Learned Policy Within the Game\n",
        "Want to see how good your agent perform in the Frozen lake / CartPole-v1? Try to visualize you policy learned in both value iteration and policy gradient! If you are not familiar with environment visualization, we recommend you to check the codebase in HW2!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_bx897eqkkbd"
      },
      "outputs": [],
      "source": [
        "############################\n",
        "# YOUR IMPLEMENTATION HERE #\n",
        "\n",
        "############################\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
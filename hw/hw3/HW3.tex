\documentclass[12pt]{article}
\usepackage[colorinlistoftodos]{todonotes}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{bm}
\usepackage{enumerate}
\usepackage{fancyvrb}
\usepackage[top=1in, bottom=1in, left=1in, right=1in]{geometry}
\usepackage{hyperref}
\usepackage{placeins}
\usepackage{tikz}
\usepackage{tikzsymbols}
\usepackage{todonotes}
\usepackage{bbm}
\usepackage{color}
\usepackage{mathrsfs}
\usepackage{enumitem}
\usepackage{soul} % for HL
\usepackage{color} % for HL
\usepackage{minted}

\usepackage{listings}
\usepackage{xcolor}

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}

\lstset{style=mystyle}

% \renewcommand{\theenumi}{\roman{enumi}}
\newcommand{\rmn}[1]{{\textcolor{blue}{\bf [{\sc rmn:} #1]}}}
\DeclareMathOperator*{\argmax}{arg\,max}

\usetikzlibrary{positioning,calc}
%%%%%%%%%
\usepackage[most]{tcolorbox}
\newtcolorbox[]{solution}[1][]{%
    breakable,
    enhanced,
    colback=white,
    title=Solution,
    #1
}

\newtcolorbox[]{fillme}[1][]{%
    breakable,
    enhanced,
    colback=white,
    title=Fill me in,
    #1
}

\newcommand*{\E}{\mathbb{E}}
\newcommand*{\prob}{\mathbb{P}}
\newcommand*{\F}{\mathcal{F}}
\newcommand*{\bo}{\mathbb{B}}
\newtheorem{theorem}{Theorem}
\newtheorem{observation}{Observation}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}

%%%%%%%%%%

\date{Due February 26, 2024}
\author{\begin{fillme}[width=0.3\textwidth]
 Your name here.
\end{fillme}} % Fill in your name!

\title{ECE433/COS435 Introduction to RL\\
  Assignment 3: Value Iteration and Policy Gradient\\
  Spring 2024\\
}

\begin{document}
  \maketitle
  \section*{Collaborators}
\begin{fillme}
 Please fill in the names and NetIDs of your collaborators in this section.
\end{fillme}

\section*{Instructions}

Writeups should be typesetted in Latex and submitted as PDFs. You can work with whatever tool you like for the code, but \textbf{please submit the asked-for snippet and answer in the solutions box as part of your writeup. We will only be grading your write-up.} Make sure still also to attach your notebook/code with your submission.

\section*{Question 1. Finite-state Value Function}
An agent is navigating a very simple environment structured as a straight path with three states labeled
1, 2, and 3, where state 3 is a terminal state. At each step, the agent can choose to move to the next state
or stay in the current state. Assume the discount factor = 0.5.
The actions available in each state are: 
\begin{itemize}
    \item In state 1: the agent can either “move” to state 2 or “stay” in state 1. 
    \item In state 2: the agent can “move” to state 3, ”stay” in state 2, or “move back” to state 1.
\end{itemize}
The rewards are as follows:\begin{itemize}
    \item Moving from state 1 to state 2 gives a reward of -1.
    \item Moving from state 2 to state 3 gives a reward of 0. 
    \item Moving back from state 2 to state 1 gives a reward of -2.
    \item Staying in state 1 or state 2 gives a reward of -1.
\end{itemize}
\subsection*{Question 1.a}
Calculate the value function for each state when the agent
always decides to ”move” to the next state when possible.
\begin{solution}
Your solution here...
\end{solution}

\subsection*{Question 1.b}
Calculate the value function at each state when the agent always chooses to move to state 2 when in state 1, and always chooses to move back to state 1 when in state 2.
\begin{solution}
Your solution here...
\end{solution}

\section*{Question 2. Properties of value functions}
In this question, we consider an infinite horizon MDP $\mathcal{M} = (\mathcal{S}, \mathcal{A}, r, p, \gamma)$. We aim to derive some interesting properties for its value function.
\subsection*{Question 2.a} 
Show that a policy $\pi$ is optimal if and only if its corresponding value functions  $V^\pi: \mathcal{S} \rightarrow \mathbb{R}$ and $Q^\pi: \mathcal{S} \times \mathcal{A} \rightarrow \mathbb{R}$ satisfies $V^{\pi}(s)\geq Q^{\pi}(s,a)$  for all $(s,a) \in \mathcal{S} \times \mathcal{A}$.
\begin{solution}
Your solution here...
\end{solution}

%  \subsection*{Question 2.b} Show that the optimal value function of $\mathcal{M}$ is unique, i.e. for 2 function $V^{\pi_1}, V^{\pi_2}: \mathcal{S} \rightarrow \mathbb{R}$, if $V^{\pi_1}$ and $V^{\pi_2}$ are both optimal, then we have $V^{\pi_1} = V^{\pi_2}$ for all $s\in \mathcal{S}$.\\
% (Hint 1: Write out the Bellman equation for $V^{\pi_1}$ and $V^{\pi_2}$ respectively, then apply triangle inequality to their difference)\\
% (Hint 2: Utilize $|\max_{s\in \mathcal{S}} f_1(s) - \max_{s\in\mathcal{S}} f_2(s)| \leq |\max_{s\in \mathcal{S}} (f_1(s) - f_2(s))|$.)
% \begin{solution}
% Your solution here...
% \end{solution}
\subsection*{Question 2.b}
Is the optimal policy of an MDP unique? Please answer by proof or a counter-example.
\begin{solution}
Your solution here...
\end{solution}
\subsection*{Question 2.d}
Suppose that $\mathcal{M}$ has no terminating state. The agent will work forever. Now someone decides to add a small reward bonus $c$ to all transitions in the MDP, which results in a new reward $r'(s,a) = r(s,a)+c$ for all $(s,a)\in \mathcal{S}\times\mathcal{A}$. Note that $r$ is the original reward function. Could this change the optimal value function of $\mathcal{M}$?
\begin{solution}
    Your solution here...
\end{solution}
\subsection*{Question 2.e}
If $\mathcal{M}$ is allowed to have terminating states, does the answer in Question 2.d still hold? If not,  provide an example MDP where your answers to Question 2.d and this one differs.
\begin{solution}
    Your solution here...
\end{solution}
\section*{Question 3. Bellman residue}
In the lecture, we introduce the (optimal) Bellman operator for an infinite horizon MDP with discount factor $\gamma$ and transition $p$:
$$(\bo V)(s) = \max_{a\in\mathcal{A}} \bigg\{r(s,a) + \gamma \sum_{s'\in\mathcal{S}} p(s'|s,a) V(s')\bigg\},$$
and the Bellman operator with respect to a certain policy $\pi$:
$$(\bo^\pi V)(s) = \sum_{a\in\mathcal{A}} r(s,a)\pi(a|s)+ \gamma \sum_{s'\in\mathcal{S},a\in \mathcal{A}} p(s'|s,a)\pi(a|s) V(s').$$
We denote the optimal policy by $\pi^*$ and the optimal value function by $V^*$. As we know from the lecture, learning $V^*$ is equivalent to solving the following Bellman equation:
$$V(s) -\bo V(s) = 0, \forall s \in \mathcal{S}.$$
For an arbitrary function $V: \mathcal{S} \rightarrow \mathbb{R}$, define the Bellman residual to be $(\bo V - V ) $, and its magnitude by $\|(\bo V - V )\|_\infty$. Recall that for a vector $x = (x_i)_{i\in[d]}$, $\|\cdot\|_\infty$ is defined by $\max_{i\in[d]} |x_i|$. As we will see through the course, this Bellman residual is an important component of several important RL algorithms such as the Deep Q-network. 
\subsection*{Question 3.1}Prove the following statements for an arbitrary $V: \mathcal{S} \rightarrow \mathbb{R}$ (not necessarily a value function for any policy):
\begin{align*}
    \|V - V^\pi \|_\infty &\leq \frac{\|V - \bo^\pi V\|_\infty}{1-\gamma},\\
    \|V - V^* \|_\infty &\leq \frac{\|V - \bo V\|_\infty}{1-\gamma}.
\end{align*}
(Hint: use Bellman equation to expand $V^\pi$, then apply triangle inequality.)
\begin{solution}
    Your solution here...
\end{solution}
\subsection*{Question 3.2} Now let's assume that $\pi$ is the greedy policy extracted from $V$, and assume $\|V - \bo V\|_\infty \leq \epsilon$. Prove the following inequality by utilizing the results in Question 3.1: 
$$V^* - V^{\pi} \leq  \frac{2\epsilon}{1-\gamma}.$$
This shows that as long as the Bellman residue of $V$ is small, then the policy learned from $V$ will be not too bad. 
\begin{solution}
    Your solution here...
\end{solution}
\section*{Question 4. Coding}
% \subsection*{Policy Evaluation}
% Paste the \textbf{entire cell}  implementing value iteration below.
% \begin{solution}
%     \begin{minted}[autogobble]{python}
% # YOUR CODE HERE!
%     \end{minted}
% \end{solution}
\subsection*{Policy Improvement}
Paste the \textbf{entire cell}  implementing value iteration below.
\begin{solution}
    \begin{minted}[autogobble]{python}
# YOUR CODE HERE!
    \end{minted}
\end{solution}
% \subsection*{Policy Iteration}
% Paste the \textbf{entire cell}  implementing policy iteration below.
% \begin{solution}
%         \begin{minted}[autogobble]{python}
% # YOUR CODE HERE!
%     \end{minted}
% \end{solution}
\subsection*{Value Iteration}
Paste the \textbf{entire cell}  implementing value iteration below.
\begin{solution}
    \begin{minted}[autogobble]{python}
# YOUR CODE HERE!
    \end{minted}
\end{solution}
\subsection*{Policy Gradient}
Please paste the \textbf{entire cell}  implementing value iteration below. Also, plot the curve of average cumulative reward v.s. training episodes with \texttt{matplotlib.pyplot}, and insert it below. 
\begin{solution}
    \begin{minted}[autogobble]{python}
# YOUR CODE AND FIGURE HERE!
    \end{minted}
\end{solution}
\end{document}

